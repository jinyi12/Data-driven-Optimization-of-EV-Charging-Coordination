{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network training with CORLAT dataset, using a feasibility promoting weighted BCE loss\n",
    "\n",
    "This notebook explores the training of a simple Multi Layer Perceptron (MLP) neural network for the CORLAT dataset.\n",
    "\n",
    "The MLP outputs assignments of binary variables for the CORLAT dataset. \n",
    "\n",
    "The idea behind the custom loss is to provide higher weights for assignments that results in a better objective value (depending on minimization or maximization). The training of MLP in this experiment differs from the majority of neural network training paradigms. The important thing to note here is that:\n",
    "\n",
    "$$\\color{lightblue}\\text{For each sample, we have multiple sets of assignments}$$\n",
    "\n",
    "For example:\n",
    "Sample 1, 100 solutions (each solution is a set of binary assignments).\n",
    "\n",
    "We train on every feasible solution gathered (up to `n_sols` specified during data collection using the `corlat.py` script).\n",
    "\n",
    "The idea is to establish the conditional probability distribution $$p(Y_{i} | X_{i}) \\quad \\text{for} \\quad i=0, 1, 2, \\dots, n $$\n",
    "\n",
    "for feasible assignments. $n$ is the number samples, and $i$ represents the $i$-th sample. i.e., $p(y^{i}_{j} = 1 | X^{i})$ is the probability of assigning a $1$ to binary variable $j$, of sample $i$, such that the assignment is feasible. \n",
    "\n",
    "Hence, it becomes clear now that the weights for each set of assignments is to encourage assignments with better objective values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch.nn.functional import relu\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gurobipy as gb\n",
    "import time\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import *\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-4045b1e6-3428-f9e1-5643-862c4834363d)\n",
      "GPU 1: NVIDIA A100 80GB PCIe (UUID: GPU-35ac16d5-81e8-f772-b9cb-a681af1fd2b5)\n",
      "  MIG 2g.20gb     Device  0: (UUID: MIG-98c0ec5f-a99f-58b2-bbfd-d5521a6986ce)\n",
      "GPU 2: NVIDIA A100 80GB PCIe (UUID: GPU-d949dd0a-b88e-ee87-9621-3a824f914f82)\n",
      "GPU 3: NVIDIA A100 80GB PCIe (UUID: GPU-8c13f3ad-24ee-bb68-5eff-7f73091e682a)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 12 19:58:15 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  On   | 00000000:17:00.0 Off |                   On |\n",
      "| N/A   64C    P0   163W / 300W |  12227MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80G...  On   | 00000000:65:00.0 Off |                   On |\n",
      "| N/A   37C    P0    65W / 300W |   2250MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80G...  On   | 00000000:CA:00.0 Off |                   On |\n",
      "| N/A   32C    P0    44W / 300W |     24MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80G...  On   | 00000000:E3:00.0 Off |                   On |\n",
      "| N/A   31C    P0    43W / 300W |     24MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n",
      "|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n",
      "|                  |                      |        ECC|                       |\n",
      "|==================+======================+===========+=======================|\n",
      "|  1    3   0   0  |      9MiB / 19968MiB | 28      0 |  2   0    1    0    0 |\n",
      "|                  |      0MiB / 32767MiB |           |                       |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0    4    0    2836701      C   ...python/3.10.5/bin/python3    12199MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is multiple GPU, you can choose which one to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# set CUDA_VISIBLE_DEVICES=0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set CUBLAS config for deterministic `torch` behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to set random seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = (\n",
    "        False  # Force cuDNN to use a consistent convolution algorithm\n",
    "    )\n",
    "    torch.backends.cudnn.deterministic = (\n",
    "        True  # Force cuDNN to use deterministic algorithms if available\n",
    "    )\n",
    "    torch.use_deterministic_algorithms(\n",
    "        True\n",
    "    )  # Force torch to use deterministic algorithms if available\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "If cannot load, we `cd` to the respective project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat_presolved/processed_data/corlat_presolved_preprocessed.pickle\", \"rb\"))\n",
    "except:\n",
    "    # move dir to /ibm/gpfs/home/yjin0055/Project/DayAheadForecast\n",
    "    os.chdir(\"/ibm/gpfs/home/yjin0055/Project/DayAheadForecast\")\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat_presolved/processed_data/corlat_presolved_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "binary_indices = corlat_dataset[0][\"indices\"][\"indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# read X_train, X_test, y_train, y_test from Data/corlat_presolved/ using numpy.load\n",
    "X_train = np.load(\"Data/corlat_presolved/train_test_data/X_train.npy\")\n",
    "X_test = np.load(\"Data/corlat_presolved/train_test_data/X_test.npy\")\n",
    "y_train = np.load(\"Data/corlat_presolved/train_test_data/y_train.npy\", allow_pickle=True)\n",
    "y_test = np.load(\"Data/corlat_presolved/train_test_data/y_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all targets to binary values. Ensuring there is no values such as -0.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# for each instance in y_train and y_test, convert it to binary\n",
    "for i in range(y_train.shape[0]):\n",
    "    # make all values positive using abs\n",
    "    # y_train[i] is a tensor of shape (arbritary shape), num_vars\n",
    "    y_train[i] = np.abs(y_train[i])\n",
    "    \n",
    "    # use numpy where to convert values > 0.5 to 1, and values <= 0.5 to 0\n",
    "    y_train[i] = np.where(y_train[i] > 0.5, 1.0, 0.0)\n",
    "    \n",
    "for i in range(y_test.shape[0]):\n",
    "    # make all values positive using abs\n",
    "    # y_train[i] is a tensor of shape (arbritary shape), num_vars\n",
    "    y_test[i] = np.abs(y_test[i])\n",
    "    \n",
    "    # use numpy where to convert values > 0.5 to 1, and values <= 0.5 to 0\n",
    "    y_test[i] = np.where(y_test[i] > 0.5, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 1., 1., 1.],\n",
       "       [1., 0., 0., ..., 0., 1., 1.],\n",
       "       [1., 0., 1., ..., 0., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 0., 1., 1.],\n",
       "       [1., 0., 1., ..., 0., 1., 1.],\n",
       "       [1., 0., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the training and testing sample indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# train and test indices\n",
    "train_indices = np.load(\"Data/corlat_presolved/train_test_data/train_idx.npy\")\n",
    "test_indices = np.load(\"Data/corlat_presolved/train_test_data/test_idx.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "out_channels = y_train[0].shape[1]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_features//8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_features//8, n_features//16)\n",
    "        self.fc3 = nn.Linear(n_features//16, n_features//32)\n",
    "        self.fc4 = nn.Linear(n_features//32, out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # add regularization\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features:  14602\n",
      "out_channels:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"n_features: \", n_features)\n",
    "print(\"out_channels: \", out_channels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the weights for weighted feasibility promoting weighted BCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "weights = np.load(\"Data/corlat_presolved/train_test_data/train_weights.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "        'train_val_split': [0.80, 0.20], # These must sum to 1.0\n",
    "        'batch_size' : 32, # Num samples to average over for gradient updates\n",
    "        'EPOCHS' : 1000, # Num times to iterate over the entire dataset\n",
    "        'LEARNING_RATE' : 5e-4, # Learning rate for the optimizer\n",
    "        'BETA1' : 0.9, # Beta1 parameter for the Adam optimizer\n",
    "        'BETA2' : 0.999, # Beta2 parameter for the Adam optimizer\n",
    "        'WEIGHT_DECAY' : 1e-4, # Weight decay parameter for the Adam optimizer\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom class for our dataset\n",
    "\n",
    "The custom class is needed as our `y` data is an `object` type tensor. This is because `y` is an array of `n` samples, where each `y[i]` is of `n_sols` x 100, where 100 is 100 binary outputs. `n_sols` can have a maximum of 100, due to our setting of collecting 100 possibl solutions. However `n_sols` varies from sample to sample, as for some sample there might not be 100 sols.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class multipleTargetCORLATDataset(TensorDataset):\n",
    "    def __init__(self, X, y, weights=None, test=False):\n",
    "        super(multipleTargetCORLATDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.weights = weights\n",
    "        self.test = test\n",
    "        # self.obj_coeffs = get_nth_feature(self.X, 1)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "        \n",
    "        # duplicate X to match the number of targets\n",
    "        # X = np.repeat(X[np.newaxis,:], y.shape[0], axis=0)\n",
    "    \n",
    "        if self.weights is None and self.test:\n",
    "            return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        weights = self.weights[index]\n",
    "        \n",
    "        \n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "        weights_tensor = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "        # obj_coeffs_tensor = torch.tensor(self.obj_coeffs[index], dtype=torch.float32)\n",
    "        return X_tensor, y_tensor, weights_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A collate function is defined for our DataLoader\n",
    "\n",
    "The collate function determines how data is queried from the DataLoader class. A custom collate function is needed as we need our data to be returned in a different format than the default.\n",
    "\n",
    "We return `X` in our collate function as stacked samples of `X` (which is the default way of how a DataLoader class handles our data), resulting in a `batch_size` x `size_of_X` shape for X.\n",
    "\n",
    "For `Y` we return a list of Y samples. The list of Y samples have varying shapes. Each element of the list is `n_sols_i` x 100 where `n_sols_i` is `n_sols` of the `i`th sample.\n",
    "\n",
    "The `weights` have similar shape to `Y`. Each element of the list is of shape `n_sols_i` where `n_sols_i` is `n_sols` of the `i`th sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(data):\n",
    "    # data is a list of tuples (X, Y, weights)\n",
    "    # X_list = []\n",
    "    # Y_list = []\n",
    "    # weights_list = []\n",
    "    # for item in data:        \n",
    "    \n",
    "    X = torch.stack([item[0] for item in data])\n",
    "    Y = [item[1] for item in data]\n",
    "    \n",
    "    \n",
    "    # only X, and Y no weights\n",
    "    if len(data[0]) == 2:\n",
    "        return X, Y    \n",
    "    \n",
    "    weights = [item[2] for item in data]\n",
    "    #     X_list.append(item[0])\n",
    "    #     Y_list.append(item[1])\n",
    "    #     weights_list.append(item[2])\n",
    "    \n",
    "    # X = torch.stack(X_list)\n",
    "    # Y = torch.cat(Y_list)\n",
    "    # weights = torch.cat(weights_list)\n",
    "    \n",
    "    return X, Y, weights\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition for feasibility promoting weighted BCELoss\n",
    "The custom loss calculates the BCEloss between the predicted binary targets and each (possible) output binary solutions (keep in mind that we have a maximum of 100 total solutions). \n",
    "\n",
    "For each sample, we take the mean across the losses of the 100 binary variables. Then for each sample, we multiply the calculated loss with the respective weight. We then sum all the losses of `n_sols_i`. \n",
    "\n",
    "For each batch, we then take the mean loss of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# custom loss for neural network\n",
    "def feasibility_promoting_weighted_BCELoss(y_pred: torch.Tensor, y_true: torch.Tensor, weights: torch.Tensor, device: torch.device):\n",
    "    \n",
    "    batch_loss = []\n",
    "    \n",
    "    loss_fn = nn.BCELoss(reduction='none')\n",
    "        \n",
    "    # sum over all targets\n",
    "    for i in range(len(y_true)):\n",
    "        loss = torch.mean(loss_fn(y_pred[i].expand(len(y_true[i]), -1), y_true[i].to(device)), dim=1)\n",
    "        loss = torch.mul(loss, weights[i].to(device))\n",
    "        batch_loss.append(torch.sum(loss))\n",
    "    \n",
    "    # sum over all samples\n",
    "    batch_loss = torch.mean(torch.stack(batch_loss))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = multipleTargetCORLATDataset(X_train, y_train, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = multipleTargetCORLATDataset(X_test, y_test, test=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize neural network, train loader, valid, loader, and optimizer.\n",
    "\n",
    "We used a one cycle learning rate in this case, where the learning rate warms up and peaks at `max_lr`. Then decreases to a small learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "net = NeuralNetwork()\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "batch_size_test = 32\n",
    "valid_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "params = list(net.parameters())\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "total_steps = len(train_loader)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config['LEARNING_RATE'], steps_per_epoch=total_steps, epochs=config['EPOCHS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main training loop\n",
    "\n",
    "If the loss is smaller than the minimum loss, then save the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.684 lr: 0.000020\n",
      "Epoch 2 loss: 0.634 lr: 0.000020\n",
      "min loss:  0.6844135401200275\n",
      "Model saved\n",
      "Epoch 3 loss: 0.615 lr: 0.000020\n",
      "min loss:  0.6340614399131463\n",
      "Model saved\n",
      "Epoch 4 loss: 0.603 lr: 0.000020\n",
      "min loss:  0.6148994783965909\n",
      "Model saved\n",
      "Epoch 5 loss: 0.592 lr: 0.000020\n",
      "min loss:  0.6025286633141187\n",
      "Model saved\n",
      "Epoch 6 loss: 0.579 lr: 0.000020\n",
      "min loss:  0.592104079772015\n",
      "Model saved\n",
      "Epoch 7 loss: 0.568 lr: 0.000020\n",
      "min loss:  0.5792618965616032\n",
      "Model saved\n",
      "Epoch 8 loss: 0.556 lr: 0.000021\n",
      "min loss:  0.5682333634824169\n",
      "Model saved\n",
      "Epoch 9 loss: 0.543 lr: 0.000021\n",
      "min loss:  0.5556475532298185\n",
      "Model saved\n",
      "Epoch 10 loss: 0.537 lr: 0.000021\n",
      "min loss:  0.5428663115112149\n",
      "Model saved\n",
      "Epoch 11 loss: 0.533 lr: 0.000021\n",
      "min loss:  0.5373138608981152\n",
      "Model saved\n",
      "Epoch 12 loss: 0.532 lr: 0.000022\n",
      "min loss:  0.5332775973543828\n",
      "Model saved\n",
      "Epoch 13 loss: 0.531 lr: 0.000022\n",
      "min loss:  0.5316924379796398\n",
      "Model saved\n",
      "Epoch 14 loss: 0.530 lr: 0.000022\n",
      "min loss:  0.5312055945396423\n",
      "Model saved\n",
      "Epoch 15 loss: 0.528 lr: 0.000023\n",
      "min loss:  0.5304336401881004\n",
      "Model saved\n",
      "Epoch 16 loss: 0.525 lr: 0.000023\n",
      "min loss:  0.527646796435726\n",
      "Model saved\n",
      "Epoch 17 loss: 0.526 lr: 0.000023\n",
      "min loss:  0.5252158891181556\n",
      "Epoch 18 loss: 0.525 lr: 0.000024\n",
      "min loss:  0.5252158891181556\n",
      "Model saved\n",
      "Epoch 19 loss: 0.524 lr: 0.000024\n",
      "min loss:  0.5252019458887528\n",
      "Model saved\n",
      "Epoch 20 loss: 0.523 lr: 0.000025\n",
      "min loss:  0.5239821994791225\n",
      "Model saved\n",
      "Epoch 21 loss: 0.523 lr: 0.000025\n",
      "min loss:  0.5233059750527752\n",
      "Epoch 22 loss: 0.524 lr: 0.000026\n",
      "min loss:  0.5233059750527752\n",
      "Epoch 23 loss: 0.522 lr: 0.000026\n",
      "min loss:  0.5233059750527752\n",
      "Model saved\n",
      "Epoch 24 loss: 0.521 lr: 0.000027\n",
      "min loss:  0.5223117829585562\n",
      "Model saved\n",
      "Epoch 25 loss: 0.520 lr: 0.000028\n",
      "min loss:  0.5205876602202045\n",
      "Model saved\n",
      "Epoch 26 loss: 0.519 lr: 0.000028\n",
      "min loss:  0.51982274286601\n",
      "Model saved\n",
      "Epoch 27 loss: 0.518 lr: 0.000029\n",
      "min loss:  0.5189528106426706\n",
      "Model saved\n",
      "Epoch 28 loss: 0.518 lr: 0.000030\n",
      "min loss:  0.5182459798394418\n",
      "Model saved\n",
      "Epoch 29 loss: 0.518 lr: 0.000030\n",
      "min loss:  0.5181478870158293\n",
      "Epoch 30 loss: 0.516 lr: 0.000031\n",
      "min loss:  0.5181478870158293\n",
      "Model saved\n",
      "Epoch 31 loss: 0.517 lr: 0.000032\n",
      "min loss:  0.5156304051681441\n",
      "Epoch 32 loss: 0.516 lr: 0.000033\n",
      "min loss:  0.5156304051681441\n",
      "Epoch 33 loss: 0.515 lr: 0.000033\n",
      "min loss:  0.5156304051681441\n",
      "Model saved\n",
      "Epoch 34 loss: 0.515 lr: 0.000034\n",
      "min loss:  0.5147576757839748\n",
      "Epoch 35 loss: 0.514 lr: 0.000035\n",
      "min loss:  0.5147576757839748\n",
      "Model saved\n",
      "Epoch 36 loss: 0.513 lr: 0.000036\n",
      "min loss:  0.5143979854729711\n",
      "Model saved\n",
      "Epoch 37 loss: 0.513 lr: 0.000037\n",
      "min loss:  0.5127555709712359\n",
      "Epoch 38 loss: 0.511 lr: 0.000038\n",
      "min loss:  0.5127555709712359\n",
      "Model saved\n",
      "Epoch 39 loss: 0.509 lr: 0.000039\n",
      "min loss:  0.5106567424170825\n",
      "Model saved\n",
      "Epoch 40 loss: 0.509 lr: 0.000040\n",
      "min loss:  0.5087219050952366\n",
      "Epoch 41 loss: 0.510 lr: 0.000041\n",
      "min loss:  0.5087219050952366\n",
      "Epoch 42 loss: 0.508 lr: 0.000042\n",
      "min loss:  0.5087219050952366\n",
      "Model saved\n",
      "Epoch 43 loss: 0.505 lr: 0.000043\n",
      "min loss:  0.5083715927844145\n",
      "Model saved\n",
      "Epoch 44 loss: 0.507 lr: 0.000044\n",
      "min loss:  0.5054421108596179\n",
      "Epoch 45 loss: 0.506 lr: 0.000045\n",
      "min loss:  0.5054421108596179\n",
      "Epoch 46 loss: 0.504 lr: 0.000046\n",
      "min loss:  0.5054421108596179\n",
      "Model saved\n",
      "Epoch 47 loss: 0.501 lr: 0.000047\n",
      "min loss:  0.5036539958447827\n",
      "Model saved\n",
      "Epoch 48 loss: 0.501 lr: 0.000048\n",
      "min loss:  0.501380271449381\n",
      "Model saved\n",
      "Epoch 49 loss: 0.499 lr: 0.000050\n",
      "min loss:  0.5008618600514471\n",
      "Model saved\n",
      "Epoch 50 loss: 0.498 lr: 0.000051\n",
      "min loss:  0.4992740750312805\n",
      "Model saved\n",
      "Epoch 51 loss: 0.499 lr: 0.000052\n",
      "min loss:  0.49776191492469946\n",
      "Epoch 52 loss: 0.496 lr: 0.000053\n",
      "min loss:  0.49776191492469946\n",
      "Model saved\n",
      "Epoch 53 loss: 0.498 lr: 0.000055\n",
      "min loss:  0.4957334253252769\n",
      "Epoch 54 loss: 0.496 lr: 0.000056\n",
      "min loss:  0.4957334253252769\n",
      "Epoch 55 loss: 0.497 lr: 0.000057\n",
      "min loss:  0.4957334253252769\n",
      "Epoch 56 loss: 0.495 lr: 0.000059\n",
      "min loss:  0.4957334253252769\n",
      "Model saved\n",
      "Epoch 57 loss: 0.495 lr: 0.000060\n",
      "min loss:  0.495406303478747\n",
      "Model saved\n",
      "Epoch 58 loss: 0.493 lr: 0.000062\n",
      "min loss:  0.49453764424032093\n",
      "Model saved\n",
      "Epoch 59 loss: 0.493 lr: 0.000063\n",
      "min loss:  0.4925891562384002\n",
      "Epoch 60 loss: 0.491 lr: 0.000064\n",
      "min loss:  0.4925891562384002\n",
      "Model saved\n",
      "Epoch 61 loss: 0.494 lr: 0.000066\n",
      "min loss:  0.49142758152922805\n",
      "Epoch 62 loss: 0.488 lr: 0.000067\n",
      "min loss:  0.49142758152922805\n",
      "Model saved\n",
      "Epoch 63 loss: 0.489 lr: 0.000069\n",
      "min loss:  0.48816776883845425\n",
      "Epoch 64 loss: 0.491 lr: 0.000070\n",
      "min loss:  0.48816776883845425\n",
      "Epoch 65 loss: 0.488 lr: 0.000072\n",
      "min loss:  0.48816776883845425\n",
      "Model saved\n",
      "Epoch 66 loss: 0.490 lr: 0.000073\n",
      "min loss:  0.4875270949334514\n",
      "Epoch 67 loss: 0.486 lr: 0.000075\n",
      "min loss:  0.4875270949334514\n",
      "Model saved\n",
      "Epoch 68 loss: 0.486 lr: 0.000077\n",
      "min loss:  0.4856720366040055\n",
      "Model saved\n",
      "Epoch 69 loss: 0.486 lr: 0.000078\n",
      "min loss:  0.4855777511791307\n",
      "Epoch 70 loss: 0.491 lr: 0.000080\n",
      "min loss:  0.4855777511791307\n",
      "Epoch 71 loss: 0.490 lr: 0.000082\n",
      "min loss:  0.4855777511791307\n",
      "Epoch 72 loss: 0.489 lr: 0.000083\n",
      "min loss:  0.4855777511791307\n",
      "Epoch 73 loss: 0.484 lr: 0.000085\n",
      "min loss:  0.4855777511791307\n",
      "Model saved\n",
      "Epoch 74 loss: 0.481 lr: 0.000087\n",
      "min loss:  0.4841923129801847\n",
      "Model saved\n",
      "Epoch 75 loss: 0.483 lr: 0.000089\n",
      "min loss:  0.48116440310770153\n",
      "Epoch 76 loss: 0.485 lr: 0.000090\n",
      "min loss:  0.48116440310770153\n",
      "Epoch 77 loss: 0.485 lr: 0.000092\n",
      "min loss:  0.48116440310770153\n",
      "Epoch 78 loss: 0.483 lr: 0.000094\n",
      "min loss:  0.48116440310770153\n",
      "Epoch 79 loss: 0.481 lr: 0.000096\n",
      "min loss:  0.48116440310770153\n",
      "Epoch 80 loss: 0.483 lr: 0.000098\n",
      "min loss:  0.48116440310770153\n",
      "Epoch 81 loss: 0.480 lr: 0.000099\n",
      "min loss:  0.48116440310770153\n",
      "Model saved\n",
      "Epoch 82 loss: 0.488 lr: 0.000101\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 83 loss: 0.486 lr: 0.000103\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 84 loss: 0.482 lr: 0.000105\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 85 loss: 0.483 lr: 0.000107\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 86 loss: 0.486 lr: 0.000109\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 87 loss: 0.487 lr: 0.000111\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 88 loss: 0.489 lr: 0.000113\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 89 loss: 0.493 lr: 0.000115\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 90 loss: 0.484 lr: 0.000117\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 91 loss: 0.483 lr: 0.000119\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 92 loss: 0.482 lr: 0.000121\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 93 loss: 0.483 lr: 0.000123\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 94 loss: 0.486 lr: 0.000125\n",
      "min loss:  0.4804015390726985\n",
      "Epoch 95 loss: 0.480 lr: 0.000127\n",
      "min loss:  0.4804015390726985\n",
      "Model saved\n",
      "Epoch 96 loss: 0.482 lr: 0.000129\n",
      "min loss:  0.47950053032563655\n",
      "Epoch 97 loss: 0.486 lr: 0.000131\n",
      "min loss:  0.47950053032563655\n",
      "Epoch 98 loss: 0.484 lr: 0.000134\n",
      "min loss:  0.47950053032563655\n",
      "Epoch 99 loss: 0.484 lr: 0.000136\n",
      "min loss:  0.47950053032563655\n",
      "Epoch 100 loss: 0.484 lr: 0.000138\n",
      "min loss:  0.47950053032563655\n",
      "Epoch 101 loss: 0.482 lr: 0.000140\n",
      "min loss:  0.47950053032563655\n",
      "Epoch 102 loss: 0.482 lr: 0.000142\n",
      "min loss:  0.47950053032563655\n",
      "Epoch 103 loss: 0.483 lr: 0.000144\n",
      "min loss:  0.47950053032563655\n",
      "Epoch 104 loss: 0.476 lr: 0.000147\n",
      "min loss:  0.47950053032563655\n",
      "Model saved\n",
      "Epoch 105 loss: 0.476 lr: 0.000149\n",
      "min loss:  0.47602788100437243\n",
      "Model saved\n",
      "Epoch 106 loss: 0.476 lr: 0.000151\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 107 loss: 0.481 lr: 0.000153\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 108 loss: 0.483 lr: 0.000156\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 109 loss: 0.483 lr: 0.000158\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 110 loss: 0.481 lr: 0.000160\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 111 loss: 0.476 lr: 0.000162\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 112 loss: 0.480 lr: 0.000165\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 113 loss: 0.478 lr: 0.000167\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 114 loss: 0.483 lr: 0.000169\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 115 loss: 0.483 lr: 0.000172\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 116 loss: 0.479 lr: 0.000174\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 117 loss: 0.478 lr: 0.000176\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 118 loss: 0.483 lr: 0.000179\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 119 loss: 0.484 lr: 0.000181\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 120 loss: 0.486 lr: 0.000183\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 121 loss: 0.487 lr: 0.000186\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 122 loss: 0.483 lr: 0.000188\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 123 loss: 0.489 lr: 0.000191\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 124 loss: 0.490 lr: 0.000193\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 125 loss: 0.485 lr: 0.000195\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 126 loss: 0.484 lr: 0.000198\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 127 loss: 0.480 lr: 0.000200\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 128 loss: 0.478 lr: 0.000203\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 129 loss: 0.484 lr: 0.000205\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 130 loss: 0.477 lr: 0.000208\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 131 loss: 0.478 lr: 0.000210\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 132 loss: 0.488 lr: 0.000213\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 133 loss: 0.480 lr: 0.000215\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 134 loss: 0.484 lr: 0.000218\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 135 loss: 0.491 lr: 0.000220\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 136 loss: 0.484 lr: 0.000222\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 137 loss: 0.479 lr: 0.000225\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 138 loss: 0.498 lr: 0.000227\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 139 loss: 0.485 lr: 0.000230\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 140 loss: 0.482 lr: 0.000232\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 141 loss: 0.479 lr: 0.000235\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 142 loss: 0.490 lr: 0.000237\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 143 loss: 0.488 lr: 0.000240\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 144 loss: 0.479 lr: 0.000242\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 145 loss: 0.481 lr: 0.000245\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 146 loss: 0.489 lr: 0.000247\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 147 loss: 0.488 lr: 0.000250\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 148 loss: 0.479 lr: 0.000252\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 149 loss: 0.483 lr: 0.000255\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 150 loss: 0.484 lr: 0.000258\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 151 loss: 0.485 lr: 0.000260\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 152 loss: 0.498 lr: 0.000263\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 153 loss: 0.489 lr: 0.000265\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 154 loss: 0.487 lr: 0.000268\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 155 loss: 0.486 lr: 0.000270\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 156 loss: 0.484 lr: 0.000273\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 157 loss: 0.487 lr: 0.000275\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 158 loss: 0.483 lr: 0.000278\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 159 loss: 0.485 lr: 0.000280\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 160 loss: 0.491 lr: 0.000283\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 161 loss: 0.482 lr: 0.000285\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 162 loss: 0.488 lr: 0.000288\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 163 loss: 0.501 lr: 0.000290\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 164 loss: 0.483 lr: 0.000293\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 165 loss: 0.487 lr: 0.000295\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 166 loss: 0.485 lr: 0.000298\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 167 loss: 0.482 lr: 0.000300\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 168 loss: 0.476 lr: 0.000303\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 169 loss: 0.478 lr: 0.000305\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 170 loss: 0.482 lr: 0.000307\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 171 loss: 0.484 lr: 0.000310\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 172 loss: 0.487 lr: 0.000312\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 173 loss: 0.481 lr: 0.000315\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 174 loss: 0.493 lr: 0.000317\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 175 loss: 0.497 lr: 0.000320\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 176 loss: 0.490 lr: 0.000322\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 177 loss: 0.503 lr: 0.000325\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 178 loss: 0.496 lr: 0.000327\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 179 loss: 0.485 lr: 0.000329\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 180 loss: 0.480 lr: 0.000332\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 181 loss: 0.479 lr: 0.000334\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 182 loss: 0.486 lr: 0.000337\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 183 loss: 0.486 lr: 0.000339\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 184 loss: 0.484 lr: 0.000341\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 185 loss: 0.484 lr: 0.000344\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 186 loss: 0.488 lr: 0.000346\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 187 loss: 0.481 lr: 0.000348\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 188 loss: 0.483 lr: 0.000351\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 189 loss: 0.487 lr: 0.000353\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 190 loss: 0.481 lr: 0.000355\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 191 loss: 0.482 lr: 0.000358\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 192 loss: 0.483 lr: 0.000360\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 193 loss: 0.496 lr: 0.000362\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 194 loss: 0.485 lr: 0.000364\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 195 loss: 0.491 lr: 0.000367\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 196 loss: 0.491 lr: 0.000369\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 197 loss: 0.484 lr: 0.000371\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 198 loss: 0.482 lr: 0.000373\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 199 loss: 0.483 lr: 0.000376\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 200 loss: 0.492 lr: 0.000378\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 201 loss: 0.495 lr: 0.000380\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 202 loss: 0.486 lr: 0.000382\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 203 loss: 0.504 lr: 0.000384\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 204 loss: 0.517 lr: 0.000386\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 205 loss: 0.503 lr: 0.000389\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 206 loss: 0.499 lr: 0.000391\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 207 loss: 0.498 lr: 0.000393\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 208 loss: 0.498 lr: 0.000395\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 209 loss: 0.524 lr: 0.000397\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 210 loss: 0.511 lr: 0.000399\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 211 loss: 0.498 lr: 0.000401\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 212 loss: 0.496 lr: 0.000403\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 213 loss: 0.491 lr: 0.000405\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 214 loss: 0.493 lr: 0.000407\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 215 loss: 0.496 lr: 0.000409\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 216 loss: 0.495 lr: 0.000411\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 217 loss: 0.493 lr: 0.000413\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 218 loss: 0.492 lr: 0.000415\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 219 loss: 0.513 lr: 0.000417\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 220 loss: 0.518 lr: 0.000419\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 221 loss: 0.515 lr: 0.000421\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 222 loss: 0.507 lr: 0.000422\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 223 loss: 0.503 lr: 0.000424\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 224 loss: 0.495 lr: 0.000426\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 225 loss: 0.493 lr: 0.000428\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 226 loss: 0.490 lr: 0.000430\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 227 loss: 0.493 lr: 0.000432\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 228 loss: 0.512 lr: 0.000433\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 229 loss: 0.504 lr: 0.000435\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 230 loss: 0.501 lr: 0.000437\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 231 loss: 0.502 lr: 0.000438\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 232 loss: 0.518 lr: 0.000440\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 233 loss: 0.504 lr: 0.000442\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 234 loss: 0.491 lr: 0.000443\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 235 loss: 0.508 lr: 0.000445\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 236 loss: 0.504 lr: 0.000447\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 237 loss: 0.514 lr: 0.000448\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 238 loss: 0.513 lr: 0.000450\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 239 loss: 0.492 lr: 0.000451\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 240 loss: 0.494 lr: 0.000453\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 241 loss: 0.511 lr: 0.000454\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 242 loss: 0.512 lr: 0.000456\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 243 loss: 0.507 lr: 0.000457\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 244 loss: 0.505 lr: 0.000459\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 245 loss: 0.509 lr: 0.000460\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 246 loss: 0.500 lr: 0.000461\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 247 loss: 0.510 lr: 0.000463\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 248 loss: 0.520 lr: 0.000464\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 249 loss: 0.495 lr: 0.000465\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 250 loss: 0.495 lr: 0.000467\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 251 loss: 0.494 lr: 0.000468\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 252 loss: 0.501 lr: 0.000469\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 253 loss: 0.496 lr: 0.000470\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 254 loss: 0.525 lr: 0.000472\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 255 loss: 0.533 lr: 0.000473\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 256 loss: 0.503 lr: 0.000474\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 257 loss: 0.492 lr: 0.000475\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 258 loss: 0.516 lr: 0.000476\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 259 loss: 0.508 lr: 0.000477\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 260 loss: 0.499 lr: 0.000478\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 261 loss: 0.492 lr: 0.000479\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 262 loss: 0.504 lr: 0.000480\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 263 loss: 0.493 lr: 0.000481\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 264 loss: 0.508 lr: 0.000482\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 265 loss: 0.502 lr: 0.000483\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 266 loss: 0.513 lr: 0.000484\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 267 loss: 0.508 lr: 0.000485\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 268 loss: 0.501 lr: 0.000486\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 269 loss: 0.508 lr: 0.000487\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 270 loss: 0.510 lr: 0.000487\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 271 loss: 0.514 lr: 0.000488\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 272 loss: 0.522 lr: 0.000489\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 273 loss: 0.504 lr: 0.000490\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 274 loss: 0.508 lr: 0.000490\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 275 loss: 0.495 lr: 0.000491\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 276 loss: 0.505 lr: 0.000492\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 277 loss: 0.503 lr: 0.000492\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 278 loss: 0.498 lr: 0.000493\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 279 loss: 0.501 lr: 0.000494\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 280 loss: 0.495 lr: 0.000494\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 281 loss: 0.499 lr: 0.000495\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 282 loss: 0.495 lr: 0.000495\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 283 loss: 0.497 lr: 0.000496\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 284 loss: 0.511 lr: 0.000496\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 285 loss: 0.504 lr: 0.000497\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 286 loss: 0.513 lr: 0.000497\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 287 loss: 0.498 lr: 0.000497\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 288 loss: 0.496 lr: 0.000498\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 289 loss: 0.493 lr: 0.000498\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 290 loss: 0.490 lr: 0.000498\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 291 loss: 0.506 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 292 loss: 0.532 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 293 loss: 0.517 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 294 loss: 0.500 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 295 loss: 0.497 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 296 loss: 0.489 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 297 loss: 0.521 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 298 loss: 0.520 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 299 loss: 0.495 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 300 loss: 0.502 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 301 loss: 0.523 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 302 loss: 0.520 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 303 loss: 0.526 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 304 loss: 0.522 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 305 loss: 0.512 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 306 loss: 0.520 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 307 loss: 0.522 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 308 loss: 0.530 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 309 loss: 0.514 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 310 loss: 0.513 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 311 loss: 0.514 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 312 loss: 0.511 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 313 loss: 0.531 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 314 loss: 0.519 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 315 loss: 0.518 lr: 0.000500\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 316 loss: 0.508 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 317 loss: 0.504 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 318 loss: 0.501 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 319 loss: 0.553 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 320 loss: 0.516 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 321 loss: 0.519 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 322 loss: 0.512 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 323 loss: 0.507 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 324 loss: 0.510 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 325 loss: 0.502 lr: 0.000499\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 326 loss: 0.521 lr: 0.000498\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 327 loss: 0.504 lr: 0.000498\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 328 loss: 0.574 lr: 0.000498\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 329 loss: 0.521 lr: 0.000498\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 330 loss: 0.528 lr: 0.000498\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 331 loss: 0.514 lr: 0.000498\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 332 loss: 0.505 lr: 0.000498\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 333 loss: 0.502 lr: 0.000497\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 334 loss: 0.505 lr: 0.000497\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 335 loss: 0.502 lr: 0.000497\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 336 loss: 0.515 lr: 0.000497\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 337 loss: 0.511 lr: 0.000497\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 338 loss: 0.516 lr: 0.000497\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 339 loss: 0.506 lr: 0.000496\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 340 loss: 0.503 lr: 0.000496\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 341 loss: 0.501 lr: 0.000496\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 342 loss: 0.505 lr: 0.000496\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 343 loss: 0.504 lr: 0.000496\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 344 loss: 0.507 lr: 0.000495\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 345 loss: 0.510 lr: 0.000495\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 346 loss: 0.503 lr: 0.000495\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 347 loss: 0.501 lr: 0.000495\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 348 loss: 0.498 lr: 0.000494\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 349 loss: 0.510 lr: 0.000494\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 350 loss: 0.499 lr: 0.000494\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 351 loss: 0.554 lr: 0.000494\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 352 loss: 0.507 lr: 0.000493\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 353 loss: 0.504 lr: 0.000493\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 354 loss: 0.511 lr: 0.000493\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 355 loss: 0.508 lr: 0.000493\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 356 loss: 0.504 lr: 0.000492\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 357 loss: 0.510 lr: 0.000492\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 358 loss: 0.513 lr: 0.000492\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 359 loss: 0.498 lr: 0.000492\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 360 loss: 0.504 lr: 0.000491\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 361 loss: 0.506 lr: 0.000491\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 362 loss: 0.498 lr: 0.000491\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 363 loss: 0.504 lr: 0.000490\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 364 loss: 0.502 lr: 0.000490\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 365 loss: 0.508 lr: 0.000490\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 366 loss: 0.524 lr: 0.000489\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 367 loss: 0.512 lr: 0.000489\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 368 loss: 0.529 lr: 0.000489\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 369 loss: 0.513 lr: 0.000488\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 370 loss: 0.512 lr: 0.000488\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 371 loss: 0.504 lr: 0.000488\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 372 loss: 0.509 lr: 0.000487\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 373 loss: 0.524 lr: 0.000487\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 374 loss: 0.506 lr: 0.000487\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 375 loss: 0.522 lr: 0.000486\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 376 loss: 0.512 lr: 0.000486\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 377 loss: 0.514 lr: 0.000486\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 378 loss: 0.526 lr: 0.000485\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 379 loss: 0.522 lr: 0.000485\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 380 loss: 0.535 lr: 0.000484\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 381 loss: 0.525 lr: 0.000484\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 382 loss: 0.515 lr: 0.000484\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 383 loss: 0.513 lr: 0.000483\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 384 loss: 0.508 lr: 0.000483\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 385 loss: 0.506 lr: 0.000482\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 386 loss: 0.514 lr: 0.000482\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 387 loss: 0.509 lr: 0.000482\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 388 loss: 0.507 lr: 0.000481\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 389 loss: 0.505 lr: 0.000481\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 390 loss: 0.508 lr: 0.000480\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 391 loss: 0.506 lr: 0.000480\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 392 loss: 0.510 lr: 0.000479\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 393 loss: 0.515 lr: 0.000479\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 394 loss: 0.505 lr: 0.000479\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 395 loss: 0.508 lr: 0.000478\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 396 loss: 0.509 lr: 0.000478\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 397 loss: 0.506 lr: 0.000477\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 398 loss: 0.511 lr: 0.000477\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 399 loss: 0.507 lr: 0.000476\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 400 loss: 0.505 lr: 0.000476\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 401 loss: 0.508 lr: 0.000475\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 402 loss: 0.515 lr: 0.000475\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 403 loss: 0.503 lr: 0.000474\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 404 loss: 0.498 lr: 0.000474\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 405 loss: 0.517 lr: 0.000473\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 406 loss: 0.518 lr: 0.000473\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 407 loss: 0.510 lr: 0.000472\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 408 loss: 0.511 lr: 0.000472\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 409 loss: 0.506 lr: 0.000471\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 410 loss: 0.510 lr: 0.000471\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 411 loss: 0.508 lr: 0.000470\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 412 loss: 0.504 lr: 0.000470\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 413 loss: 0.501 lr: 0.000469\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 414 loss: 0.503 lr: 0.000469\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 415 loss: 0.511 lr: 0.000468\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 416 loss: 0.512 lr: 0.000467\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 417 loss: 0.504 lr: 0.000467\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 418 loss: 0.503 lr: 0.000466\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 419 loss: 0.503 lr: 0.000466\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 420 loss: 0.506 lr: 0.000465\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 421 loss: 0.506 lr: 0.000465\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 422 loss: 0.512 lr: 0.000464\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 423 loss: 0.506 lr: 0.000463\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 424 loss: 0.541 lr: 0.000463\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 425 loss: 0.513 lr: 0.000462\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 426 loss: 0.555 lr: 0.000462\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 427 loss: 0.527 lr: 0.000461\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 428 loss: 0.511 lr: 0.000460\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 429 loss: 0.506 lr: 0.000460\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 430 loss: 0.506 lr: 0.000459\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 431 loss: 0.511 lr: 0.000459\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 432 loss: 0.535 lr: 0.000458\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 433 loss: 0.508 lr: 0.000457\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 434 loss: 0.502 lr: 0.000457\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 435 loss: 0.506 lr: 0.000456\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 436 loss: 0.505 lr: 0.000455\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 437 loss: 0.505 lr: 0.000455\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 438 loss: 0.507 lr: 0.000454\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 439 loss: 0.505 lr: 0.000454\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 440 loss: 0.504 lr: 0.000453\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 441 loss: 0.508 lr: 0.000452\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 442 loss: 0.508 lr: 0.000452\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 443 loss: 0.508 lr: 0.000451\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 444 loss: 0.500 lr: 0.000450\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 445 loss: 0.507 lr: 0.000450\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 446 loss: 0.509 lr: 0.000449\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 447 loss: 0.499 lr: 0.000448\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 448 loss: 0.507 lr: 0.000448\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 449 loss: 0.505 lr: 0.000447\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 450 loss: 0.505 lr: 0.000446\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 451 loss: 0.500 lr: 0.000445\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 452 loss: 0.504 lr: 0.000445\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 453 loss: 0.525 lr: 0.000444\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 454 loss: 0.520 lr: 0.000443\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 455 loss: 0.511 lr: 0.000443\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 456 loss: 0.547 lr: 0.000442\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 457 loss: 0.536 lr: 0.000441\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 458 loss: 0.513 lr: 0.000440\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 459 loss: 0.508 lr: 0.000440\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 460 loss: 0.506 lr: 0.000439\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 461 loss: 0.504 lr: 0.000438\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 462 loss: 0.503 lr: 0.000438\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 463 loss: 0.503 lr: 0.000437\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 464 loss: 0.507 lr: 0.000436\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 465 loss: 0.504 lr: 0.000435\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 466 loss: 0.517 lr: 0.000435\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 467 loss: 0.505 lr: 0.000434\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 468 loss: 0.507 lr: 0.000433\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 469 loss: 0.507 lr: 0.000432\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 470 loss: 0.506 lr: 0.000431\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 471 loss: 0.504 lr: 0.000431\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 472 loss: 0.506 lr: 0.000430\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 473 loss: 0.504 lr: 0.000429\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 474 loss: 0.536 lr: 0.000428\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 475 loss: 0.503 lr: 0.000428\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 476 loss: 0.507 lr: 0.000427\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 477 loss: 0.503 lr: 0.000426\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 478 loss: 0.506 lr: 0.000425\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 479 loss: 0.501 lr: 0.000424\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 480 loss: 0.502 lr: 0.000424\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 481 loss: 0.504 lr: 0.000423\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 482 loss: 0.502 lr: 0.000422\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 483 loss: 0.504 lr: 0.000421\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 484 loss: 0.499 lr: 0.000420\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 485 loss: 0.503 lr: 0.000419\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 486 loss: 0.503 lr: 0.000419\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 487 loss: 0.502 lr: 0.000418\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 488 loss: 0.507 lr: 0.000417\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 489 loss: 0.505 lr: 0.000416\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 490 loss: 0.550 lr: 0.000415\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 491 loss: 0.510 lr: 0.000414\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 492 loss: 0.507 lr: 0.000414\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 493 loss: 0.500 lr: 0.000413\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 494 loss: 0.500 lr: 0.000412\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 495 loss: 0.515 lr: 0.000411\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 496 loss: 0.508 lr: 0.000410\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 497 loss: 0.500 lr: 0.000409\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 498 loss: 0.507 lr: 0.000408\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 499 loss: 0.527 lr: 0.000408\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 500 loss: 0.501 lr: 0.000407\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 501 loss: 0.503 lr: 0.000406\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 502 loss: 0.502 lr: 0.000405\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 503 loss: 0.509 lr: 0.000404\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 504 loss: 0.508 lr: 0.000403\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 505 loss: 0.502 lr: 0.000402\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 506 loss: 0.501 lr: 0.000401\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 507 loss: 0.497 lr: 0.000401\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 508 loss: 0.501 lr: 0.000400\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 509 loss: 0.502 lr: 0.000399\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 510 loss: 0.504 lr: 0.000398\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 511 loss: 0.499 lr: 0.000397\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 512 loss: 0.518 lr: 0.000396\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 513 loss: 0.550 lr: 0.000395\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 514 loss: 0.509 lr: 0.000394\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 515 loss: 0.517 lr: 0.000393\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 516 loss: 0.506 lr: 0.000392\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 517 loss: 0.513 lr: 0.000391\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 518 loss: 0.500 lr: 0.000391\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 519 loss: 0.503 lr: 0.000390\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 520 loss: 0.498 lr: 0.000389\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 521 loss: 0.506 lr: 0.000388\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 522 loss: 0.503 lr: 0.000387\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 523 loss: 0.503 lr: 0.000386\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 524 loss: 0.505 lr: 0.000385\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 525 loss: 0.501 lr: 0.000384\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 526 loss: 0.500 lr: 0.000383\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 527 loss: 0.503 lr: 0.000382\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 528 loss: 0.503 lr: 0.000381\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 529 loss: 0.502 lr: 0.000380\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 530 loss: 0.510 lr: 0.000379\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 531 loss: 0.513 lr: 0.000378\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 532 loss: 0.517 lr: 0.000377\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 533 loss: 0.519 lr: 0.000376\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 534 loss: 0.505 lr: 0.000375\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 535 loss: 0.502 lr: 0.000374\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 536 loss: 0.501 lr: 0.000373\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 537 loss: 0.505 lr: 0.000372\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 538 loss: 0.505 lr: 0.000371\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 539 loss: 0.561 lr: 0.000370\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 540 loss: 0.519 lr: 0.000369\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 541 loss: 0.510 lr: 0.000368\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 542 loss: 0.511 lr: 0.000367\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 543 loss: 0.507 lr: 0.000366\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 544 loss: 0.511 lr: 0.000365\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 545 loss: 0.503 lr: 0.000364\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 546 loss: 0.502 lr: 0.000363\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 547 loss: 0.495 lr: 0.000362\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 548 loss: 0.509 lr: 0.000361\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 549 loss: 0.501 lr: 0.000360\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 550 loss: 0.500 lr: 0.000359\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 551 loss: 0.502 lr: 0.000358\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 552 loss: 0.519 lr: 0.000357\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 553 loss: 0.507 lr: 0.000356\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 554 loss: 0.502 lr: 0.000355\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 555 loss: 0.503 lr: 0.000354\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 556 loss: 0.503 lr: 0.000353\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 557 loss: 0.511 lr: 0.000352\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 558 loss: 0.500 lr: 0.000351\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 559 loss: 0.501 lr: 0.000350\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 560 loss: 0.499 lr: 0.000349\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 561 loss: 0.499 lr: 0.000348\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 562 loss: 0.501 lr: 0.000347\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 563 loss: 0.502 lr: 0.000346\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 564 loss: 0.498 lr: 0.000345\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 565 loss: 0.499 lr: 0.000344\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 566 loss: 0.499 lr: 0.000343\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 567 loss: 0.498 lr: 0.000342\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 568 loss: 0.540 lr: 0.000341\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 569 loss: 0.512 lr: 0.000340\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 570 loss: 0.510 lr: 0.000339\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 571 loss: 0.508 lr: 0.000338\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 572 loss: 0.508 lr: 0.000337\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 573 loss: 0.506 lr: 0.000336\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 574 loss: 0.503 lr: 0.000335\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 575 loss: 0.503 lr: 0.000334\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 576 loss: 0.504 lr: 0.000333\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 577 loss: 0.500 lr: 0.000331\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 578 loss: 0.503 lr: 0.000330\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 579 loss: 0.501 lr: 0.000329\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 580 loss: 0.498 lr: 0.000328\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 581 loss: 0.501 lr: 0.000327\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 582 loss: 0.505 lr: 0.000326\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 583 loss: 0.500 lr: 0.000325\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 584 loss: 0.499 lr: 0.000324\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 585 loss: 0.500 lr: 0.000323\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 586 loss: 0.499 lr: 0.000322\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 587 loss: 0.503 lr: 0.000321\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 588 loss: 0.498 lr: 0.000320\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 589 loss: 0.498 lr: 0.000319\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 590 loss: 0.502 lr: 0.000318\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 591 loss: 0.502 lr: 0.000316\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 592 loss: 0.503 lr: 0.000315\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 593 loss: 0.502 lr: 0.000314\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 594 loss: 0.502 lr: 0.000313\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 595 loss: 0.503 lr: 0.000312\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 596 loss: 0.501 lr: 0.000311\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 597 loss: 0.502 lr: 0.000310\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 598 loss: 0.494 lr: 0.000309\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 599 loss: 0.498 lr: 0.000308\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 600 loss: 0.492 lr: 0.000307\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 601 loss: 0.500 lr: 0.000306\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 602 loss: 0.499 lr: 0.000305\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 603 loss: 0.500 lr: 0.000303\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 604 loss: 0.500 lr: 0.000302\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 605 loss: 0.496 lr: 0.000301\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 606 loss: 0.498 lr: 0.000300\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 607 loss: 0.495 lr: 0.000299\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 608 loss: 0.503 lr: 0.000298\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 609 loss: 0.500 lr: 0.000297\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 610 loss: 0.507 lr: 0.000296\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 611 loss: 0.514 lr: 0.000295\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 612 loss: 0.502 lr: 0.000294\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 613 loss: 0.500 lr: 0.000292\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 614 loss: 0.494 lr: 0.000291\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 615 loss: 0.496 lr: 0.000290\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 616 loss: 0.502 lr: 0.000289\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 617 loss: 0.499 lr: 0.000288\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 618 loss: 0.496 lr: 0.000287\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 619 loss: 0.503 lr: 0.000286\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 620 loss: 0.497 lr: 0.000285\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 621 loss: 0.498 lr: 0.000284\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 622 loss: 0.538 lr: 0.000282\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 623 loss: 0.531 lr: 0.000281\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 624 loss: 0.507 lr: 0.000280\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 625 loss: 0.507 lr: 0.000279\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 626 loss: 0.503 lr: 0.000278\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 627 loss: 0.497 lr: 0.000277\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 628 loss: 0.506 lr: 0.000276\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 629 loss: 0.498 lr: 0.000275\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 630 loss: 0.499 lr: 0.000274\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 631 loss: 0.492 lr: 0.000272\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 632 loss: 0.497 lr: 0.000271\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 633 loss: 0.498 lr: 0.000270\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 634 loss: 0.495 lr: 0.000269\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 635 loss: 0.493 lr: 0.000268\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 636 loss: 0.496 lr: 0.000267\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 637 loss: 0.494 lr: 0.000266\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 638 loss: 0.500 lr: 0.000265\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 639 loss: 0.494 lr: 0.000263\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 640 loss: 0.491 lr: 0.000262\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 641 loss: 0.497 lr: 0.000261\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 642 loss: 0.495 lr: 0.000260\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 643 loss: 0.495 lr: 0.000259\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 644 loss: 0.520 lr: 0.000258\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 645 loss: 0.512 lr: 0.000257\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 646 loss: 0.506 lr: 0.000256\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 647 loss: 0.503 lr: 0.000254\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 648 loss: 0.497 lr: 0.000253\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 649 loss: 0.500 lr: 0.000252\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 650 loss: 0.494 lr: 0.000251\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 651 loss: 0.494 lr: 0.000250\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 652 loss: 0.492 lr: 0.000249\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 653 loss: 0.492 lr: 0.000248\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 654 loss: 0.495 lr: 0.000247\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 655 loss: 0.491 lr: 0.000245\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 656 loss: 0.491 lr: 0.000244\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 657 loss: 0.492 lr: 0.000243\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 658 loss: 0.495 lr: 0.000242\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 659 loss: 0.494 lr: 0.000241\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 660 loss: 0.493 lr: 0.000240\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 661 loss: 0.519 lr: 0.000239\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 662 loss: 0.501 lr: 0.000238\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 663 loss: 0.509 lr: 0.000237\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 664 loss: 0.501 lr: 0.000235\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 665 loss: 0.500 lr: 0.000234\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 666 loss: 0.498 lr: 0.000233\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 667 loss: 0.498 lr: 0.000232\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 668 loss: 0.501 lr: 0.000231\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 669 loss: 0.493 lr: 0.000230\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 670 loss: 0.494 lr: 0.000229\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 671 loss: 0.498 lr: 0.000228\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 672 loss: 0.491 lr: 0.000226\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 673 loss: 0.493 lr: 0.000225\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 674 loss: 0.501 lr: 0.000224\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 675 loss: 0.494 lr: 0.000223\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 676 loss: 0.490 lr: 0.000222\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 677 loss: 0.492 lr: 0.000221\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 678 loss: 0.496 lr: 0.000220\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 679 loss: 0.503 lr: 0.000219\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 680 loss: 0.489 lr: 0.000218\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 681 loss: 0.498 lr: 0.000216\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 682 loss: 0.490 lr: 0.000215\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 683 loss: 0.492 lr: 0.000214\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 684 loss: 0.494 lr: 0.000213\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 685 loss: 0.492 lr: 0.000212\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 686 loss: 0.510 lr: 0.000211\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 687 loss: 0.504 lr: 0.000210\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 688 loss: 0.495 lr: 0.000209\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 689 loss: 0.495 lr: 0.000208\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 690 loss: 0.492 lr: 0.000206\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 691 loss: 0.490 lr: 0.000205\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 692 loss: 0.490 lr: 0.000204\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 693 loss: 0.491 lr: 0.000203\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 694 loss: 0.494 lr: 0.000202\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 695 loss: 0.494 lr: 0.000201\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 696 loss: 0.495 lr: 0.000200\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 697 loss: 0.491 lr: 0.000199\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 698 loss: 0.496 lr: 0.000198\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 699 loss: 0.493 lr: 0.000197\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 700 loss: 0.497 lr: 0.000195\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 701 loss: 0.492 lr: 0.000194\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 702 loss: 0.493 lr: 0.000193\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 703 loss: 0.498 lr: 0.000192\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 704 loss: 0.493 lr: 0.000191\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 705 loss: 0.492 lr: 0.000190\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 706 loss: 0.492 lr: 0.000189\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 707 loss: 0.488 lr: 0.000188\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 708 loss: 0.489 lr: 0.000187\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 709 loss: 0.492 lr: 0.000186\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 710 loss: 0.489 lr: 0.000185\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 711 loss: 0.490 lr: 0.000183\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 712 loss: 0.491 lr: 0.000182\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 713 loss: 0.494 lr: 0.000181\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 714 loss: 0.491 lr: 0.000180\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 715 loss: 0.492 lr: 0.000179\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 716 loss: 0.490 lr: 0.000178\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 717 loss: 0.495 lr: 0.000177\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 718 loss: 0.496 lr: 0.000176\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 719 loss: 0.491 lr: 0.000175\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 720 loss: 0.495 lr: 0.000174\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 721 loss: 0.503 lr: 0.000173\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 722 loss: 0.495 lr: 0.000172\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 723 loss: 0.498 lr: 0.000171\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 724 loss: 0.491 lr: 0.000170\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 725 loss: 0.495 lr: 0.000168\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 726 loss: 0.492 lr: 0.000167\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 727 loss: 0.494 lr: 0.000166\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 728 loss: 0.495 lr: 0.000165\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 729 loss: 0.491 lr: 0.000164\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 730 loss: 0.490 lr: 0.000163\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 731 loss: 0.493 lr: 0.000162\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 732 loss: 0.494 lr: 0.000161\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 733 loss: 0.495 lr: 0.000160\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 734 loss: 0.488 lr: 0.000159\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 735 loss: 0.489 lr: 0.000158\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 736 loss: 0.488 lr: 0.000157\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 737 loss: 0.498 lr: 0.000156\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 738 loss: 0.492 lr: 0.000155\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 739 loss: 0.494 lr: 0.000154\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 740 loss: 0.495 lr: 0.000153\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 741 loss: 0.495 lr: 0.000152\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 742 loss: 0.494 lr: 0.000151\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 743 loss: 0.491 lr: 0.000150\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 744 loss: 0.493 lr: 0.000149\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 745 loss: 0.486 lr: 0.000148\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 746 loss: 0.528 lr: 0.000147\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 747 loss: 0.511 lr: 0.000146\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 748 loss: 0.503 lr: 0.000145\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 749 loss: 0.502 lr: 0.000144\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 750 loss: 0.498 lr: 0.000143\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 751 loss: 0.503 lr: 0.000142\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 752 loss: 0.496 lr: 0.000141\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 753 loss: 0.497 lr: 0.000139\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 754 loss: 0.497 lr: 0.000138\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 755 loss: 0.493 lr: 0.000137\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 756 loss: 0.489 lr: 0.000136\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 757 loss: 0.492 lr: 0.000135\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 758 loss: 0.490 lr: 0.000134\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 759 loss: 0.490 lr: 0.000133\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 760 loss: 0.491 lr: 0.000133\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 761 loss: 0.495 lr: 0.000132\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 762 loss: 0.487 lr: 0.000131\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 763 loss: 0.488 lr: 0.000130\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 764 loss: 0.487 lr: 0.000129\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 765 loss: 0.491 lr: 0.000128\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 766 loss: 0.493 lr: 0.000127\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 767 loss: 0.488 lr: 0.000126\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 768 loss: 0.507 lr: 0.000125\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 769 loss: 0.498 lr: 0.000124\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 770 loss: 0.491 lr: 0.000123\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 771 loss: 0.494 lr: 0.000122\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 772 loss: 0.488 lr: 0.000121\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 773 loss: 0.490 lr: 0.000120\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 774 loss: 0.485 lr: 0.000119\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 775 loss: 0.489 lr: 0.000118\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 776 loss: 0.492 lr: 0.000117\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 777 loss: 0.488 lr: 0.000116\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 778 loss: 0.491 lr: 0.000115\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 779 loss: 0.493 lr: 0.000114\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 780 loss: 0.488 lr: 0.000113\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 781 loss: 0.487 lr: 0.000112\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 782 loss: 0.491 lr: 0.000111\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 783 loss: 0.485 lr: 0.000110\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 784 loss: 0.490 lr: 0.000109\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 785 loss: 0.488 lr: 0.000109\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 786 loss: 0.490 lr: 0.000108\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 787 loss: 0.488 lr: 0.000107\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 788 loss: 0.490 lr: 0.000106\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 789 loss: 0.501 lr: 0.000105\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 790 loss: 0.492 lr: 0.000104\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 791 loss: 0.490 lr: 0.000103\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 792 loss: 0.487 lr: 0.000102\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 793 loss: 0.491 lr: 0.000101\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 794 loss: 0.486 lr: 0.000100\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 795 loss: 0.482 lr: 0.000099\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 796 loss: 0.493 lr: 0.000099\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 797 loss: 0.490 lr: 0.000098\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 798 loss: 0.488 lr: 0.000097\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 799 loss: 0.486 lr: 0.000096\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 800 loss: 0.487 lr: 0.000095\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 801 loss: 0.489 lr: 0.000094\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 802 loss: 0.486 lr: 0.000093\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 803 loss: 0.489 lr: 0.000092\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 804 loss: 0.491 lr: 0.000091\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 805 loss: 0.493 lr: 0.000091\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 806 loss: 0.488 lr: 0.000090\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 807 loss: 0.486 lr: 0.000089\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 808 loss: 0.489 lr: 0.000088\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 809 loss: 0.485 lr: 0.000087\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 810 loss: 0.492 lr: 0.000086\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 811 loss: 0.490 lr: 0.000085\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 812 loss: 0.486 lr: 0.000085\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 813 loss: 0.492 lr: 0.000084\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 814 loss: 0.487 lr: 0.000083\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 815 loss: 0.489 lr: 0.000082\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 816 loss: 0.488 lr: 0.000081\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 817 loss: 0.486 lr: 0.000080\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 818 loss: 0.487 lr: 0.000080\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 819 loss: 0.489 lr: 0.000079\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 820 loss: 0.483 lr: 0.000078\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 821 loss: 0.487 lr: 0.000077\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 822 loss: 0.490 lr: 0.000076\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 823 loss: 0.487 lr: 0.000076\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 824 loss: 0.485 lr: 0.000075\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 825 loss: 0.484 lr: 0.000074\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 826 loss: 0.486 lr: 0.000073\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 827 loss: 0.489 lr: 0.000072\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 828 loss: 0.486 lr: 0.000072\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 829 loss: 0.488 lr: 0.000071\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 830 loss: 0.492 lr: 0.000070\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 831 loss: 0.494 lr: 0.000069\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 832 loss: 0.488 lr: 0.000069\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 833 loss: 0.484 lr: 0.000068\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 834 loss: 0.488 lr: 0.000067\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 835 loss: 0.485 lr: 0.000066\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 836 loss: 0.487 lr: 0.000065\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 837 loss: 0.486 lr: 0.000065\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 838 loss: 0.491 lr: 0.000064\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 839 loss: 0.490 lr: 0.000063\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 840 loss: 0.490 lr: 0.000062\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 841 loss: 0.488 lr: 0.000062\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 842 loss: 0.490 lr: 0.000061\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 843 loss: 0.490 lr: 0.000060\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 844 loss: 0.487 lr: 0.000060\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 845 loss: 0.482 lr: 0.000059\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 846 loss: 0.486 lr: 0.000058\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 847 loss: 0.486 lr: 0.000057\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 848 loss: 0.489 lr: 0.000057\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 849 loss: 0.488 lr: 0.000056\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 850 loss: 0.488 lr: 0.000055\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 851 loss: 0.489 lr: 0.000055\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 852 loss: 0.489 lr: 0.000054\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 853 loss: 0.492 lr: 0.000053\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 854 loss: 0.488 lr: 0.000052\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 855 loss: 0.489 lr: 0.000052\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 856 loss: 0.489 lr: 0.000051\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 857 loss: 0.492 lr: 0.000050\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 858 loss: 0.489 lr: 0.000050\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 859 loss: 0.485 lr: 0.000049\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 860 loss: 0.482 lr: 0.000048\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 861 loss: 0.485 lr: 0.000048\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 862 loss: 0.488 lr: 0.000047\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 863 loss: 0.487 lr: 0.000046\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 864 loss: 0.488 lr: 0.000046\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 865 loss: 0.485 lr: 0.000045\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 866 loss: 0.484 lr: 0.000044\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 867 loss: 0.488 lr: 0.000044\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 868 loss: 0.487 lr: 0.000043\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 869 loss: 0.485 lr: 0.000043\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 870 loss: 0.486 lr: 0.000042\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 871 loss: 0.488 lr: 0.000041\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 872 loss: 0.485 lr: 0.000041\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 873 loss: 0.485 lr: 0.000040\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 874 loss: 0.485 lr: 0.000040\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 875 loss: 0.488 lr: 0.000039\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 876 loss: 0.491 lr: 0.000038\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 877 loss: 0.489 lr: 0.000038\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 878 loss: 0.492 lr: 0.000037\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 879 loss: 0.497 lr: 0.000037\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 880 loss: 0.487 lr: 0.000036\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 881 loss: 0.488 lr: 0.000035\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 882 loss: 0.489 lr: 0.000035\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 883 loss: 0.488 lr: 0.000034\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 884 loss: 0.487 lr: 0.000034\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 885 loss: 0.486 lr: 0.000033\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 886 loss: 0.488 lr: 0.000033\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 887 loss: 0.487 lr: 0.000032\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 888 loss: 0.485 lr: 0.000031\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 889 loss: 0.491 lr: 0.000031\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 890 loss: 0.487 lr: 0.000030\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 891 loss: 0.487 lr: 0.000030\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 892 loss: 0.487 lr: 0.000029\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 893 loss: 0.486 lr: 0.000029\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 894 loss: 0.489 lr: 0.000028\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 895 loss: 0.486 lr: 0.000028\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 896 loss: 0.484 lr: 0.000027\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 897 loss: 0.488 lr: 0.000027\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 898 loss: 0.487 lr: 0.000026\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 899 loss: 0.490 lr: 0.000026\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 900 loss: 0.491 lr: 0.000025\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 901 loss: 0.484 lr: 0.000025\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 902 loss: 0.488 lr: 0.000024\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 903 loss: 0.490 lr: 0.000024\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 904 loss: 0.489 lr: 0.000023\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 905 loss: 0.489 lr: 0.000023\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 906 loss: 0.488 lr: 0.000022\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 907 loss: 0.489 lr: 0.000022\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 908 loss: 0.486 lr: 0.000021\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 909 loss: 0.488 lr: 0.000021\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 910 loss: 0.486 lr: 0.000021\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 911 loss: 0.492 lr: 0.000020\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 912 loss: 0.483 lr: 0.000020\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 913 loss: 0.490 lr: 0.000019\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 914 loss: 0.489 lr: 0.000019\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 915 loss: 0.479 lr: 0.000018\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 916 loss: 0.488 lr: 0.000018\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 917 loss: 0.487 lr: 0.000018\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 918 loss: 0.489 lr: 0.000017\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 919 loss: 0.483 lr: 0.000017\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 920 loss: 0.484 lr: 0.000016\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 921 loss: 0.483 lr: 0.000016\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 922 loss: 0.484 lr: 0.000016\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 923 loss: 0.486 lr: 0.000015\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 924 loss: 0.489 lr: 0.000015\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 925 loss: 0.485 lr: 0.000014\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 926 loss: 0.486 lr: 0.000014\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 927 loss: 0.492 lr: 0.000014\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 928 loss: 0.486 lr: 0.000013\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 929 loss: 0.484 lr: 0.000013\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 930 loss: 0.482 lr: 0.000013\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 931 loss: 0.484 lr: 0.000012\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 932 loss: 0.489 lr: 0.000012\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 933 loss: 0.483 lr: 0.000012\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 934 loss: 0.484 lr: 0.000011\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 935 loss: 0.485 lr: 0.000011\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 936 loss: 0.491 lr: 0.000011\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 937 loss: 0.484 lr: 0.000010\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 938 loss: 0.482 lr: 0.000010\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 939 loss: 0.489 lr: 0.000010\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 940 loss: 0.494 lr: 0.000009\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 941 loss: 0.485 lr: 0.000009\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 942 loss: 0.487 lr: 0.000009\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 943 loss: 0.486 lr: 0.000008\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 944 loss: 0.489 lr: 0.000008\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 945 loss: 0.489 lr: 0.000008\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 946 loss: 0.486 lr: 0.000008\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 947 loss: 0.490 lr: 0.000007\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 948 loss: 0.491 lr: 0.000007\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 949 loss: 0.487 lr: 0.000007\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 950 loss: 0.489 lr: 0.000007\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 951 loss: 0.486 lr: 0.000006\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 952 loss: 0.487 lr: 0.000006\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 953 loss: 0.485 lr: 0.000006\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 954 loss: 0.487 lr: 0.000006\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 955 loss: 0.488 lr: 0.000005\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 956 loss: 0.486 lr: 0.000005\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 957 loss: 0.488 lr: 0.000005\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 958 loss: 0.487 lr: 0.000005\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 959 loss: 0.488 lr: 0.000004\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 960 loss: 0.484 lr: 0.000004\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 961 loss: 0.485 lr: 0.000004\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 962 loss: 0.488 lr: 0.000004\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 963 loss: 0.482 lr: 0.000004\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 964 loss: 0.488 lr: 0.000003\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 965 loss: 0.484 lr: 0.000003\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 966 loss: 0.486 lr: 0.000003\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 967 loss: 0.486 lr: 0.000003\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 968 loss: 0.487 lr: 0.000003\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 969 loss: 0.486 lr: 0.000003\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 970 loss: 0.485 lr: 0.000002\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 971 loss: 0.484 lr: 0.000002\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 972 loss: 0.486 lr: 0.000002\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 973 loss: 0.484 lr: 0.000002\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 974 loss: 0.488 lr: 0.000002\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 975 loss: 0.486 lr: 0.000002\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 976 loss: 0.490 lr: 0.000002\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 977 loss: 0.488 lr: 0.000001\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 978 loss: 0.486 lr: 0.000001\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 979 loss: 0.483 lr: 0.000001\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 980 loss: 0.489 lr: 0.000001\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 981 loss: 0.489 lr: 0.000001\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 982 loss: 0.485 lr: 0.000001\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 983 loss: 0.487 lr: 0.000001\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 984 loss: 0.490 lr: 0.000001\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 985 loss: 0.484 lr: 0.000001\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 986 loss: 0.487 lr: 0.000001\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 987 loss: 0.489 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 988 loss: 0.488 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 989 loss: 0.486 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 990 loss: 0.488 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 991 loss: 0.485 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 992 loss: 0.488 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 993 loss: 0.491 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 994 loss: 0.490 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 995 loss: 0.487 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 996 loss: 0.486 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 997 loss: 0.487 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 998 loss: 0.489 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 999 loss: 0.489 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n",
      "Epoch 1000 loss: 0.486 lr: 0.000000\n",
      "min loss:  0.4755648842879704\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "\n",
    "for epoch in range(config[\"EPOCHS\"]):\n",
    "    running_loss = 0.0\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels, weights = data\n",
    "        \n",
    "        inputs = inputs.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = feasibility_promoting_weighted_BCELoss(outputs, labels, weights, device=device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d loss: %.3f lr: %.6f' % (epoch + 1, running_loss / len(train_loader), curr_lr))\n",
    "    \n",
    "    if len(loss_list) > 0:\n",
    "        print(\"min loss: \", min(loss_list))\n",
    "        if (running_loss / len(train_loader)) < min(loss_list):\n",
    "            torch.save(net.state_dict(), \"Data/corlat_presolved/models/MLP_corlat_presolved_constraint_weighted_loss.pth\")\n",
    "            print(\"Model saved\")\n",
    "    \n",
    "    loss_list.append(running_loss / len(train_loader))\n",
    "    \n",
    "    # if training loss is lower than previous loss, save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "net = NeuralNetwork()\n",
    "net.load_state_dict(torch.load(\"Data/corlat_presolved/models/MLP_corlat_presolved_constraint_weighted_loss.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=14602, out_features=1825, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=1825, out_features=912, bias=True)\n",
       "  (fc3): Linear(in_features=912, out_features=456, bias=True)\n",
       "  (fc4): Linear(in_features=456, out_features=100, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test number of feasible solutions\n",
    "# test the model on the test set\n",
    "net.eval()\n",
    "net.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing feasibility and time needed for optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for testing feasibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def feasibility_test(batch_size: int, y_pred: npt.NDArray, test_models: List, indices: List) -> List[int]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to test the feasibility of the solution predicted by the neural network.\n",
    "    For each instance in the test set, we will relax the binary variables to continuous variables with bounds of 0 and 1, and set the value of the binary variables to the value predicted by the neural network.\n",
    "    We will then compute the IIS to find the list of violated constraints and variables.\n",
    "    The number of violated constraints is the number of non zero elements in IISConstr.\n",
    "    \n",
    "    Args:\n",
    "    batch_size (int): batch size\n",
    "    y_pred (npt.NDArray): predictions of the neural network\n",
    "    test_models (List): list of gurobi models for each instance in the test set\n",
    "    indices (List): list of indices of binary variables\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    n_violated_constraints: list of number of violated constraints for each instance in the test set    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_violated_constraints = []\n",
    "\n",
    "    # convert predictions of N_samples, N_variables to binary\n",
    "    y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "    \n",
    "    # Compute the weights for each training instance\n",
    "    for i in range(len(test_models)):\n",
    "        \n",
    "        model = test_models[i]\n",
    "        \n",
    "        modelVars = model.getVars()\n",
    "        \n",
    "        instanceBinaryIndices = indices\n",
    "\n",
    "        # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "        for j in range(len(instanceBinaryIndices)):\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "            # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "        \n",
    "        \n",
    "        # Compute the IIS to find the list of violated constraints and variables\n",
    "        try:\n",
    "            model.computeIIS()\n",
    "        except gb.GurobiError:\n",
    "            print(\"Model is feasible\")\n",
    "            n_violated_constraints.append(0)\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # get number of violated constraints\n",
    "        IISConstr = model.getAttr(\"IISConstr\", model.getConstrs())\n",
    "\n",
    "        # count number of non zero elements in IISConstr        \n",
    "        n_violated_constraints.append(np.count_nonzero(IISConstr))\n",
    "        \n",
    "    return n_violated_constraints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the gurobi test instances into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-06-02\n"
     ]
    }
   ],
   "source": [
    "test_models = []\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "model_files = pkl.load(open(\"Data/corlat_presolved/train_test_data/pickle_filenames.pkl\", \"rb\"))\n",
    "# rename from .pickle to .lp\n",
    "filename = []\n",
    "for i in range(len(model_files)):\n",
    "    filename.append(model_files[i].replace(\".pickle\", \".lp\"))\n",
    "    \n",
    "for i in range(len(test_indices)):\n",
    "    model = gb.read(\"Data/corlat_presolved/instances/\" + filename[test_indices[i]], env=gurobi_env)\n",
    "    test_models.append(model)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model objective sense:  -1\n"
     ]
    }
   ],
   "source": [
    "model.ModelSense\n",
    "# if -1, minimize, if 1, maximize\n",
    "print(\"Model objective sense: \", model.ModelSense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', '=', '=', '=', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '<', '=', '=', '=', '=', '=']\n"
     ]
    }
   ],
   "source": [
    "obj = model.getObjective()\n",
    "print(model.getAttr(\"Sense\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of violated constraints for each test instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "n_violated_constraints = []\n",
    "for i, data in enumerate(valid_loader):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # get slices of test_models according to batch size\n",
    "    len_test_models = len(test_models)\n",
    "    test_models_batch = test_models[i*batch_size: min((i+1)*batch_size, len_test_models)]\n",
    "    \n",
    "    n_violated_constraints_batch = feasibility_test(batch_size_test, outputs.detach().cpu().numpy(), test_models_batch, binary_indices)\n",
    "    \n",
    "    n_violated_constraints.append(n_violated_constraints_batch)\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# flatten n_violated_constraints\n",
    "n_violated_constraints = [item for sublist in n_violated_constraints for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of violated constraints:  2.2313624678663238\n",
      "Length of n_violated_constraints:  389\n",
      "[1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 74, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 38, 2, 2, 1, 1, 11, 1, 1, 1, 0, 1, 1, 1, 1, 1, 30, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 57, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 13, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 15, 1, 27, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 2, 1, 1, 1, 16, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 19, 1, 28, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 21, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 38, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 0, 1, 19, 1, 1, 1, 14, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 15, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 21, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of violated constraints: \", np.mean(n_violated_constraints))\n",
    "print(\"Length of n_violated_constraints: \", len(n_violated_constraints))\n",
    "print(n_violated_constraints)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate the optimization time for each instance in the test set if we use $\\color{lightblue}\\text{warm start}$ to find optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_diving_opt_time(models: List, binary_indices: List, y_pred: npt.NDArray):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate the optimization time for each instance in the test set if we use warm start to find the optimal solution.\n",
    "    For each instance in the test set, we will relax the binary variables to continuous variables with bounds of 0 and 1, and set the value of the binary variables to the value predicted by the neural network.\n",
    "    We will then compute the IIS to find the list of violated constraints and variables.\n",
    "    If the model is infeasible, we will set the bounds of the binary variables to 0 and 1, and set the starting value of the binary variables to the value predicted by the neural network.\n",
    "    We will then optimize the model and record the optimization time.\n",
    "\n",
    "    Args:\n",
    "    models: list of gurobi models for each instance in the test set\n",
    "    binary_indices: list of indices of binary variables\n",
    "    y_pred: predictions of the neural network\n",
    "    \n",
    "    Returns:\n",
    "    opt_time: list of optimization time for each instance in the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    opt_time = []\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        \n",
    "        modelVars = model.getVars()\n",
    "        \n",
    "        instanceBinaryIndices = binary_indices\n",
    "        \n",
    "        y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "        \n",
    "        # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "        for j in range(len(instanceBinaryIndices)):\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "            # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "        \n",
    "        \n",
    "        # Compute the IIS to find the list of violated constraints and variables\n",
    "        try:\n",
    "            model.computeIIS()\n",
    "            infeasible_flag = True\n",
    "        except gb.GurobiError:\n",
    "            print(\"Model is feasible\")\n",
    "            infeasible_flag = False\n",
    "            continue\n",
    "        \n",
    "        if infeasible_flag:\n",
    "            for j in range(len(instanceBinaryIndices)):\n",
    "                if modelVars[instanceBinaryIndices[j]].IISLB == 0 and modelVars[instanceBinaryIndices[j]].IISUB == 0:\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")              \n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", 0)\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", 1)\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"Start\", y_pred_binary[i, j])\n",
    "                    \n",
    "                    # else if the variable is in the IIS, \n",
    "                    # get the relaxed variable and \n",
    "                    # set the bounds to 0 and 1 for the relaxed binary variables\n",
    "                else:\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", 0)\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", 1)\n",
    "        \n",
    "        else:\n",
    "            for j in range(len(instanceBinaryIndices)):\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", 0)\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", 1)\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"Start\", y_pred_binary[i, j])\n",
    "        \n",
    "        model.Params.Threads = 1\n",
    "        model.optimize()\n",
    "        print(\"Optimization time for model \", i, \": \", model.Runtime)\n",
    "        opt_time.append(model.Runtime)\n",
    "        \n",
    "    return opt_time\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimization time for each instance in the test set if we use $\\color{lightblue}\\text{warm start}$ to find optimal solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-06-02\n",
      "Optimization time for model  0 :  0.02565288543701172\n",
      "Optimization time for model  1 :  0.8026258945465088\n",
      "Optimization time for model  2 :  8.61807894706726\n",
      "Optimization time for model  3 :  1.3193280696868896\n",
      "Optimization time for model  4 :  1.424036979675293\n",
      "Optimization time for model  5 :  0.07971501350402832\n",
      "Optimization time for model  6 :  3.008183002471924\n",
      "Optimization time for model  7 :  0.03709888458251953\n",
      "Optimization time for model  8 :  0.021116018295288086\n",
      "Optimization time for model  9 :  0.09508609771728516\n",
      "Optimization time for model  10 :  0.6641838550567627\n",
      "Optimization time for model  11 :  0.0036399364471435547\n",
      "Optimization time for model  12 :  0.9146218299865723\n",
      "Optimization time for model  13 :  0.19783592224121094\n",
      "Optimization time for model  14 :  0.08869719505310059\n",
      "Optimization time for model  15 :  0.1148231029510498\n",
      "Optimization time for model  16 :  1.8725841045379639\n",
      "Optimization time for model  17 :  0.4477691650390625\n",
      "Optimization time for model  18 :  0.7398281097412109\n",
      "Optimization time for model  19 :  1.2500710487365723\n",
      "Optimization time for model  20 :  1.1779310703277588\n",
      "Optimization time for model  21 :  0.2757081985473633\n",
      "Optimization time for model  22 :  0.10755109786987305\n",
      "Optimization time for model  23 :  0.09323501586914062\n",
      "Optimization time for model  24 :  0.1280839443206787\n",
      "Optimization time for model  25 :  0.6644749641418457\n",
      "Optimization time for model  26 :  1.9671990871429443\n",
      "Optimization time for model  27 :  1.3656249046325684\n",
      "Optimization time for model  28 :  0.17643094062805176\n",
      "Optimization time for model  29 :  0.005939006805419922\n",
      "Optimization time for model  30 :  1.2138819694519043\n",
      "Optimization time for model  31 :  0.10807108879089355\n",
      "Optimization time for model  0 :  1.1621699333190918\n",
      "Optimization time for model  1 :  0.006350994110107422\n",
      "Optimization time for model  2 :  2.415148973464966\n",
      "Optimization time for model  3 :  0.0038161277770996094\n",
      "Optimization time for model  4 :  0.07542800903320312\n",
      "Optimization time for model  5 :  3.3713979721069336\n",
      "Optimization time for model  6 :  0.02014017105102539\n",
      "Optimization time for model  7 :  0.6719849109649658\n",
      "Optimization time for model  8 :  0.00687718391418457\n",
      "Optimization time for model  9 :  4.840878963470459\n",
      "Optimization time for model  10 :  0.40941715240478516\n",
      "Optimization time for model  11 :  0.07209181785583496\n",
      "Optimization time for model  12 :  0.3697700500488281\n",
      "Optimization time for model  13 :  3.2762439250946045\n",
      "Optimization time for model  14 :  0.4058821201324463\n",
      "Optimization time for model  15 :  0.4302659034729004\n",
      "Optimization time for model  16 :  1.2430870532989502\n",
      "Optimization time for model  17 :  0.004992961883544922\n",
      "Optimization time for model  18 :  0.40020203590393066\n",
      "Optimization time for model  19 :  0.054800987243652344\n",
      "Optimization time for model  20 :  0.025285005569458008\n",
      "Optimization time for model  21 :  0.002351045608520508\n",
      "Optimization time for model  22 :  0.029679059982299805\n",
      "Optimization time for model  23 :  0.7770099639892578\n",
      "Optimization time for model  24 :  0.6959969997406006\n",
      "Optimization time for model  25 :  1.3280489444732666\n",
      "Optimization time for model  26 :  0.49256396293640137\n",
      "Optimization time for model  27 :  0.9726369380950928\n",
      "Optimization time for model  28 :  0.007266044616699219\n",
      "Optimization time for model  29 :  0.14299488067626953\n",
      "Optimization time for model  30 :  0.25641894340515137\n",
      "Optimization time for model  31 :  0.8228518962860107\n",
      "Optimization time for model  0 :  0.9278838634490967\n",
      "Optimization time for model  1 :  0.022495031356811523\n",
      "Optimization time for model  2 :  0.00622105598449707\n",
      "Optimization time for model  3 :  0.2532958984375\n",
      "Optimization time for model  4 :  7.49826979637146\n",
      "Optimization time for model  5 :  0.041769981384277344\n",
      "Optimization time for model  6 :  0.0582427978515625\n",
      "Optimization time for model  7 :  0.021994829177856445\n",
      "Optimization time for model  8 :  0.003690004348754883\n",
      "Optimization time for model  9 :  0.0032720565795898438\n",
      "Model is feasible\n",
      "Optimization time for model  11 :  0.20142006874084473\n",
      "Optimization time for model  12 :  2.8697969913482666\n",
      "Optimization time for model  13 :  0.3013489246368408\n",
      "Optimization time for model  14 :  1.3126728534698486\n",
      "Optimization time for model  15 :  0.41845703125\n",
      "Optimization time for model  16 :  0.6251699924468994\n",
      "Optimization time for model  17 :  1.4446680545806885\n",
      "Optimization time for model  18 :  0.42052698135375977\n",
      "Optimization time for model  19 :  0.45000386238098145\n",
      "Optimization time for model  20 :  0.6982359886169434\n",
      "Optimization time for model  21 :  1.4067440032958984\n",
      "Optimization time for model  22 :  0.014760017395019531\n",
      "Optimization time for model  23 :  0.09044694900512695\n",
      "Optimization time for model  24 :  6.062311172485352\n",
      "Optimization time for model  25 :  0.26912903785705566\n",
      "Optimization time for model  26 :  0.002045154571533203\n",
      "Optimization time for model  27 :  0.48971104621887207\n",
      "Optimization time for model  28 :  3.4987480640411377\n",
      "Optimization time for model  29 :  0.5181639194488525\n",
      "Optimization time for model  30 :  0.19916296005249023\n",
      "Optimization time for model  31 :  0.13051605224609375\n",
      "Optimization time for model  0 :  0.09661221504211426\n",
      "Optimization time for model  1 :  0.6276772022247314\n",
      "Optimization time for model  2 :  0.0027480125427246094\n",
      "Optimization time for model  3 :  2.0622100830078125\n",
      "Optimization time for model  4 :  0.002662181854248047\n",
      "Optimization time for model  5 :  0.022829055786132812\n",
      "Optimization time for model  6 :  0.5551259517669678\n",
      "Optimization time for model  7 :  0.4268200397491455\n",
      "Optimization time for model  8 :  0.5336718559265137\n",
      "Optimization time for model  9 :  1.353682041168213\n",
      "Optimization time for model  10 :  0.0857229232788086\n",
      "Optimization time for model  11 :  0.31963396072387695\n",
      "Optimization time for model  12 :  0.08209490776062012\n",
      "Optimization time for model  13 :  0.2236320972442627\n",
      "Optimization time for model  14 :  0.0038399696350097656\n",
      "Optimization time for model  15 :  0.08683991432189941\n",
      "Model is feasible\n",
      "Optimization time for model  17 :  1.7011778354644775\n",
      "Optimization time for model  18 :  0.278980016708374\n",
      "Optimization time for model  19 :  0.366865873336792\n",
      "Optimization time for model  20 :  0.060765981674194336\n",
      "Optimization time for model  21 :  0.17877888679504395\n",
      "Optimization time for model  22 :  0.002228975296020508\n",
      "Optimization time for model  23 :  1.808027982711792\n",
      "Optimization time for model  24 :  0.866229772567749\n",
      "Optimization time for model  25 :  0.7039601802825928\n",
      "Optimization time for model  26 :  0.011594057083129883\n",
      "Optimization time for model  27 :  8.028197050094604\n",
      "Optimization time for model  28 :  0.5446629524230957\n",
      "Optimization time for model  29 :  1.4859719276428223\n",
      "Optimization time for model  30 :  0.21053695678710938\n",
      "Optimization time for model  31 :  0.6079959869384766\n",
      "Optimization time for model  0 :  0.01622605323791504\n",
      "Optimization time for model  1 :  0.05538487434387207\n",
      "Optimization time for model  2 :  0.41936683654785156\n",
      "Optimization time for model  3 :  1.8033599853515625\n",
      "Optimization time for model  4 :  0.005652904510498047\n",
      "Optimization time for model  5 :  4.895665884017944\n",
      "Optimization time for model  6 :  1.0703251361846924\n",
      "Optimization time for model  7 :  0.003761768341064453\n",
      "Optimization time for model  8 :  0.3838980197906494\n",
      "Optimization time for model  9 :  0.14704489707946777\n",
      "Optimization time for model  10 :  2.2061660289764404\n",
      "Optimization time for model  11 :  1.0911171436309814\n",
      "Optimization time for model  12 :  0.0016570091247558594\n",
      "Optimization time for model  13 :  0.003729104995727539\n",
      "Optimization time for model  14 :  0.33468198776245117\n",
      "Optimization time for model  15 :  0.5078480243682861\n",
      "Optimization time for model  16 :  1.1533360481262207\n",
      "Optimization time for model  17 :  0.29012513160705566\n",
      "Model is feasible\n",
      "Optimization time for model  19 :  3.7432289123535156\n",
      "Optimization time for model  20 :  6.046564102172852\n",
      "Optimization time for model  21 :  5.035889148712158\n",
      "Optimization time for model  22 :  0.22199082374572754\n",
      "Optimization time for model  23 :  0.5449979305267334\n",
      "Optimization time for model  24 :  0.23959803581237793\n",
      "Optimization time for model  25 :  3.1733241081237793\n",
      "Optimization time for model  26 :  0.0036780834197998047\n",
      "Optimization time for model  27 :  1.4066340923309326\n",
      "Optimization time for model  28 :  0.09984898567199707\n",
      "Optimization time for model  29 :  3.707690954208374\n",
      "Optimization time for model  30 :  0.13403892517089844\n",
      "Optimization time for model  31 :  0.6859769821166992\n",
      "Optimization time for model  0 :  0.08094406127929688\n",
      "Optimization time for model  1 :  1.9677140712738037\n",
      "Optimization time for model  2 :  0.22381997108459473\n",
      "Optimization time for model  3 :  6.715929985046387\n",
      "Optimization time for model  4 :  1.2299561500549316\n",
      "Optimization time for model  5 :  0.10655593872070312\n",
      "Optimization time for model  6 :  0.11402297019958496\n",
      "Optimization time for model  7 :  1.1980109214782715\n",
      "Optimization time for model  8 :  0.2466728687286377\n",
      "Optimization time for model  9 :  0.1959240436553955\n",
      "Optimization time for model  10 :  0.0019919872283935547\n",
      "Optimization time for model  11 :  0.0068988800048828125\n",
      "Optimization time for model  12 :  0.030411958694458008\n",
      "Optimization time for model  13 :  0.07664990425109863\n",
      "Optimization time for model  14 :  1.1253340244293213\n",
      "Optimization time for model  15 :  0.7609100341796875\n",
      "Optimization time for model  16 :  15.355030059814453\n",
      "Optimization time for model  17 :  0.019894123077392578\n",
      "Optimization time for model  18 :  0.35092687606811523\n",
      "Optimization time for model  19 :  0.23041319847106934\n",
      "Optimization time for model  20 :  0.02701711654663086\n",
      "Optimization time for model  21 :  0.9736511707305908\n",
      "Optimization time for model  22 :  0.6925919055938721\n",
      "Optimization time for model  23 :  0.01947808265686035\n",
      "Optimization time for model  24 :  1.5528969764709473\n",
      "Optimization time for model  25 :  0.006947994232177734\n",
      "Optimization time for model  26 :  1.956200122833252\n",
      "Optimization time for model  27 :  0.2193000316619873\n",
      "Optimization time for model  28 :  0.10025620460510254\n",
      "Optimization time for model  29 :  2.076753854751587\n",
      "Model is feasible\n",
      "Optimization time for model  31 :  0.6520919799804688\n",
      "Optimization time for model  0 :  0.004918098449707031\n",
      "Optimization time for model  1 :  0.05220913887023926\n",
      "Optimization time for model  2 :  0.12150883674621582\n",
      "Optimization time for model  3 :  0.7239360809326172\n",
      "Optimization time for model  4 :  0.8253200054168701\n",
      "Optimization time for model  5 :  0.0824270248413086\n",
      "Optimization time for model  6 :  1.518730878829956\n",
      "Optimization time for model  7 :  1.0201408863067627\n",
      "Optimization time for model  8 :  0.8120548725128174\n",
      "Optimization time for model  9 :  0.07124781608581543\n",
      "Optimization time for model  10 :  4.334362030029297\n",
      "Optimization time for model  11 :  0.005608081817626953\n",
      "Optimization time for model  12 :  0.05492281913757324\n",
      "Optimization time for model  13 :  0.06918692588806152\n",
      "Optimization time for model  14 :  0.003804922103881836\n",
      "Optimization time for model  15 :  0.1495370864868164\n",
      "Optimization time for model  16 :  1.4185209274291992\n",
      "Optimization time for model  17 :  0.6330630779266357\n",
      "Optimization time for model  18 :  0.9031450748443604\n",
      "Optimization time for model  19 :  0.33692193031311035\n",
      "Optimization time for model  20 :  0.00997614860534668\n",
      "Optimization time for model  21 :  1.042297124862671\n",
      "Model is feasible\n",
      "Optimization time for model  23 :  5.613656997680664\n",
      "Optimization time for model  24 :  0.6126317977905273\n",
      "Optimization time for model  25 :  0.8850739002227783\n",
      "Optimization time for model  26 :  1.4616258144378662\n",
      "Optimization time for model  27 :  0.013092994689941406\n",
      "Model is feasible\n",
      "Optimization time for model  29 :  0.1518239974975586\n",
      "Optimization time for model  30 :  1.8424220085144043\n",
      "Optimization time for model  31 :  1.4599158763885498\n",
      "Optimization time for model  0 :  5.5539870262146\n",
      "Optimization time for model  1 :  0.22287392616271973\n",
      "Optimization time for model  2 :  3.051326036453247\n",
      "Optimization time for model  3 :  0.01739192008972168\n",
      "Optimization time for model  4 :  0.002418994903564453\n",
      "Optimization time for model  5 :  0.15454411506652832\n",
      "Optimization time for model  6 :  0.4848310947418213\n",
      "Optimization time for model  7 :  2.093003988265991\n",
      "Optimization time for model  8 :  0.027755022048950195\n",
      "Optimization time for model  9 :  0.9156410694122314\n",
      "Optimization time for model  10 :  0.019978046417236328\n",
      "Optimization time for model  11 :  0.01672077178955078\n",
      "Optimization time for model  12 :  0.22766590118408203\n",
      "Optimization time for model  13 :  4.888190031051636\n",
      "Optimization time for model  14 :  0.6398100852966309\n",
      "Optimization time for model  15 :  0.506702184677124\n",
      "Optimization time for model  16 :  1.1338930130004883\n",
      "Optimization time for model  17 :  0.4791598320007324\n",
      "Optimization time for model  18 :  2.8922641277313232\n",
      "Optimization time for model  19 :  1.8375499248504639\n",
      "Optimization time for model  20 :  0.036375999450683594\n",
      "Optimization time for model  21 :  0.5908679962158203\n",
      "Optimization time for model  22 :  0.2736811637878418\n",
      "Optimization time for model  23 :  0.30841708183288574\n",
      "Optimization time for model  24 :  2.307664155960083\n",
      "Optimization time for model  25 :  0.026433944702148438\n",
      "Optimization time for model  26 :  0.13480377197265625\n",
      "Optimization time for model  27 :  0.40457582473754883\n",
      "Optimization time for model  28 :  2.5056509971618652\n",
      "Optimization time for model  29 :  0.14893794059753418\n",
      "Optimization time for model  30 :  0.009662866592407227\n",
      "Optimization time for model  31 :  0.6563570499420166\n",
      "Optimization time for model  0 :  0.7066290378570557\n",
      "Optimization time for model  1 :  0.11385607719421387\n",
      "Optimization time for model  2 :  0.0036368370056152344\n",
      "Optimization time for model  3 :  0.04497694969177246\n",
      "Optimization time for model  4 :  6.227760076522827\n",
      "Optimization time for model  5 :  0.434006929397583\n",
      "Optimization time for model  6 :  0.06082892417907715\n",
      "Optimization time for model  7 :  0.6240460872650146\n",
      "Optimization time for model  8 :  3.1734321117401123\n",
      "Optimization time for model  9 :  1.385451078414917\n",
      "Optimization time for model  10 :  4.0681681632995605\n",
      "Optimization time for model  11 :  0.4629049301147461\n",
      "Optimization time for model  12 :  0.022747039794921875\n",
      "Optimization time for model  13 :  0.01145792007446289\n",
      "Optimization time for model  14 :  1.8656392097473145\n",
      "Optimization time for model  15 :  0.4748702049255371\n",
      "Optimization time for model  16 :  0.3482050895690918\n",
      "Optimization time for model  17 :  0.9876871109008789\n",
      "Optimization time for model  18 :  0.04151296615600586\n",
      "Optimization time for model  19 :  0.15540194511413574\n",
      "Optimization time for model  20 :  0.878350019454956\n",
      "Optimization time for model  21 :  0.12020087242126465\n",
      "Optimization time for model  22 :  0.21505999565124512\n",
      "Optimization time for model  23 :  0.053598880767822266\n",
      "Optimization time for model  24 :  0.011453866958618164\n",
      "Optimization time for model  25 :  0.7977490425109863\n",
      "Optimization time for model  26 :  2.4440619945526123\n",
      "Optimization time for model  27 :  0.23192787170410156\n",
      "Optimization time for model  28 :  0.09848594665527344\n",
      "Optimization time for model  29 :  3.2433249950408936\n",
      "Optimization time for model  30 :  2.519879102706909\n",
      "Optimization time for model  31 :  0.0404360294342041\n",
      "Optimization time for model  0 :  1.0652759075164795\n",
      "Optimization time for model  1 :  12.384612083435059\n",
      "Optimization time for model  2 :  0.5263731479644775\n",
      "Optimization time for model  3 :  0.0038728713989257812\n",
      "Optimization time for model  4 :  0.45107293128967285\n",
      "Optimization time for model  5 :  1.32362699508667\n",
      "Optimization time for model  6 :  0.0069310665130615234\n",
      "Optimization time for model  7 :  6.252964019775391\n",
      "Optimization time for model  8 :  2.6701269149780273\n",
      "Optimization time for model  9 :  0.43801021575927734\n",
      "Optimization time for model  10 :  0.10911297798156738\n",
      "Optimization time for model  11 :  2.1100008487701416\n",
      "Model is feasible\n",
      "Optimization time for model  13 :  0.7661120891571045\n",
      "Optimization time for model  14 :  0.6292440891265869\n",
      "Optimization time for model  15 :  0.48308682441711426\n",
      "Optimization time for model  16 :  0.10652494430541992\n",
      "Optimization time for model  17 :  0.17847704887390137\n",
      "Optimization time for model  18 :  2.1751301288604736\n",
      "Optimization time for model  19 :  0.0066220760345458984\n",
      "Optimization time for model  20 :  0.08774614334106445\n",
      "Optimization time for model  21 :  0.0068819522857666016\n",
      "Optimization time for model  22 :  6.1316869258880615\n",
      "Optimization time for model  23 :  0.8005709648132324\n",
      "Optimization time for model  24 :  0.3970470428466797\n",
      "Optimization time for model  25 :  0.010204076766967773\n",
      "Optimization time for model  26 :  0.024796009063720703\n",
      "Optimization time for model  27 :  8.247819185256958\n",
      "Optimization time for model  28 :  0.28810596466064453\n",
      "Optimization time for model  29 :  0.7830989360809326\n",
      "Optimization time for model  30 :  3.6213409900665283\n",
      "Optimization time for model  31 :  0.07096004486083984\n",
      "Optimization time for model  0 :  1.5622060298919678\n",
      "Optimization time for model  1 :  0.0018739700317382812\n",
      "Optimization time for model  2 :  0.1912240982055664\n",
      "Optimization time for model  3 :  0.04055500030517578\n",
      "Optimization time for model  4 :  1.3539581298828125\n",
      "Optimization time for model  5 :  0.20886802673339844\n",
      "Optimization time for model  6 :  0.04400014877319336\n",
      "Optimization time for model  7 :  3.0395760536193848\n",
      "Optimization time for model  8 :  1.2422850131988525\n",
      "Optimization time for model  9 :  0.2367539405822754\n",
      "Optimization time for model  10 :  0.003607034683227539\n",
      "Optimization time for model  11 :  3.0499370098114014\n",
      "Optimization time for model  12 :  0.07652807235717773\n",
      "Optimization time for model  13 :  0.0019419193267822266\n",
      "Optimization time for model  14 :  8.257001161575317\n",
      "Optimization time for model  15 :  1.5334892272949219\n",
      "Optimization time for model  16 :  0.37053990364074707\n",
      "Optimization time for model  17 :  0.7191319465637207\n",
      "Optimization time for model  18 :  1.2396180629730225\n",
      "Optimization time for model  19 :  0.10555005073547363\n",
      "Model is feasible\n",
      "Optimization time for model  21 :  0.569242000579834\n",
      "Optimization time for model  22 :  0.29787588119506836\n",
      "Optimization time for model  23 :  0.42054009437561035\n",
      "Model is feasible\n",
      "Optimization time for model  25 :  0.01718616485595703\n",
      "Optimization time for model  26 :  0.5502071380615234\n",
      "Optimization time for model  27 :  0.011175155639648438\n",
      "Optimization time for model  28 :  0.4423079490661621\n",
      "Optimization time for model  29 :  0.08014488220214844\n",
      "Optimization time for model  30 :  2.4080100059509277\n",
      "Optimization time for model  31 :  0.34988880157470703\n",
      "Optimization time for model  0 :  1.1838719844818115\n",
      "Optimization time for model  1 :  0.1264488697052002\n",
      "Optimization time for model  2 :  0.0778050422668457\n",
      "Optimization time for model  3 :  1.3194689750671387\n",
      "Optimization time for model  4 :  0.0019099712371826172\n",
      "Optimization time for model  5 :  0.5380048751831055\n",
      "Optimization time for model  6 :  0.2633018493652344\n",
      "Optimization time for model  7 :  0.9047789573669434\n",
      "Optimization time for model  8 :  0.03240799903869629\n",
      "Optimization time for model  9 :  0.6132159233093262\n",
      "Optimization time for model  10 :  0.020064115524291992\n",
      "Optimization time for model  11 :  2.3215320110321045\n",
      "Optimization time for model  12 :  0.6081399917602539\n",
      "Optimization time for model  13 :  1.8127110004425049\n",
      "Optimization time for model  14 :  0.4803159236907959\n",
      "Optimization time for model  15 :  0.47213196754455566\n",
      "Optimization time for model  16 :  0.7607390880584717\n",
      "Optimization time for model  17 :  0.34337306022644043\n",
      "Optimization time for model  18 :  0.006273031234741211\n",
      "Optimization time for model  19 :  1.5148108005523682\n",
      "Optimization time for model  20 :  0.6735479831695557\n",
      "Optimization time for model  21 :  0.7392089366912842\n",
      "Optimization time for model  22 :  0.0019130706787109375\n",
      "Optimization time for model  23 :  0.46405601501464844\n",
      "Optimization time for model  24 :  0.3974781036376953\n",
      "Optimization time for model  25 :  0.007361888885498047\n",
      "Optimization time for model  26 :  1.964043140411377\n",
      "Optimization time for model  27 :  0.1782090663909912\n",
      "Optimization time for model  28 :  1.3096790313720703\n",
      "Optimization time for model  29 :  0.2808799743652344\n",
      "Optimization time for model  30 :  0.005719184875488281\n",
      "Optimization time for model  31 :  0.273374080657959\n",
      "Optimization time for model  0 :  0.4253418445587158\n",
      "Optimization time for model  1 :  0.3072080612182617\n",
      "Optimization time for model  2 :  0.27764201164245605\n",
      "Optimization time for model  3 :  0.6807870864868164\n",
      "Optimization time for model  4 :  7.0227320194244385\n"
     ]
    }
   ],
   "source": [
    "test_models = []\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "model_files = pkl.load(open(\"Data/corlat_presolved/train_test_data/pickle_filenames.pkl\", \"rb\"))\n",
    "# rename from .pickle to .lp\n",
    "filename = []\n",
    "for i in range(len(model_files)):\n",
    "    filename.append(model_files[i].replace(\".pickle\", \".lp\"))\n",
    "    \n",
    "for i in range(len(test_indices)):\n",
    "    model = gb.read(\"Data/corlat_presolved/instances/\" + filename[test_indices[i]], env=gurobi_env)\n",
    "    test_models.append(model)\n",
    "    \n",
    "    \n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time = []\n",
    "for i, data in enumerate(valid_loader):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # get slices of test_models according to batch size\n",
    "    len_test_models = len(test_models)\n",
    "\n",
    "    test_models_batch = test_models[i*batch_size: min((i+1)*batch_size, len_test_models)]\n",
    "    \n",
    "    opt_time_batch = calculate_diving_opt_time(test_models_batch, binary_indices, outputs.detach().cpu().numpy())\n",
    "    \n",
    "    opt_time.append(opt_time_batch)\n",
    "    \n",
    "# save opt_time\n",
    "with open(\"Data/corlat_presolved/opt_time.pickle\", \"wb\") as f:\n",
    "    pkl.dump(opt_time, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average optimization time:  1.0126795436206617\n"
     ]
    }
   ],
   "source": [
    "# flatten opt_time\n",
    "opt_time_flat = [item for sublist in opt_time for item in sublist]\n",
    "print(\"Average optimization time: \", np.mean(opt_time_flat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimization time for each instance in the test set if we use $\\color{lightblue}\\text{equality constraint}$ to find optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_equality_constraint_opt_time(models: list, binary_indices: list, y_pred: npt.NDArray):\n",
    "    \"\"\"\n",
    "    Function to calculate the optimization time for each instance in the test set if we use equality constraint to find the optimal solution.\n",
    "    For each instance in the test set, we will relax the binary variables to continuous variables with bounds of 0 and 1, and set the value of the binary variables to the value predicted by the neural network.\n",
    "    We will then compute the IIS to find the list of violated constraints and variables.\n",
    "    If the model is infeasible, we will set the bounds of the binary variables to 0 and 1, and set the starting value of the binary variables to the value predicted by the neural network.\n",
    "    We will then optimize the model and record the optimization time.\n",
    "\n",
    "    Args:\n",
    "        models: list of gurobi models for each instance in the test set\n",
    "        binary_indices: list of indices of binary variables\n",
    "        y_pred: predictions of the neural network\n",
    "\n",
    "    Returns:\n",
    "        opt_time: list of optimization time for each instance in the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    opt_time = []\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        \n",
    "        modelVars = model.getVars()\n",
    "        \n",
    "        instanceBinaryIndices = binary_indices\n",
    "        \n",
    "        y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "        \n",
    "        # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "        for j in range(len(instanceBinaryIndices)):\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "            # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "        \n",
    "        \n",
    "        # Compute the IIS to find the list of violated constraints and variables\n",
    "        try:\n",
    "            model.computeIIS()\n",
    "            infeasible_flag = True\n",
    "        except gb.GurobiError:\n",
    "            print(\"Model is feasible\")\n",
    "            infeasible_flag = False\n",
    "            continue\n",
    "        \n",
    "        if infeasible_flag:\n",
    "            for j in range(len(instanceBinaryIndices)):\n",
    "                if modelVars[instanceBinaryIndices[j]].IISLB == 0 and modelVars[instanceBinaryIndices[j]].IISUB == 0:\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                    # for each index in binary_indices, set the value of the corresponding variable to the value predicted by model\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])                 \n",
    "                    \n",
    "                    # else if the variable is in the IIS, \n",
    "                    # get the relaxed variable and \n",
    "                    # set the bounds to 0 and 1 for the relaxed binary variables\n",
    "                else:\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", 0)\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", 1)\n",
    "        \n",
    "        else:\n",
    "            for j in range(len(instanceBinaryIndices)):\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "\n",
    "        \n",
    "        model.Params.Threads = 1\n",
    "        model.optimize()\n",
    "        print(\"Optimization time for model \", i, \": \", model.Runtime)\n",
    "        opt_time.append(model.Runtime)\n",
    "        \n",
    "    return opt_time\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equality Constraint test optimization time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-06-02\n",
      "Optimization time for model  0 :  0.00022721290588378906\n",
      "Optimization time for model  1 :  0.0001480579376220703\n",
      "Optimization time for model  2 :  0.0001850128173828125\n",
      "Optimization time for model  3 :  2.482743978500366\n",
      "Optimization time for model  4 :  0.0002071857452392578\n",
      "Optimization time for model  5 :  0.00013899803161621094\n",
      "Optimization time for model  6 :  0.00017905235290527344\n",
      "Optimization time for model  7 :  0.0001270771026611328\n",
      "Optimization time for model  8 :  0.0001277923583984375\n",
      "Optimization time for model  9 :  0.0001220703125\n",
      "Optimization time for model  10 :  0.00016999244689941406\n",
      "Optimization time for model  11 :  0.00016689300537109375\n",
      "Optimization time for model  12 :  1.0004680156707764\n",
      "Optimization time for model  13 :  0.00015020370483398438\n",
      "Optimization time for model  14 :  0.27437400817871094\n",
      "Optimization time for model  15 :  0.00019598007202148438\n",
      "Optimization time for model  16 :  2.3989410400390625\n",
      "Optimization time for model  17 :  0.0001990795135498047\n",
      "Optimization time for model  18 :  2.180712938308716\n",
      "Optimization time for model  19 :  0.00021004676818847656\n",
      "Optimization time for model  20 :  0.0001380443572998047\n",
      "Optimization time for model  21 :  0.00018095970153808594\n",
      "Optimization time for model  22 :  0.0001289844512939453\n",
      "Optimization time for model  23 :  0.0001671314239501953\n",
      "Optimization time for model  24 :  0.0031991004943847656\n",
      "Optimization time for model  25 :  0.0001800060272216797\n",
      "Optimization time for model  26 :  0.00013113021850585938\n",
      "Optimization time for model  27 :  0.0001761913299560547\n",
      "Optimization time for model  28 :  0.00017118453979492188\n",
      "Model is feasible\n",
      "Optimization time for model  30 :  0.00017690658569335938\n",
      "Optimization time for model  31 :  0.00013899803161621094\n",
      "Optimization time for model  0 :  0.00021004676818847656\n",
      "Optimization time for model  1 :  0.00018596649169921875\n",
      "Optimization time for model  2 :  0.00017404556274414062\n",
      "Optimization time for model  3 :  0.00016808509826660156\n",
      "Optimization time for model  4 :  0.00012302398681640625\n",
      "Optimization time for model  5 :  0.0001780986785888672\n",
      "Optimization time for model  6 :  0.008738040924072266\n",
      "Optimization time for model  7 :  0.00015091896057128906\n",
      "Model is feasible\n",
      "Optimization time for model  9 :  4.44206690788269\n",
      "Optimization time for model  10 :  0.15534114837646484\n",
      "Optimization time for model  11 :  0.000164031982421875\n",
      "Optimization time for model  12 :  0.00014209747314453125\n",
      "Optimization time for model  13 :  0.00021004676818847656\n",
      "Optimization time for model  14 :  0.00018787384033203125\n",
      "Optimization time for model  15 :  0.00018286705017089844\n",
      "Optimization time for model  16 :  2.2842140197753906\n",
      "Optimization time for model  17 :  0.00020599365234375\n",
      "Optimization time for model  18 :  0.49007105827331543\n",
      "Optimization time for model  19 :  0.0002040863037109375\n",
      "Optimization time for model  20 :  0.00017786026000976562\n",
      "Optimization time for model  21 :  0.0001270771026611328\n",
      "Optimization time for model  22 :  0.00012302398681640625\n",
      "Optimization time for model  23 :  0.0003039836883544922\n",
      "Optimization time for model  24 :  0.00017499923706054688\n",
      "Optimization time for model  25 :  0.0001380443572998047\n",
      "Optimization time for model  26 :  0.4153718948364258\n",
      "Optimization time for model  27 :  0.00021505355834960938\n",
      "Optimization time for model  28 :  0.00018906593322753906\n",
      "Optimization time for model  29 :  0.000125885009765625\n",
      "Optimization time for model  30 :  0.0001239776611328125\n",
      "Optimization time for model  31 :  0.0001659393310546875\n",
      "Optimization time for model  0 :  4.985518932342529\n",
      "Optimization time for model  1 :  0.00015997886657714844\n",
      "Optimization time for model  2 :  0.002875089645385742\n",
      "Optimization time for model  3 :  0.1884298324584961\n",
      "Optimization time for model  4 :  10.197344779968262\n",
      "Optimization time for model  5 :  0.0001537799835205078\n",
      "Optimization time for model  6 :  0.00014090538024902344\n",
      "Optimization time for model  7 :  0.014796972274780273\n",
      "Optimization time for model  8 :  0.00019502639770507812\n",
      "Optimization time for model  9 :  0.00012803077697753906\n",
      "Optimization time for model  10 :  0.00017595291137695312\n",
      "Optimization time for model  11 :  0.00017690658569335938\n",
      "Optimization time for model  12 :  0.0001709461212158203\n",
      "Optimization time for model  13 :  0.0001709461212158203\n",
      "Optimization time for model  14 :  0.0001900196075439453\n",
      "Optimization time for model  15 :  0.00026988983154296875\n",
      "Optimization time for model  16 :  0.00017499923706054688\n",
      "Optimization time for model  17 :  0.00012111663818359375\n",
      "Optimization time for model  18 :  0.00014400482177734375\n",
      "Optimization time for model  19 :  0.0001671314239501953\n",
      "Optimization time for model  20 :  0.0001881122589111328\n",
      "Optimization time for model  21 :  0.0001761913299560547\n",
      "Optimization time for model  22 :  0.00017118453979492188\n",
      "Optimization time for model  23 :  0.0001800060272216797\n",
      "Optimization time for model  24 :  0.00016689300537109375\n",
      "Optimization time for model  25 :  0.000164031982421875\n",
      "Optimization time for model  26 :  0.0001380443572998047\n",
      "Optimization time for model  27 :  0.0001881122589111328\n",
      "Optimization time for model  28 :  1.3897490501403809\n",
      "Optimization time for model  29 :  0.00017118453979492188\n",
      "Optimization time for model  30 :  0.0001900196075439453\n",
      "Optimization time for model  31 :  0.0001239776611328125\n",
      "Optimization time for model  0 :  0.08613395690917969\n",
      "Optimization time for model  1 :  0.00018215179443359375\n",
      "Optimization time for model  2 :  0.0001277923583984375\n",
      "Optimization time for model  3 :  0.00012302398681640625\n",
      "Optimization time for model  4 :  0.00012803077697753906\n",
      "Optimization time for model  5 :  0.0001239776611328125\n",
      "Optimization time for model  6 :  0.0001709461212158203\n",
      "Optimization time for model  7 :  0.17322993278503418\n",
      "Optimization time for model  8 :  0.5257420539855957\n",
      "Optimization time for model  9 :  2.208940029144287\n",
      "Optimization time for model  10 :  0.00014090538024902344\n",
      "Optimization time for model  11 :  0.0001780986785888672\n",
      "Optimization time for model  12 :  0.00017905235290527344\n",
      "Optimization time for model  13 :  0.00018596649169921875\n",
      "Model is feasible\n",
      "Optimization time for model  15 :  0.00020194053649902344\n",
      "Optimization time for model  16 :  0.00018715858459472656\n",
      "Optimization time for model  17 :  0.0001728534698486328\n",
      "Optimization time for model  18 :  0.0001690387725830078\n",
      "Optimization time for model  19 :  0.00016617774963378906\n",
      "Optimization time for model  20 :  0.000125885009765625\n",
      "Optimization time for model  21 :  0.00017595291137695312\n",
      "Optimization time for model  22 :  0.0001220703125\n",
      "Optimization time for model  23 :  0.00016617774963378906\n",
      "Optimization time for model  24 :  0.0001430511474609375\n",
      "Optimization time for model  25 :  0.00017189979553222656\n",
      "Optimization time for model  26 :  0.0001659393310546875\n",
      "Optimization time for model  27 :  0.00016689300537109375\n",
      "Optimization time for model  28 :  0.00016188621520996094\n",
      "Optimization time for model  29 :  0.0001659393310546875\n",
      "Optimization time for model  30 :  0.00016689300537109375\n",
      "Optimization time for model  31 :  0.0001628398895263672\n",
      "Optimization time for model  0 :  0.0002658367156982422\n",
      "Optimization time for model  1 :  0.00019502639770507812\n",
      "Optimization time for model  2 :  0.8886549472808838\n",
      "Optimization time for model  3 :  0.00019884109497070312\n",
      "Optimization time for model  4 :  0.00013589859008789062\n",
      "Optimization time for model  5 :  0.0001800060272216797\n",
      "Optimization time for model  6 :  0.0001881122589111328\n",
      "Optimization time for model  7 :  0.0001881122589111328\n",
      "Optimization time for model  8 :  0.00017881393432617188\n",
      "Optimization time for model  9 :  0.00018405914306640625\n",
      "Optimization time for model  10 :  0.00014495849609375\n",
      "Optimization time for model  11 :  0.0001659393310546875\n",
      "Optimization time for model  12 :  0.00012183189392089844\n",
      "Optimization time for model  13 :  0.00016379356384277344\n",
      "Optimization time for model  14 :  0.00016808509826660156\n",
      "Optimization time for model  15 :  0.00013399124145507812\n",
      "Optimization time for model  16 :  0.00017905235290527344\n",
      "Optimization time for model  17 :  0.00017380714416503906\n",
      "Model is feasible\n",
      "Optimization time for model  19 :  0.0001900196075439453\n",
      "Optimization time for model  20 :  0.00018095970153808594\n",
      "Optimization time for model  21 :  0.0002079010009765625\n",
      "Optimization time for model  22 :  0.00020503997802734375\n",
      "Optimization time for model  23 :  0.00019502639770507812\n",
      "Optimization time for model  24 :  0.2935168743133545\n",
      "Optimization time for model  25 :  3.478048086166382\n",
      "Model is feasible\n",
      "Optimization time for model  27 :  0.00019478797912597656\n",
      "Optimization time for model  28 :  0.0001289844512939453\n",
      "Optimization time for model  29 :  0.00017690658569335938\n",
      "Optimization time for model  30 :  0.00017404556274414062\n",
      "Optimization time for model  31 :  0.0001709461212158203\n",
      "Optimization time for model  0 :  0.0001499652862548828\n",
      "Optimization time for model  1 :  0.00020003318786621094\n",
      "Optimization time for model  2 :  0.00015997886657714844\n",
      "Optimization time for model  3 :  9.055922985076904\n",
      "Optimization time for model  4 :  0.00021195411682128906\n",
      "Optimization time for model  5 :  0.00018906593322753906\n",
      "Optimization time for model  6 :  0.0001819133758544922\n",
      "Optimization time for model  7 :  0.00016999244689941406\n",
      "Optimization time for model  8 :  0.00012493133544921875\n",
      "Optimization time for model  9 :  0.00016689300537109375\n",
      "Optimization time for model  10 :  0.000125885009765625\n",
      "Optimization time for model  11 :  0.00642704963684082\n",
      "Optimization time for model  12 :  0.0001780986785888672\n",
      "Optimization time for model  13 :  0.00013589859008789062\n",
      "Optimization time for model  14 :  0.00017905235290527344\n",
      "Optimization time for model  15 :  0.00014901161193847656\n",
      "Optimization time for model  16 :  0.00016999244689941406\n",
      "Optimization time for model  17 :  0.0001220703125\n",
      "Optimization time for model  18 :  0.0001690387725830078\n",
      "Optimization time for model  19 :  0.0001709461212158203\n",
      "Optimization time for model  20 :  0.0001227855682373047\n",
      "Optimization time for model  21 :  0.000164031982421875\n",
      "Optimization time for model  22 :  0.00014090538024902344\n",
      "Optimization time for model  23 :  0.000164031982421875\n",
      "Optimization time for model  24 :  1.2354998588562012\n",
      "Optimization time for model  25 :  0.00018787384033203125\n",
      "Optimization time for model  26 :  0.00017905235290527344\n",
      "Optimization time for model  27 :  0.00017690658569335938\n",
      "Optimization time for model  28 :  0.0001239776611328125\n",
      "Optimization time for model  29 :  0.00014591217041015625\n",
      "Optimization time for model  30 :  0.00017309188842773438\n",
      "Optimization time for model  31 :  0.00015497207641601562\n",
      "Optimization time for model  0 :  0.00017380714416503906\n",
      "Optimization time for model  1 :  0.0001380443572998047\n",
      "Optimization time for model  2 :  0.0001888275146484375\n",
      "Optimization time for model  3 :  0.00017499923706054688\n",
      "Optimization time for model  4 :  0.00017499923706054688\n",
      "Optimization time for model  5 :  0.00017499923706054688\n",
      "Optimization time for model  6 :  0.00016808509826660156\n",
      "Optimization time for model  7 :  0.000244140625\n",
      "Optimization time for model  8 :  0.00011706352233886719\n",
      "Optimization time for model  9 :  0.0001239776611328125\n",
      "Optimization time for model  10 :  15.260473012924194\n",
      "Optimization time for model  11 :  0.0002110004425048828\n",
      "Optimization time for model  12 :  0.00013899803161621094\n",
      "Optimization time for model  13 :  0.00013208389282226562\n",
      "Optimization time for model  14 :  0.00017690658569335938\n",
      "Optimization time for model  15 :  0.09217500686645508\n",
      "Optimization time for model  16 :  0.0001900196075439453\n",
      "Optimization time for model  17 :  0.00017595291137695312\n",
      "Optimization time for model  18 :  0.00017404556274414062\n",
      "Optimization time for model  19 :  0.00016999244689941406\n",
      "Optimization time for model  20 :  0.00012803077697753906\n",
      "Optimization time for model  21 :  0.00017690658569335938\n",
      "Optimization time for model  22 :  0.00017595291137695312\n",
      "Optimization time for model  23 :  0.0001690387725830078\n",
      "Optimization time for model  24 :  0.00016880035400390625\n",
      "Optimization time for model  25 :  0.0001659393310546875\n",
      "Optimization time for model  26 :  0.00017404556274414062\n",
      "Optimization time for model  27 :  0.013454914093017578\n",
      "Optimization time for model  28 :  0.00019097328186035156\n",
      "Optimization time for model  29 :  0.00012993812561035156\n",
      "Optimization time for model  30 :  0.0001819133758544922\n",
      "Optimization time for model  31 :  4.118869066238403\n",
      "Optimization time for model  0 :  0.00021505355834960938\n",
      "Optimization time for model  1 :  0.00019097328186035156\n",
      "Optimization time for model  2 :  0.00016880035400390625\n",
      "Optimization time for model  3 :  0.00017595291137695312\n",
      "Optimization time for model  4 :  0.00014209747314453125\n",
      "Optimization time for model  5 :  0.0001800060272216797\n",
      "Optimization time for model  6 :  0.00017309188842773438\n",
      "Optimization time for model  7 :  0.00016999244689941406\n",
      "Optimization time for model  8 :  0.00016498565673828125\n",
      "Optimization time for model  9 :  0.0005879402160644531\n",
      "Optimization time for model  10 :  0.00017905235290527344\n",
      "Optimization time for model  11 :  0.000125885009765625\n",
      "Optimization time for model  12 :  0.00017213821411132812\n",
      "Optimization time for model  13 :  0.00014591217041015625\n",
      "Optimization time for model  14 :  0.00017404556274414062\n",
      "Optimization time for model  15 :  0.00011992454528808594\n",
      "Optimization time for model  16 :  0.00014090538024902344\n",
      "Optimization time for model  17 :  0.9406149387359619\n",
      "Optimization time for model  18 :  0.00020003318786621094\n",
      "Optimization time for model  19 :  0.00019097328186035156\n",
      "Optimization time for model  20 :  0.030024051666259766\n",
      "Optimization time for model  21 :  1.6307578086853027\n",
      "Optimization time for model  22 :  0.0001881122589111328\n",
      "Optimization time for model  23 :  0.00017595291137695312\n",
      "Optimization time for model  24 :  0.0001709461212158203\n",
      "Optimization time for model  25 :  0.00016617774963378906\n",
      "Optimization time for model  26 :  0.13378596305847168\n",
      "Optimization time for model  27 :  0.00020503997802734375\n",
      "Optimization time for model  28 :  0.00014901161193847656\n",
      "Optimization time for model  29 :  0.00013899803161621094\n",
      "Optimization time for model  30 :  0.00018715858459472656\n",
      "Optimization time for model  31 :  0.00017595291137695312\n",
      "Optimization time for model  0 :  0.00019478797912597656\n",
      "Optimization time for model  1 :  0.00014495849609375\n",
      "Optimization time for model  2 :  0.00017905235290527344\n",
      "Optimization time for model  3 :  0.0001399517059326172\n",
      "Optimization time for model  4 :  0.00022792816162109375\n",
      "Optimization time for model  5 :  0.0001850128173828125\n",
      "Optimization time for model  6 :  0.00017309188842773438\n",
      "Optimization time for model  7 :  0.5154168605804443\n",
      "Optimization time for model  8 :  0.18373608589172363\n",
      "Optimization time for model  9 :  1.7882080078125\n",
      "Optimization time for model  10 :  0.00014591217041015625\n",
      "Optimization time for model  11 :  0.0002598762512207031\n",
      "Optimization time for model  12 :  0.00013494491577148438\n",
      "Optimization time for model  13 :  0.00017189979553222656\n",
      "Optimization time for model  14 :  0.00016808509826660156\n",
      "Optimization time for model  15 :  0.00016689300537109375\n",
      "Optimization time for model  16 :  0.00016498565673828125\n",
      "Optimization time for model  17 :  1.4246900081634521\n",
      "Optimization time for model  18 :  0.00015616416931152344\n",
      "Optimization time for model  19 :  0.000186920166015625\n",
      "Optimization time for model  20 :  0.0001780986785888672\n",
      "Optimization time for model  21 :  0.00012302398681640625\n",
      "Optimization time for model  22 :  0.0001201629638671875\n",
      "Optimization time for model  23 :  0.00018596649169921875\n",
      "Optimization time for model  24 :  0.00017404556274414062\n",
      "Optimization time for model  25 :  0.0001220703125\n",
      "Optimization time for model  26 :  7.3572001457214355\n",
      "Optimization time for model  27 :  0.0001900196075439453\n",
      "Optimization time for model  28 :  0.0001800060272216797\n",
      "Optimization time for model  29 :  0.0001671314239501953\n",
      "Optimization time for model  30 :  0.00016808509826660156\n",
      "Optimization time for model  31 :  0.00012302398681640625\n",
      "Optimization time for model  0 :  0.00026702880859375\n",
      "Optimization time for model  1 :  0.00016617774963378906\n",
      "Optimization time for model  2 :  0.00017714500427246094\n",
      "Optimization time for model  3 :  0.0001690387725830078\n",
      "Optimization time for model  4 :  0.00016689300537109375\n",
      "Optimization time for model  5 :  0.00012111663818359375\n",
      "Optimization time for model  6 :  0.0022230148315429688\n",
      "Optimization time for model  7 :  0.00013017654418945312\n",
      "Optimization time for model  8 :  0.0001819133758544922\n",
      "Optimization time for model  9 :  0.00017189979553222656\n",
      "Optimization time for model  10 :  0.0001251697540283203\n",
      "Optimization time for model  11 :  0.0001709461212158203\n",
      "Optimization time for model  12 :  0.0001671314239501953\n",
      "Optimization time for model  13 :  1.5318639278411865\n",
      "Optimization time for model  14 :  0.00019598007202148438\n",
      "Optimization time for model  15 :  0.0001800060272216797\n",
      "Optimization time for model  16 :  0.0001659393310546875\n",
      "Optimization time for model  17 :  0.00012493133544921875\n",
      "Optimization time for model  18 :  0.00018787384033203125\n",
      "Optimization time for model  19 :  0.00019812583923339844\n",
      "Optimization time for model  20 :  0.0001239776611328125\n",
      "Optimization time for model  21 :  0.0065381526947021484\n",
      "Optimization time for model  22 :  0.000186920166015625\n",
      "Optimization time for model  23 :  0.34697484970092773\n",
      "Optimization time for model  24 :  0.00019288063049316406\n",
      "Optimization time for model  25 :  0.010138988494873047\n",
      "Optimization time for model  26 :  0.0001900196075439453\n",
      "Optimization time for model  27 :  0.00017309188842773438\n",
      "Optimization time for model  28 :  0.0001220703125\n",
      "Optimization time for model  29 :  0.0001220703125\n",
      "Optimization time for model  30 :  0.0001709461212158203\n",
      "Optimization time for model  31 :  0.0001800060272216797\n",
      "Optimization time for model  0 :  0.00019884109497070312\n",
      "Optimization time for model  1 :  0.0001418590545654297\n",
      "Optimization time for model  2 :  0.19054102897644043\n",
      "Optimization time for model  3 :  0.00019788742065429688\n",
      "Optimization time for model  4 :  0.00017404556274414062\n",
      "Optimization time for model  5 :  0.0001850128173828125\n",
      "Optimization time for model  6 :  0.00017905235290527344\n",
      "Optimization time for model  7 :  0.00017905235290527344\n",
      "Optimization time for model  8 :  0.00017499923706054688\n",
      "Optimization time for model  9 :  0.16460299491882324\n",
      "Optimization time for model  10 :  0.00018095970153808594\n",
      "Optimization time for model  11 :  0.00011992454528808594\n",
      "Optimization time for model  12 :  0.000125885009765625\n",
      "Optimization time for model  13 :  0.00012493133544921875\n",
      "Optimization time for model  14 :  0.00014591217041015625\n",
      "Optimization time for model  15 :  0.00018906593322753906\n",
      "Optimization time for model  16 :  0.00017714500427246094\n",
      "Optimization time for model  17 :  0.0001270771026611328\n",
      "Optimization time for model  18 :  2.5398731231689453\n",
      "Optimization time for model  19 :  0.00016808509826660156\n",
      "Optimization time for model  20 :  0.00021219253540039062\n",
      "Optimization time for model  21 :  0.000186920166015625\n",
      "Optimization time for model  22 :  0.00017404556274414062\n",
      "Optimization time for model  23 :  1.2188098430633545\n",
      "Model is feasible\n",
      "Optimization time for model  25 :  0.0016558170318603516\n",
      "Optimization time for model  26 :  0.000186920166015625\n",
      "Optimization time for model  27 :  0.00017905235290527344\n",
      "Optimization time for model  28 :  0.00017595291137695312\n",
      "Optimization time for model  29 :  0.000125885009765625\n",
      "Optimization time for model  30 :  0.00017404556274414062\n",
      "Optimization time for model  31 :  0.00014901161193847656\n",
      "Optimization time for model  0 :  0.00019884109497070312\n",
      "Optimization time for model  1 :  0.00019502639770507812\n",
      "Optimization time for model  2 :  0.0001800060272216797\n",
      "Optimization time for model  3 :  0.00028586387634277344\n",
      "Optimization time for model  4 :  0.00019216537475585938\n",
      "Optimization time for model  5 :  0.00014495849609375\n",
      "Optimization time for model  6 :  0.0001690387725830078\n",
      "Optimization time for model  7 :  0.0001709461212158203\n",
      "Optimization time for model  8 :  0.0001239776611328125\n",
      "Optimization time for model  9 :  0.00017404556274414062\n",
      "Optimization time for model  10 :  0.00030493736267089844\n",
      "Optimization time for model  11 :  3.3061318397521973\n",
      "Optimization time for model  12 :  0.00019693374633789062\n",
      "Optimization time for model  13 :  0.0001838207244873047\n",
      "Optimization time for model  14 :  0.00017905235290527344\n",
      "Optimization time for model  15 :  1.9343159198760986\n",
      "Optimization time for model  16 :  0.00014591217041015625\n",
      "Optimization time for model  17 :  0.00018215179443359375\n",
      "Optimization time for model  18 :  0.003847837448120117\n",
      "Optimization time for model  19 :  0.00018596649169921875\n",
      "Optimization time for model  20 :  0.00017499923706054688\n",
      "Optimization time for model  21 :  0.00017786026000976562\n",
      "Optimization time for model  22 :  0.0001251697540283203\n",
      "Optimization time for model  23 :  0.0001220703125\n",
      "Optimization time for model  24 :  0.0001709461212158203\n",
      "Optimization time for model  25 :  0.00707697868347168\n",
      "Optimization time for model  26 :  0.00018405914306640625\n",
      "Optimization time for model  27 :  0.0001499652862548828\n",
      "Optimization time for model  28 :  2.463463068008423\n",
      "Optimization time for model  29 :  0.00021314620971679688\n",
      "Optimization time for model  30 :  0.00013494491577148438\n",
      "Optimization time for model  31 :  0.30609917640686035\n",
      "Optimization time for model  0 :  0.000186920166015625\n",
      "Optimization time for model  1 :  0.6151201725006104\n",
      "Optimization time for model  2 :  0.00014400482177734375\n",
      "Optimization time for model  3 :  0.0001888275146484375\n",
      "Optimization time for model  4 :  0.00017309188842773438\n"
     ]
    }
   ],
   "source": [
    "test_models = []\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "model_files = pkl.load(open(\"Data/corlat_presolved/train_test_data/pickle_filenames.pkl\", \"rb\"))\n",
    "# rename from .pickle to .lp\n",
    "filename = []\n",
    "for i in range(len(model_files)):\n",
    "    filename.append(model_files[i].replace(\".pickle\", \".lp\"))\n",
    "    \n",
    "for i in range(len(test_indices)):\n",
    "    model = gb.read(\"Data/corlat_presolved/instances/\" + filename[test_indices[i]], env=gurobi_env)\n",
    "    test_models.append(model)\n",
    "    \n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time = []\n",
    "for i, data in enumerate(valid_loader):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # get slices of test_models according to batch size\n",
    "    len_test_models = len(test_models)\n",
    "\n",
    "    test_models_batch = test_models[i*batch_size: min((i+1)*batch_size, len_test_models)]\n",
    "    \n",
    "    opt_time_batch = calculate_equality_constraint_opt_time(test_models_batch, binary_indices, outputs.detach().cpu().numpy())\n",
    "    \n",
    "    opt_time.append(opt_time_batch)\n",
    "    \n",
    "# save opt_time\n",
    "with open(\"Data/corlat_presolved/opt_time_equality_constraint.pickle\", \"wb\") as f:\n",
    "    pkl.dump(opt_time, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average optimization time:  0.2586452220189665\n"
     ]
    }
   ],
   "source": [
    "# flatten opt_time\n",
    "opt_time_flat = [item for sublist in opt_time for item in sublist]\n",
    "print(\"Average optimization time: \", np.mean(opt_time_flat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (optimization)",
   "language": "python",
   "name": "optimization"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
