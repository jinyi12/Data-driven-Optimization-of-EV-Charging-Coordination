{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CORLAT dataset to be used for Neural Network Training\n",
    "\n",
    "In this notebook, the preprocessed dataset from `preprocess_corlat.ipynb` will be loaded to be further processed into a numpy arrays suitable to be loaded in directly in a Neural Network training script.\n",
    "This notebook will output the following numpy files for training and testing:\n",
    "\n",
    "    1. `train_weights.npy`, which are the weights for the custom feasibility promoting weighted BCELoss.\n",
    "\n",
    "    2. `X_train.npy`, the input training data.\n",
    "\n",
    "    3. `X_test.npy`, the input testing data.\n",
    "\n",
    "    4. `y_train.npy`, the output target solutions.\n",
    "\n",
    "    5. `y_test.npy`, the output testing solutions.\n",
    "    \n",
    "    6. `train_idx.npy`, the indices for the corresponding gurobi model files of training.\n",
    "\n",
    "    7. `test_idx.npy`, the indices for the corresponding gurobi model files of testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch.nn.functional import relu\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = (\n",
    "        False  # Force cuDNN to use a consistent convolution algorithm\n",
    "    )\n",
    "    torch.backends.cudnn.deterministic = (\n",
    "        True  # Force cuDNN to use deterministic algorithms if available\n",
    "    )\n",
    "    torch.use_deterministic_algorithms(\n",
    "        True\n",
    "    )  # Force torch to use deterministic algorithms if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/processed_data/corlat_preprocessed.pickle\", \"rb\"))\n",
    "except:\n",
    "    # move dir to /ibm/gpfs/home/yjin0055/Project/DayAheadForecast\n",
    "    os.chdir(\"/ibm/gpfs/home/yjin0055/Project/DayAheadForecast\")\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/processed_data/corlat_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the maximum size of N_constraints and N_variables across the dataset.\n",
    "\n",
    "max_N_constraints = max(\n",
    "    len(x[\"constraint_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "max_N_variables = max(\n",
    "    len(x[\"var_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "min_N_constraints = min(\n",
    "    len(x[\"constraint_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "min_N_variables = min(\n",
    "    len(x[\"var_node_features\"]) for x in corlat_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum number of variables: \", max_N_variables)\n",
    "print(\"Maximum number of constraints: \", max_N_constraints)\n",
    "print(\"Minimum number of variables: \", min_N_variables)\n",
    "print(\"Minimum number of constraints: \", min_N_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of variable node features: \", len(corlat_dataset[0][\"var_node_features\"].columns))\n",
    "print(\"Number of constraint node features: \", len(corlat_dataset[0][\"constraint_node_features\"].columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding with 0s to match the maximum number of variables and constraints\n",
    "\n",
    "Pad with 0s to match maximum number of constraints and variables. After that stack them together to form a `numpy.NDArray`\n",
    "\n",
    "`var_node_features` will have shape of `(n_samples, max_vars, n_var_features)`\n",
    "\n",
    "`constraint_node_features` will have shape of `(n_samples, max_constraints, n_constraint_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each variable node features, pad with 0.0s to make it the same length as the maximum number of variables\n",
    "\n",
    "var_node_features = np.stack(\n",
    "    [\n",
    "        np.pad(\n",
    "            x[\"var_node_features\"].values,\n",
    "            ((0, max_N_variables - len(x[\"var_node_features\"])), (0, 0)),\n",
    "            \"constant\",\n",
    "            constant_values=0.0,\n",
    "        )\n",
    "        for x in corlat_dataset\n",
    "    ]\n",
    ")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each constraint node features, pad with 0.0s to make it the same length as the maximum number of constraints\n",
    "\n",
    "constraint_node_features = np.stack(\n",
    "    [\n",
    "        np.pad(\n",
    "            x[\"constraint_node_features\"].values,\n",
    "            ((0, max_N_constraints - len(x[\"constraint_node_features\"])), (0, 0)),\n",
    "            \"constant\",\n",
    "            constant_values=0.0,\n",
    "        )\n",
    "        for x in corlat_dataset\n",
    "    ]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_node_features and constraint_node_features, reshape to (N_samples, -1) to feed into the neural network\n",
    "var_input = var_node_features.reshape(var_node_features.shape[0], -1)\n",
    "constraint_input = constraint_node_features.reshape(constraint_node_features.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of variable features input: \", var_input.shape)\n",
    "print(\"Shape of constraint features input: \", constraint_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_input[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commented code used for generating edge index and edge attribute in COO format.\n",
    "\n",
    "$\\color{lightblue}\\text{This section of code is not used in the current paradigm of training a standard neural network.}$\n",
    "\n",
    "This section of commented code might be useful for Graph Neural Network training, where the edge index and edge attributes are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get A matrix input by stacking the csr_matrix of each sample getting shape of N_samples x (A.shape[0] x A.shape[1])\n",
    "# A_input = np.vstack([x[\"A\"] for x in corlat_dataset])\n",
    "\n",
    "# A_feature_list = []\n",
    "# for i in range(len(corlat_dataset)):\n",
    "#     n_cons = corlat_dataset[i][\"A\"].shape[0]\n",
    "\n",
    "#     # for row in range(n_vars):\n",
    "#     #     for col in range(n_cons):\n",
    "#     #         if input_dict_list[i][\"A\"][row, col] != 0:\n",
    "#     #             adj_matrix[row, n_vars + col] = input_dict_list[i][\"A\"][row, col]\n",
    "#     #             adj_matrix[n_vars + col, row] = input_dict_list[i][\"A\"][row, col]\n",
    "\n",
    "#     # get the indices and values of the non-zero elements in the sparse matrix\n",
    "#     I, J, V = scipy.sparse.find(corlat_dataset[i][\"A\"])\n",
    "\n",
    "#     # # convert to COO format\n",
    "#     edge_index = torch.stack([torch.tensor(I), torch.tensor(n_cons + J)], dim=0)\n",
    "\n",
    "#     # expand V to 2D\n",
    "#     edge_attr = torch.tensor(V).unsqueeze(1)\n",
    "\n",
    "#     tmp_dict = {\"edge_index\": edge_index, \"edge_attr\": edge_attr}\n",
    "#     A_feature_list.append(tmp_dict)\n",
    "    \n",
    "    \n",
    "# # for each sample, pad the edge_index and edge_attr to make it the same length as the maximum length of edge_index and edge_attr\n",
    "# max_edge_index_len = max([len(x[\"edge_index\"][0]) for x in A_feature_list])\n",
    "# max_edge_attr_len = max([len(x[\"edge_attr\"]) for x in A_feature_list])\n",
    "\n",
    "# for i in range(len(A_feature_list)):\n",
    "#     edge_index = A_feature_list[i][\"edge_index\"]\n",
    "#     edge_attr = A_feature_list[i][\"edge_attr\"]\n",
    "\n",
    "#     # pad edge_index\n",
    "#     edge_index = torch.cat(\n",
    "#         [\n",
    "#             edge_index,\n",
    "#             torch.zeros(\n",
    "#                 2, max_edge_index_len - len(edge_index[0]), dtype=torch.long\n",
    "#             ),\n",
    "#         ],\n",
    "#         dim=1,\n",
    "#     )\n",
    "\n",
    "#     # pad edge_attr\n",
    "#     edge_attr = torch.cat(\n",
    "#         [\n",
    "#             edge_attr,\n",
    "#             torch.zeros(\n",
    "#                 max_edge_attr_len - len(edge_attr), 1, dtype=torch.float32\n",
    "#             ),\n",
    "#         ],\n",
    "#         dim=0,\n",
    "#     )\n",
    "\n",
    "#     A_feature_list[i][\"edge_index\"] = edge_index\n",
    "#     A_feature_list[i][\"edge_attr\"] = edge_attr\n",
    "    \n",
    "# # check if the padding is correct by checking the shape of edge_index and edge_attr\n",
    "# for i in range(len(A_feature_list)):\n",
    "#     assert A_feature_list[i][\"edge_index\"].shape == (2, max_edge_index_len)\n",
    "#     assert A_feature_list[i][\"edge_attr\"].shape == (max_edge_attr_len, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset for input and targets\n",
    "\n",
    "Create the input and targets, specifically we create input dataset `X` and output targets `solutions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each solution convert the dictionary to a list of values\n",
    "solutions = [\n",
    "    list(corlat_dataset[i][\"solution\"].values())\n",
    "    if type(corlat_dataset[i][\"solution\"]) == dict\n",
    "    else corlat_dataset[i][\"solution\"]\n",
    "    for i in range(len(corlat_dataset))    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the variable features and constraint features into a single input\n",
    "X = np.hstack([var_input, constraint_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.shape[1]\n",
    "out_channels = solutions[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X and solutions to float32\n",
    "X = X.astype(np.float32)\n",
    "solutions = np.array([solution.astype(np.float32) for solution in solutions])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split to create training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split to get indices for train and test\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(solutions)), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train = X[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_train = solutions[train_idx]\n",
    "y_test = solutions[test_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create weights for feasibility promoting weighted BCELoss during training. \n",
    "\n",
    "The weights for feasibility promoting weighted BCELoss during training and testing is created. \n",
    "\n",
    "The test weights are not utilized, $\\color{lightblue}\\text{we are not utilizing the test loss as a performance metric during this experiment}$. The test loss is not calculated. Only the number of violated constraints and the performance of optimization is taken as a performance metric for now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train weights\n",
    "train_weights = []\n",
    "for i in range(len(y_train)):\n",
    "    train_weights.append(corlat_dataset[train_idx[i]][\"current_instance_weight\"])\n",
    "train_weights = np.array(train_weights)\n",
    "\n",
    "# create test weights\n",
    "test_weights = []\n",
    "for i in range(len(y_test)):\n",
    "    test_weights.append(corlat_dataset[test_idx[i]][\"current_instance_weight\"])\n",
    "test_weights = np.array(test_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy pickle_filenames from processed_data folder to the train_test_data folder\n",
    "os.system(\"cp Data/corlat/processed_data/pickle_filenames.pkl Data/corlat/train_test_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Data/corlat/train_test_data/train_weights.npy\", train_weights)\n",
    "np.save(\"Data/corlat/train_test_data/test_weights.npy\", test_weights)\n",
    "np.save(\"Data/corlat/train_test_data/X_train.npy\", X_train)\n",
    "np.save(\"Data/corlat/train_test_data/X_test.npy\", X_test)\n",
    "np.save(\"Data/corlat/train_test_data/y_train.npy\", y_train)\n",
    "np.save(\"Data/corlat/train_test_data/y_test.npy\", y_test)\n",
    "np.save(\"Data/corlat/train_test_data/train_idx.npy\", train_idx)\n",
    "np.save(\"Data/corlat/train_test_data/test_idx.npy\", test_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (optimization)",
   "language": "python",
   "name": "optimization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
