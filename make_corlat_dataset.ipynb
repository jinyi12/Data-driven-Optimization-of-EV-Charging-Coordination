{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch.nn.functional import relu\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = (\n",
    "        False  # Force cuDNN to use a consistent convolution algorithm\n",
    "    )\n",
    "    torch.backends.cudnn.deterministic = (\n",
    "        True  # Force cuDNN to use deterministic algorithms if available\n",
    "    )\n",
    "    torch.use_deterministic_algorithms(\n",
    "        True\n",
    "    )  # Force torch to use deterministic algorithms if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/processed_data/corlat_preprocessed.pickle\", \"rb\"))\n",
    "except:\n",
    "    # move dir to /ibm/gpfs/home/yjin0055/Project/DayAheadForecast\n",
    "    os.chdir(\"/ibm/gpfs/home/yjin0055/Project/DayAheadForecast\")\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/processed_data/corlat_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the maximum size of N_constraints and N_variables across the dataset.\n",
    "\n",
    "max_N_constraints = max(\n",
    "    len(x[\"constraint_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "max_N_variables = max(\n",
    "    len(x[\"var_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "min_N_constraints = min(\n",
    "    len(x[\"constraint_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "min_N_variables = min(\n",
    "    len(x[\"var_node_features\"]) for x in corlat_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of variables:  466\n",
      "Maximum number of constraints:  551\n",
      "Minimum number of variables:  466\n",
      "Minimum number of constraints:  470\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum number of variables: \", max_N_variables)\n",
    "print(\"Maximum number of constraints: \", max_N_constraints)\n",
    "print(\"Minimum number of variables: \", min_N_variables)\n",
    "print(\"Minimum number of constraints: \", min_N_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variable node features:  18\n",
      "Number of constraint node features:  10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of variable node features: \", len(corlat_dataset[0][\"var_node_features\"].columns))\n",
    "print(\"Number of constraint node features: \", len(corlat_dataset[0][\"constraint_node_features\"].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each variable node features, pad with 0.0s to make it the same length as the maximum number of variables\n",
    "\n",
    "var_node_features = np.stack(\n",
    "    [\n",
    "        np.pad(\n",
    "            x[\"var_node_features\"].values,\n",
    "            ((0, max_N_variables - len(x[\"var_node_features\"])), (0, 0)),\n",
    "            \"constant\",\n",
    "            constant_values=0.0,\n",
    "        )\n",
    "        for x in corlat_dataset\n",
    "    ]\n",
    ")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_node_features = np.stack(\n",
    "    [\n",
    "        np.pad(\n",
    "            x[\"constraint_node_features\"].values,\n",
    "            ((0, max_N_constraints - len(x[\"constraint_node_features\"])), (0, 0)),\n",
    "            \"constant\",\n",
    "            constant_values=0.0,\n",
    "        )\n",
    "        for x in corlat_dataset\n",
    "    ]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_node_features and constraint_node_features, reshape to (N_samples, -1) to feed into the neural network\n",
    "var_input = var_node_features.reshape(var_node_features.shape[0], -1)\n",
    "constraint_input = constraint_node_features.reshape(constraint_node_features.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001, 466, 18)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of variable features input:  (2001, 8388)\n",
      "Shape of constraint features input:  (2001, 5510)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of variable features input: \", var_input.shape)\n",
    "print(\"Shape of constraint features input: \", constraint_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get A matrix input by stacking the csr_matrix of each sample getting shape of N_samples x (A.shape[0] x A.shape[1])\n",
    "A_input = np.vstack([x[\"A\"] for x in corlat_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_feature_list = []\n",
    "for i in range(len(corlat_dataset)):\n",
    "    n_cons = corlat_dataset[i][\"A\"].shape[0]\n",
    "\n",
    "    # for row in range(n_vars):\n",
    "    #     for col in range(n_cons):\n",
    "    #         if input_dict_list[i][\"A\"][row, col] != 0:\n",
    "    #             adj_matrix[row, n_vars + col] = input_dict_list[i][\"A\"][row, col]\n",
    "    #             adj_matrix[n_vars + col, row] = input_dict_list[i][\"A\"][row, col]\n",
    "\n",
    "    I, J, V = scipy.sparse.find(corlat_dataset[i][\"A\"])\n",
    "    # adj_matrix[I, n_vars + J] = V\n",
    "    # adj_matrix[n_vars + J, I] = V\n",
    "\n",
    "    # # convert to COO format\n",
    "    edge_index = torch.stack([torch.tensor(I), torch.tensor(n_cons + J)], dim=0)\n",
    "\n",
    "    # expand V to 2D\n",
    "    edge_attr = torch.tensor(V).unsqueeze(1)\n",
    "\n",
    "    tmp_dict = {\"edge_index\": edge_index, \"edge_attr\": edge_attr}\n",
    "    A_feature_list.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sample, pad the edge_index and edge_attr to make it the same length as the maximum length of edge_index and edge_attr\n",
    "max_edge_index_len = max([len(x[\"edge_index\"][0]) for x in A_feature_list])\n",
    "max_edge_attr_len = max([len(x[\"edge_attr\"]) for x in A_feature_list])\n",
    "\n",
    "for i in range(len(A_feature_list)):\n",
    "    edge_index = A_feature_list[i][\"edge_index\"]\n",
    "    edge_attr = A_feature_list[i][\"edge_attr\"]\n",
    "\n",
    "    # pad edge_index\n",
    "    edge_index = torch.cat(\n",
    "        [\n",
    "            edge_index,\n",
    "            torch.zeros(\n",
    "                2, max_edge_index_len - len(edge_index[0]), dtype=torch.long\n",
    "            ),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    # pad edge_attr\n",
    "    edge_attr = torch.cat(\n",
    "        [\n",
    "            edge_attr,\n",
    "            torch.zeros(\n",
    "                max_edge_attr_len - len(edge_attr), 1, dtype=torch.float32\n",
    "            ),\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    A_feature_list[i][\"edge_index\"] = edge_index\n",
    "    A_feature_list[i][\"edge_attr\"] = edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the padding is correct by checking the shape of edge_index and edge_attr\n",
    "for i in range(len(A_feature_list)):\n",
    "    assert A_feature_list[i][\"edge_index\"].shape == (2, max_edge_index_len)\n",
    "    assert A_feature_list[i][\"edge_attr\"].shape == (max_edge_attr_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each solution convert the dictionary to a list of values\n",
    "solutions = [\n",
    "    list(corlat_dataset[i][\"solution\"].values())\n",
    "    if type(corlat_dataset[i][\"solution\"]) == dict\n",
    "    else corlat_dataset[i][\"solution\"]\n",
    "    for i in range(len(corlat_dataset))    \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the variable features and constraint features into a single input\n",
    "X = np.hstack([var_input, constraint_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.shape[1]\n",
    "out_channels = solutions[0].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3786612/1676924900.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  solutions = np.array([solution.astype(np.float32) for solution in solutions])\n"
     ]
    }
   ],
   "source": [
    "# convert X and solutions to float32\n",
    "X = X.astype(np.float32)\n",
    "solutions = np.array([solution.astype(np.float32) for solution in solutions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split to get indices for train and test\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(solutions)), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train = X[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_train = solutions[train_idx]\n",
    "y_test = solutions[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Data/corlat/train_test_data/X_train.npy\", X_train)\n",
    "np.save(\"Data/corlat/train_test_data/X_test.npy\", X_test)\n",
    "np.save(\"Data/corlat/train_test_data/y_train.npy\", y_train)\n",
    "np.save(\"Data/corlat/train_test_data/y_test.npy\", y_test)\n",
    "np.save(\"Data/corlat/train_test_data/train_idx.npy\", train_idx)\n",
    "np.save(\"Data/corlat/train_test_data/test_idx.npy\", test_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (optimization)",
   "language": "python",
   "name": "optimization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
