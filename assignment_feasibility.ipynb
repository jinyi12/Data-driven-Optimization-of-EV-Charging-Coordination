{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch.nn.functional import relu\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gurobipy as gb\n",
    "import time\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = (\n",
    "        False  # Force cuDNN to use a consistent convolution algorithm\n",
    "    )\n",
    "    torch.backends.cudnn.deterministic = (\n",
    "        True  # Force cuDNN to use deterministic algorithms if available\n",
    "    )\n",
    "    torch.use_deterministic_algorithms(\n",
    "        True\n",
    "    )  # Force torch to use deterministic algorithms if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/corlat_preprocessed.pickle\", \"rb\"))\n",
    "except:\n",
    "    # move dir to /ibm/gpfs/home/yjin0055/Project/DayAheadForecast\n",
    "    os.chdir(\"/ibm/gpfs/home/yjin0055/Project/DayAheadForecast\")\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/corlat_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each solution convert the dictionary to a list of values\n",
    "solutions = [\n",
    "    list(corlat_dataset[i][\"solution\"].values())\n",
    "    for i in range(len(corlat_dataset))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert solutions_list to numpy array\n",
    "solutions = np.array(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = os.listdir(\"instances/mip/data/COR-LAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices of the binary variables\n",
    "indices = []\n",
    "for i in range(len(corlat_dataset)):\n",
    "    indices.append(list(corlat_dataset[i][\"solution\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert indices to numpy array\n",
    "indices = np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X_train, X_test, y_train, y_test from Data/corlat/ using numpy.load\n",
    "X_train = np.load(\"Data/corlat/X_train.npy\")\n",
    "X_test = np.load(\"Data/corlat/X_test.npy\")\n",
    "y_train = np.load(\"Data/corlat/y_train.npy\")\n",
    "y_test = np.load(\"Data/corlat/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test indices\n",
    "train_indices = np.load(\"Data/corlat/train_idx.npy\")\n",
    "test_indices = np.load(\"Data/corlat/test_idx.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the xgboost model\n",
    "with open(\"Models/Tabular/xgboost_model_corlat.pkl\", \"rb\") as f:\n",
    "    xgb_model = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.9237003241685676\n",
      "Precision:  0.9232569302772111\n",
      "Recall:  0.9241441441441441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28533/3270043010.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_test = y_test.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test.astype(np.int)\n",
    "print(\"F1 score: \", f1_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xgboost model\n",
    "xgb_model.save_model(\"Models/Tabular/xgboost_model_corlat.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now test feasibility of the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Read LP format model from file instances/mip/data/COR-LAT/cor-lat-2f+r-u-10-10-10-5-100-3.462.b208.000000.prune2.lp\n",
      "Reading time = 0.01 seconds\n",
      "obj: 470 rows, 466 columns, 1751 nonzeros\n",
      "Set parameter Threads to value 1\n",
      "Gurobi Optimizer version 10.0.1 build v10.0.1rc0 (linux64)\n",
      "\n",
      "CPU model: AMD Ryzen Threadripper 1920X 12-Core Processor, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 12 physical cores, 24 logical processors, using up to 1 threads\n",
      "\n",
      "Optimize a model with 470 rows, 466 columns and 1751 nonzeros\n",
      "Model fingerprint: 0x3db7ce21\n",
      "Variable types: 366 continuous, 100 integer (100 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+02]\n",
      "  Objective range  [1e+00, 1e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+02]\n",
      "Presolve removed 17 rows and 9 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 453 rows, 457 columns, 1715 nonzeros\n",
      "Variable types: 360 continuous, 97 integer (97 binary)\n",
      "\n",
      "Root relaxation: objective 4.122460e+02, 474 iterations, 0.01 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0  412.24598    0   11          -  412.24598      -     -    0s\n",
      "     0     0  407.74433    0   17          -  407.74433      -     -    0s\n",
      "     0     0  407.74433    0   17          -  407.74433      -     -    0s\n",
      "     0     0  406.53041    0   24          -  406.53041      -     -    0s\n",
      "     0     0  405.13437    0   18          -  405.13437      -     -    0s\n",
      "     0     0  405.07234    0   17          -  405.07234      -     -    0s\n",
      "     0     0  405.07234    0   17          -  405.07234      -     -    0s\n",
      "     0     0  402.10521    0   17          -  402.10521      -     -    0s\n",
      "     0     0  401.44774    0   18          -  401.44774      -     -    0s\n",
      "     0     0  401.44592    0   16          -  401.44592      -     -    0s\n",
      "     0     0  401.07083    0   19          -  401.07083      -     -    0s\n",
      "     0     0  400.70802    0   14          -  400.70802      -     -    0s\n",
      "     0     0  400.70802    0   15          -  400.70802      -     -    0s\n",
      "     0     0  400.48464    0   16          -  400.48464      -     -    0s\n",
      "     0     0  400.47799    0   18          -  400.47799      -     -    0s\n",
      "     0     0  400.47799    0   18          -  400.47799      -     -    0s\n",
      "     0     0  399.72899    0   23          -  399.72899      -     -    0s\n",
      "     0     0  399.67979    0   21          -  399.67979      -     -    0s\n",
      "     0     0  399.67979    0   21          -  399.67979      -     -    0s\n",
      "     0     0  398.90104    0   16          -  398.90104      -     -    0s\n",
      "     0     0  398.90101    0   17          -  398.90101      -     -    0s\n",
      "     0     0  398.55711    0   24          -  398.55711      -     -    0s\n",
      "     0     0  398.50732    0   28          -  398.50732      -     -    0s\n",
      "     0     0  398.50702    0   29          -  398.50702      -     -    0s\n",
      "     0     0  398.22469    0   23          -  398.22469      -     -    0s\n",
      "     0     0  398.21664    0   20          -  398.21664      -     -    0s\n",
      "     0     0  398.21351    0   20          -  398.21351      -     -    0s\n",
      "     0     0  398.19946    0   22          -  398.19946      -     -    0s\n",
      "     0     0  398.19946    0   22          -  398.19946      -     -    0s\n",
      "H    0     0                     300.0000000  398.19946  32.7%     -    0s\n",
      "     0     2  398.15771    0   22  300.00000  398.15771  32.7%     -    0s\n",
      "H  208   179                     302.0000000  397.77318  31.7%  15.2    0s\n",
      "H  234   202                     305.0000000  397.77318  30.4%  14.0    0s\n",
      "H  312   271                     307.0000000  397.71169  29.5%  13.7    0s\n",
      "H  468   400                     314.0000000  397.62075  26.6%  13.1    0s\n",
      "H  600   479                     387.0000000  394.92000  2.05%  17.6    1s\n",
      "*  817   499              40     389.0000000  394.47921  1.41%  22.7    1s\n",
      "* 1070   403              33     391.0000000  393.89688  0.74%  22.4    2s\n",
      "* 1270   234              31     392.0000000  393.33983  0.34%  22.3    2s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 2\n",
      "  Lift-and-project: 5\n",
      "  Cover: 16\n",
      "  Implied bound: 12\n",
      "  Projected implied bound: 2\n",
      "  MIR: 30\n",
      "  StrongCG: 1\n",
      "  Flow cover: 134\n",
      "  Inf proof: 3\n",
      "  Zero half: 1\n",
      "  Network: 3\n",
      "  Relax-and-lift: 2\n",
      "\n",
      "Explored 1316 nodes (30295 simplex iterations) in 2.37 seconds (1.51 work units)\n",
      "Thread count was 1 (of 24 available processors)\n",
      "\n",
      "Solution count 9: 392 391 389 ... 300\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.920000000000e+02, best bound 3.920000000000e+02, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# basic optimization solving time\n",
    "firstInstanceTest = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[0]])\n",
    "firstInstanceTest.Params.Threads = 1\n",
    "firstInstanceTest.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read LP format model from file instances/mip/data/COR-LAT/cor-lat-2f+r-u-10-10-10-5-100-3.462.b208.000000.prune2.lp\n",
      "Reading time = 0.01 seconds\n",
      "obj: 470 rows, 466 columns, 1751 nonzeros\n",
      "Set parameter Threads to value 1\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    3.9700000e+02   2.880000e+02   0.000000e+00      0s\n",
      "\n",
      "IIS computed: 56 constraints and 73 bounds\n",
      "IIS runtime: 0.01 seconds (0.00 work units)\n",
      "Gurobi Optimizer version 10.0.1 build v10.0.1rc0 (linux64)\n",
      "\n",
      "CPU model: AMD Ryzen Threadripper 1920X 12-Core Processor, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 12 physical cores, 24 logical processors, using up to 1 threads\n",
      "\n",
      "Optimize a model with 470 rows, 466 columns and 1751 nonzeros\n",
      "Model fingerprint: 0x499154c7\n",
      "Variable types: 366 continuous, 100 integer (100 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+02]\n",
      "  Objective range  [1e+00, 1e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+02]\n",
      "\n",
      "User MIP start did not produce a new incumbent solution\n",
      "\n",
      "Presolve removed 17 rows and 9 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 453 rows, 457 columns, 1715 nonzeros\n",
      "Variable types: 360 continuous, 97 integer (97 binary)\n",
      "\n",
      "Root relaxation: objective 4.122460e+02, 474 iterations, 0.00 seconds (0.00 work units)\n",
      "Another try with MIP start\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "H    0     0                     392.0000000  412.24598  5.16%     -    0s\n",
      "     0     0  412.24598    0   11  392.00000  412.24598  5.16%     -    0s\n",
      "     0     0  407.74433    0   17  392.00000  407.74433  4.02%     -    0s\n",
      "     0     0  407.74433    0   17  392.00000  407.74433  4.02%     -    0s\n",
      "     0     0  406.53041    0   24  392.00000  406.53041  3.71%     -    0s\n",
      "     0     0  405.13437    0   18  392.00000  405.13437  3.35%     -    0s\n",
      "     0     0  405.11859    0   20  392.00000  405.11859  3.35%     -    0s\n",
      "     0     0  405.11859    0   20  392.00000  405.11859  3.35%     -    0s\n",
      "     0     0  402.67029    0   18  392.00000  402.67029  2.72%     -    0s\n",
      "     0     0  402.66658    0   18  392.00000  402.66658  2.72%     -    0s\n",
      "     0     0  402.66658    0   18  392.00000  402.66658  2.72%     -    0s\n",
      "     0     0  402.28818    0   15  392.00000  402.28818  2.62%     -    0s\n",
      "     0     0  402.28727    0   17  392.00000  402.28727  2.62%     -    0s\n",
      "     0     0  401.46451    0   17  392.00000  401.46451  2.41%     -    0s\n",
      "     0     0  401.41814    0   20  392.00000  401.41814  2.40%     -    0s\n",
      "     0     0  401.39719    0   21  392.00000  401.39719  2.40%     -    0s\n",
      "     0     0  401.39578    0   21  392.00000  401.39578  2.40%     -    0s\n",
      "     0     0  400.86970    0   20  392.00000  400.86970  2.26%     -    0s\n",
      "     0     0  400.84105    0   19  392.00000  400.84105  2.26%     -    0s\n",
      "     0     0  400.81287    0   18  392.00000  400.81287  2.25%     -    0s\n",
      "     0     0  400.81253    0   20  392.00000  400.81253  2.25%     -    0s\n",
      "     0     0  400.02344    0   19  392.00000  400.02344  2.05%     -    0s\n",
      "     0     0  399.87352    0   19  392.00000  399.87352  2.01%     -    0s\n",
      "     0     0  399.87066    0   19  392.00000  399.87066  2.01%     -    0s\n",
      "     0     0  399.22949    0   22  392.00000  399.22949  1.84%     -    0s\n",
      "     0     0  399.07770    0   22  392.00000  399.07770  1.81%     -    0s\n",
      "     0     0  399.07770    0   22  392.00000  399.07770  1.81%     -    0s\n",
      "     0     0  398.79615    0   21  392.00000  398.79615  1.73%     -    0s\n",
      "     0     0  398.78726    0   21  392.00000  398.78726  1.73%     -    0s\n",
      "     0     0  398.67684    0   21  392.00000  398.67684  1.70%     -    0s\n",
      "     0     0  398.53102    0   19  392.00000  398.53102  1.67%     -    0s\n",
      "     0     0  398.52865    0   20  392.00000  398.52865  1.67%     -    0s\n",
      "     0     0  398.50625    0   23  392.00000  398.50625  1.66%     -    0s\n",
      "     0     0  398.50625    0   23  392.00000  398.50625  1.66%     -    0s\n",
      "     0     0  398.49631    0   24  392.00000  398.49631  1.66%     -    0s\n",
      "     0     0  398.49631    0   24  392.00000  398.49631  1.66%     -    0s\n",
      "     0     2  398.47352    0   24  392.00000  398.47352  1.65%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 3\n",
      "  Cover: 14\n",
      "  Implied bound: 29\n",
      "  MIR: 30\n",
      "  StrongCG: 1\n",
      "  Flow cover: 118\n",
      "  Inf proof: 4\n",
      "  Network: 22\n",
      "  Relax-and-lift: 1\n",
      "\n",
      "Explored 444 nodes (8608 simplex iterations) in 0.55 seconds (0.29 work units)\n",
      "Thread count was 1 (of 24 available processors)\n",
      "\n",
      "Solution count 1: 392 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.920000000000e+02, best bound 3.920000000000e+02, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# we are going to use first instance of test data\n",
    "firstInstanceTest = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[0]])\n",
    "firstInstanceTest.Params.Threads = 1\n",
    "\n",
    "# get indices of binary variables\n",
    "firstInstanceTestBinaryIndices = indices[test_indices[0]]\n",
    "\n",
    "# for this first instance of test data, we are going to use the xgboost prediction and fix the binary variables' values\n",
    "# to the values predicted by xgboost\n",
    "\n",
    "# get predictions from xgboost model\n",
    "xgb_pred = xgb_model.predict(X_test[0].reshape(1, -1)).reshape(-1)\n",
    "\n",
    "# get variables from the model\n",
    "modelVars = firstInstanceTest.getVars()\n",
    "\n",
    "# need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "for i in range(len(firstInstanceTestBinaryIndices)):\n",
    "    modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "    # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "    modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"LB\", xgb_pred[i])\n",
    "    modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"UB\", xgb_pred[i])\n",
    "    \n",
    "# After relaxing or fixing the binary variables, we can compute the IIS as before\n",
    "firstInstanceTest.computeIIS()\n",
    "# # Print the conflicting variables and constraints\n",
    "# for v in firstInstanceTest.getVars():\n",
    "#   if v.IISLB > 0 or v.IISUB > 0:\n",
    "#     print(v.varName, \"is part of the IIS\")\n",
    "# for c in firstInstanceTest.getConstrs():\n",
    "#   if c.IISConstr > 0:\n",
    "#     print(c.ConstrName, \"is part of the IIS\")\n",
    "\n",
    "# only assign the predicted variables that are not in the IIS to warm start the model\n",
    "for i, v in enumerate(firstInstanceTest.getVars()):\n",
    "    if v.IISLB == 0 and v.IISUB == 0:\n",
    "        if i in firstInstanceTestBinaryIndices:\n",
    "            # print(v.varName, \"is not part of the IIS\")\n",
    "            v.setAttr(\"VType\", \"B\")\n",
    "            v.setAttr(\"LB\", 0)\n",
    "            v.setAttr(\"UB\", 1)\n",
    "            v.setAttr(\"Start\", xgb_pred[i])\n",
    "            \n",
    "    \n",
    "    # else if the variable is in the IIS, \n",
    "    # get the relaxed variable and \n",
    "    # set the bounds to 0 and 1 for the relaxed binary variables\n",
    "    else:\n",
    "        if i in firstInstanceTestBinaryIndices:\n",
    "            # print(v.varName, \"is part of the IIS\")\n",
    "            v.setAttr(\"VType\", \"B\")\n",
    "            v.setAttr(\"LB\", 0)\n",
    "            v.setAttr(\"UB\", 1)\n",
    "            \n",
    "\n",
    "firstInstanceTest.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we are going to use first instance of test data\n",
    "# firstInstanceTest = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[0]])\n",
    "# firstInstanceTest.Params.Threads = 1\n",
    "\n",
    "# # get indices of binary variables\n",
    "# firstInstanceTestBinaryIndices = indices[test_indices[0]]\n",
    "\n",
    "# # for this first instance of test data, we are going to use the xgboost prediction and fix the binary variables' values\n",
    "# # to the values predicted by xgboost\n",
    "\n",
    "# # get predictions from xgboost model\n",
    "# xgb_pred = xgb_model.predict(X_test[0].reshape(1, -1)).reshape(-1)\n",
    "\n",
    "# # get variables from the model\n",
    "# modelVars = firstInstanceTest.getVars()\n",
    "\n",
    "# # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "# for i in range(len(firstInstanceTestBinaryIndices)):\n",
    "#     modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "#     # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "#     modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"LB\", xgb_pred[i])\n",
    "#     modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"UB\", xgb_pred[i])\n",
    "    \n",
    "# # After relaxing or fixing the binary variables, we can compute the IIS as before\n",
    "# firstInstanceTest.computeIIS()\n",
    "\n",
    "# # for each constraint, if constraint in IIS, get the slack value. Implement this.\n",
    "# for c in firstInstanceTest.getConstrs():\n",
    "#     if c.IISConstr > 0:\n",
    "#         print(c.ConstrName, \"is part of the IIS\")\n",
    "#         print(c.Slack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstInstanceTest.getVars()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only assign the predicted variables that are not in the IIS to warm start the model\n",
    "# for i, v in enumerate(firstInstanceTest.getVars()):\n",
    "#     if v.IISLB == 0 and v.IISUB == 0:\n",
    "#         if i in firstInstanceTestBinaryIndices:\n",
    "#             print(v.varName, \"is not part of the IIS\")\n",
    "#             v.setAttr(\"VType\", \"B\")\n",
    "#             v.setAttr(\"LB\", 0)\n",
    "#             v.setAttr(\"UB\", 1)\n",
    "            \n",
    "    \n",
    "#     # else if the variable is in the IIS, \n",
    "#     # get the relaxed variable and \n",
    "#     # set the bounds to 0 and 1 for the relaxed binary variables\n",
    "#     else:\n",
    "#         if i in firstInstanceTestBinaryIndices:\n",
    "#             print(v.varName, \"is part of the IIS\")\n",
    "#             v.setAttr(\"VType\", \"B\")\n",
    "#             v.setAttr(\"LB\", 0)\n",
    "#             v.setAttr(\"UB\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # continue solving the model\n",
    "# firstInstanceTest.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights for each variable in the loss function should take the form of\n",
    "# w_{ij} = exp(-c_i^T x^{i, j}) / sum(exp(-c_i^T x^{i, k})) for k = 1, ..., N_i\n",
    "# where c_i is the vector of cost coefficient for training instance i, j is the index of the training instance, and N_i is the number of training instances\n",
    "\n",
    "\n",
    "# we are going to use first instance of test data\n",
    "# firstInstanceTest = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[0]])\n",
    "\n",
    "def custom_obj(model_files: list, indices, train_indices, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \n",
    "    instance_weights = []\n",
    "    \n",
    "    gurobi_env = gb.Env()\n",
    "    gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "    \n",
    "    # convert logit predictions to probabilities\n",
    "    y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "\n",
    "    # convert predictions of N_samples, N_variables to binary\n",
    "    y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "    \n",
    "    # Compute the weights for each training instance\n",
    "    for i in range(y_true.shape[0]):\n",
    "        \n",
    "        model = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[train_indices[i]], env=gurobi_env)\n",
    "        \n",
    "        modelVars = model.getVars()\n",
    "        \n",
    "        instanceBinaryIndices = indices[train_indices[i]]\n",
    "\n",
    "        # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "        for j in range(len(instanceBinaryIndices)):\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "            # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "        \n",
    "        \n",
    "        # Compute the IIS to find the list of violated constraints and variables\n",
    "        model.computeIIS()\n",
    "        \n",
    "        # Initialize the weights\n",
    "        weights = np.zeros_like(y_true[i])\n",
    "        \n",
    "        c = model.getAttr(\"Obj\", model.getVars())\n",
    "        \n",
    "        # get violated variables indices\n",
    "        violated_vars_indices = [k for k, v in enumerate(model.getVars()) if (v.IISLB > 0 or v.IISUB > 0) and k in instanceBinaryIndices]\n",
    "        \n",
    "        for j, v in enumerate(model.getVars()):\n",
    "            # not violated\n",
    "            if (v.IISLB == 0 and v.IISUB == 0) and j in instanceBinaryIndices:\n",
    "                weights[j] = np.exp(np.dot(c[j], y_pred_binary[i, j]))\n",
    "        \n",
    "                \n",
    "        denominator = sum(  np.exp(np.dot(c[k], y_pred_binary[i, k])) for k in range(y_pred_binary[i].shape[0]) if not k in violated_vars_indices  )\n",
    "        \n",
    "        weights /= denominator    \n",
    "        \n",
    "        instance_weights.append(weights)\n",
    "    \n",
    "    \n",
    "    # y_pred is of shape (N_samples, N_binary_variables)\n",
    "    # weights is of shape (N_samples, N_binary_variables)\n",
    "    # each element in weights is the weight for the corresponding element in y_pred\n",
    "    # multiply the weights by the loss\n",
    "    \n",
    "    grad = y_pred - y_true\n",
    "    hess = y_pred * (1.0 - y_pred)\n",
    "    \n",
    "    # multiply the weights by the gradient and hessian\n",
    "    instance_weights = np.array(instance_weights)\n",
    "    grad = np.multiply(grad, instance_weights)\n",
    "    hess = np.multiply(hess, instance_weights)\n",
    "    \n",
    "    grad = grad.reshape(-1, 1)\n",
    "    hess = hess.reshape(-1, 1)\n",
    "\n",
    "    return grad, hess\n",
    "\n",
    "# Define a wrapper function that takes only y_true and y_pred as arguments\n",
    "def custom_obj_wrapper_train(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    # reshape y_true to be of shape (N_samples, N_binary_variables)\n",
    "    y_true = y_true.reshape(y_pred.shape)\n",
    "    return custom_obj(model_files, indices, train_indices, y_true, y_pred)\n",
    "\n",
    "# Initialize an XGBClassifier model with the custom objective function\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbmodel = XGBClassifier(objective=custom_obj_wrapper_train, tree_method=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22975/3213858680.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_train = y_train.astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&lt;function custom_obj_wrapper_train at 0x7f417f5d3a30&gt;,\n",
       "              predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=&lt;function custom_obj_wrapper_train at 0x7f417f5d3a30&gt;,\n",
       "              predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              objective=<function custom_obj_wrapper_train at 0x7f417f5d3a30>,\n",
       "              predictor=None, ...)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.astype(np.int)\n",
    "xgbmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinyi12/miniconda3/envs/optimization/lib/python3.10/site-packages/xgboost/sklearn.py:761: UserWarning: objective is not saved in Scikit-Learn meta.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# save the feasibility constrained model\n",
    "xgbmodel.save_model(\"Models/Tabular/xgbmodel_feasibility_constrained.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original xgboost model\n",
    "xgbmodel_ori = XGBClassifier(tree_method=\"hist\")\n",
    "xgbmodel_ori.load_model(\"Models/Tabular/xgboost_model_corlat.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data and calculate average number of infeasible assignments\n",
    "y_pred = xgbmodel.predict(X_test)\n",
    "y_pred_ori = xgbmodel_ori.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "59\n",
      "72\n",
      "169\n",
      "222\n",
      "303\n",
      "400\n",
      "446\n",
      "543\n",
      "617\n",
      "645\n",
      "704\n",
      "801\n",
      "870\n",
      "892\n",
      "917\n",
      "963\n",
      "966\n",
      "978\n",
      "1008\n",
      "1039\n",
      "1122\n",
      "1201\n",
      "1275\n",
      "1372\n",
      "1469\n",
      "1508\n",
      "1577\n",
      "1591\n",
      "1637\n",
      "1733\n",
      "1822\n",
      "1860\n",
      "1957\n",
      "2054\n",
      "2060\n",
      "2078\n",
      "2170\n",
      "2239\n",
      "2306\n",
      "2365\n",
      "2387\n",
      "2415\n",
      "2429\n",
      "2458\n",
      "2555\n",
      "2611\n",
      "2675\n",
      "2700\n",
      "2746\n",
      "2838\n",
      "2928\n",
      "2969\n",
      "3066\n",
      "3152\n",
      "3159\n",
      "3229\n",
      "3315\n",
      "3321\n",
      "3418\n",
      "3515\n",
      "3551\n",
      "3640\n",
      "3654\n",
      "3751\n",
      "3795\n",
      "3890\n",
      "3987\n",
      "4061\n",
      "4158\n",
      "4250\n",
      "4273\n",
      "4362\n",
      "4410\n",
      "4492\n",
      "4574\n",
      "4588\n",
      "4673\n",
      "4770\n",
      "4778\n",
      "4875\n",
      "4890\n",
      "4965\n",
      "5062\n",
      "5159\n",
      "5163\n",
      "5212\n",
      "5302\n",
      "5399\n",
      "5482\n",
      "5576\n",
      "5602\n",
      "5632\n",
      "5703\n",
      "5714\n",
      "5811\n",
      "5845\n",
      "5929\n",
      "6011\n",
      "6108\n",
      "6194\n",
      "6291\n",
      "6299\n",
      "6396\n",
      "6478\n",
      "6569\n",
      "6666\n",
      "6693\n",
      "6790\n",
      "6808\n",
      "6905\n",
      "6983\n",
      "7070\n",
      "7150\n",
      "7185\n",
      "7253\n",
      "7321\n",
      "7335\n",
      "7351\n",
      "7386\n",
      "7483\n",
      "7575\n",
      "7671\n",
      "7718\n",
      "7811\n",
      "7886\n",
      "7982\n",
      "8062\n",
      "8159\n",
      "8256\n",
      "8312\n",
      "8409\n",
      "8474\n",
      "8571\n",
      "8587\n",
      "8684\n",
      "8781\n",
      "8878\n",
      "8914\n",
      "9011\n",
      "9098\n",
      "9159\n",
      "9256\n",
      "9260\n",
      "9357\n",
      "9431\n",
      "9434\n",
      "9531\n",
      "9574\n",
      "9659\n",
      "9740\n",
      "9749\n",
      "9780\n",
      "9838\n",
      "9904\n",
      "9998\n",
      "10095\n",
      "10122\n",
      "10212\n",
      "10231\n",
      "10290\n",
      "10378\n",
      "10397\n",
      "10458\n",
      "10522\n",
      "10558\n",
      "10626\n",
      "10713\n",
      "10810\n",
      "10892\n",
      "10989\n",
      "11072\n",
      "11159\n",
      "11249\n",
      "11275\n",
      "11320\n",
      "11387\n",
      "11418\n",
      "11515\n",
      "11610\n",
      "11663\n",
      "11760\n",
      "11764\n",
      "11841\n",
      "11931\n",
      "12028\n",
      "12078\n",
      "12129\n",
      "12226\n",
      "12323\n",
      "12419\n",
      "12505\n",
      "12592\n",
      "12689\n",
      "12772\n",
      "12866\n",
      "12963\n",
      "13006\n",
      "13103\n",
      "13199\n",
      "13259\n",
      "13267\n",
      "13337\n",
      "13432\n",
      "13527\n",
      "13595\n",
      "13667\n",
      "13682\n",
      "13689\n",
      "13762\n",
      "13859\n",
      "13948\n",
      "13951\n",
      "13954\n",
      "14009\n",
      "14084\n",
      "14181\n",
      "14202\n",
      "14299\n",
      "14368\n",
      "14465\n",
      "14476\n",
      "14569\n",
      "14666\n",
      "14733\n",
      "14781\n",
      "14818\n",
      "14852\n",
      "14910\n",
      "15007\n",
      "15091\n",
      "15188\n",
      "15285\n",
      "15360\n",
      "15449\n",
      "15471\n",
      "15568\n",
      "15665\n",
      "15689\n",
      "15757\n",
      "15854\n",
      "15862\n",
      "15954\n",
      "16021\n",
      "16030\n",
      "16127\n",
      "16176\n",
      "16273\n",
      "16296\n",
      "16357\n",
      "16395\n",
      "16492\n",
      "16541\n",
      "16638\n",
      "16730\n",
      "16795\n",
      "16833\n",
      "16930\n",
      "17016\n",
      "17113\n",
      "17210\n",
      "17295\n",
      "17309\n",
      "17392\n",
      "17444\n",
      "17468\n",
      "17531\n",
      "17628\n",
      "17725\n",
      "17808\n",
      "17809\n",
      "17896\n",
      "17993\n",
      "18005\n",
      "18084\n",
      "18095\n",
      "18136\n",
      "18206\n",
      "18234\n",
      "18320\n",
      "18378\n",
      "18455\n",
      "18552\n",
      "18564\n",
      "18614\n",
      "18711\n",
      "18737\n",
      "18834\n",
      "18879\n",
      "18882\n",
      "18915\n",
      "18924\n",
      "18960\n",
      "19030\n",
      "19067\n",
      "19096\n",
      "19193\n",
      "19220\n",
      "19262\n",
      "19270\n",
      "19298\n",
      "19310\n",
      "19381\n",
      "19435\n",
      "19493\n",
      "19576\n",
      "19595\n",
      "19639\n",
      "19720\n",
      "19791\n",
      "19821\n",
      "19840\n",
      "19903\n",
      "Average number of infeasible assignments for original xgboost model:  49.7575\n"
     ]
    }
   ],
   "source": [
    "# calculate the average number of infeasible assignments\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "n_infeasible = 0\n",
    "n_infeasible_ori = 0\n",
    "\n",
    "# Compute the weights for each training instance\n",
    "for i in range(y_pred_ori.shape[0]):\n",
    "    \n",
    "    model = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[i]], env=gurobi_env)\n",
    "    \n",
    "    modelVars = model.getVars()\n",
    "    \n",
    "    instanceBinaryIndices = indices[test_indices[i]]\n",
    "\n",
    "    # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "    for j in range(len(instanceBinaryIndices)):\n",
    "        modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "        # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "        modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_ori[i, j])\n",
    "        modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_ori[i, j])\n",
    "    \n",
    "    \n",
    "    # Compute the IIS to find the list of violated constraints and variables\n",
    "    try:\n",
    "        model.computeIIS()\n",
    "        infeasible_flag = True\n",
    "    except:\n",
    "        infeasible_flag = False\n",
    "        continue\n",
    "    \n",
    "    if infeasible_flag:\n",
    "        # count the number of violated variables\n",
    "        for j, v in enumerate(model.getVars()):\n",
    "            # violated\n",
    "            if (v.IISLB > 0 or v.IISUB > 0) and j in instanceBinaryIndices:\n",
    "                n_infeasible += 1\n",
    "        \n",
    "    print(n_infeasible)\n",
    "\n",
    "print(\"Average number of infeasible assignments for original xgboost model: \", n_infeasible / y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "Average number of infeasible assignments for constrained xgboost model:  1.0\n"
     ]
    }
   ],
   "source": [
    "# calculate the average number of infeasible assignments\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "n_infeasible = 0\n",
    "\n",
    "# Compute the weights for each training instance\n",
    "for i in range(y_pred_ori.shape[0]):\n",
    "    \n",
    "    model = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[i]], env=gurobi_env)\n",
    "    \n",
    "    modelVars = model.getVars()\n",
    "    \n",
    "    instanceBinaryIndices = indices[test_indices[i]]\n",
    "\n",
    "    # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "    for j in range(len(instanceBinaryIndices)):\n",
    "        modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "        # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "        modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred[i, j])\n",
    "        modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred[i, j])\n",
    "    \n",
    "    \n",
    "    # Compute the IIS to find the list of violated constraints and variables\n",
    "    try:\n",
    "        model.computeIIS()\n",
    "        infeasible_flag = True\n",
    "    except:\n",
    "        infeasible_flag = False\n",
    "        continue\n",
    "    \n",
    "    if infeasible_flag:\n",
    "        # count the number of violated variables\n",
    "        for j, v in enumerate(model.getVars()):\n",
    "            # violated\n",
    "            if (v.IISLB > 0 or v.IISUB > 0) and j in instanceBinaryIndices:\n",
    "                n_infeasible += 1\n",
    "        \n",
    "    print(n_infeasible)\n",
    "\n",
    "print(\"Average number of infeasible assignments for constrained xgboost model: \", n_infeasible / y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.8429579591948815\n",
      "Precision:  0.7764961805028583\n",
      "Recall:  0.9218618618618618\n"
     ]
    }
   ],
   "source": [
    "# f1, precision, recall for constrained xgboost model\n",
    "print(\"F1 score: \", f1_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred, average=\"micro\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "out_channels = solutions.shape[1]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feasibility_constrain_weights(models, indices, train_indices, y_true, y_pred):\n",
    "    \n",
    "    instance_weights = []\n",
    "    \n",
    "    # gurobi_env = gb.Env()\n",
    "    # gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "    # convert predictions of N_samples, N_variables to binary\n",
    "    y_pred_binary = torch.where(y_pred > 0.5, 1, 0)\n",
    "    \n",
    "    # Compute the weights for each training instance\n",
    "    \n",
    "    for i in range(y_true.shape[0]):\n",
    "        \n",
    "        # t1 = time.time()\n",
    "        model = models[i]\n",
    "        \n",
    "        \n",
    "        # t2 = time.time()\n",
    "        modelVars = model.getVars()\n",
    "        \n",
    "        # t3 = time.time()\n",
    "        instanceBinaryIndices = indices[i]\n",
    "        \n",
    "        modelVarsBinary = list(itemgetter(*instanceBinaryIndices)(modelVars))\n",
    "        \n",
    "        \n",
    "        # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "        # for j in range(len(instanceBinaryIndices)):\n",
    "        #     modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "        #     # for each index in instanceBinaryIndices, set the value of the corresponding variable to the value predicted by the neural network\n",
    "        #     modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j].item())\n",
    "        #     modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j].item())\n",
    "        \n",
    "        model.setAttr(\"VType\", modelVarsBinary, \"C\")\n",
    "        model.setAttr(\"LB\", modelVarsBinary, y_pred_binary[i])\n",
    "        model.setAttr(\"UB\", modelVarsBinary, y_pred_binary[i])\n",
    "        \n",
    "        \n",
    "        # t4 = time.time()\n",
    "        # Compute the IIS to find the list of violated constraints and variables\n",
    "        try:\n",
    "            model.computeIIS()\n",
    "            feasible_flag = False\n",
    "        except  Exception as e:\n",
    "            print(e)\n",
    "            print(\"Model is feasible\")\n",
    "            feasible_flag = True\n",
    "            continue\n",
    "        \n",
    "        if not feasible_flag:\n",
    "            # t5 = time.time()\n",
    "            # Initialize the weights\n",
    "            # weights = torch.zeros_like(y_true[i], dtype=torch.float32)\n",
    "            c = torch.tensor(model.getAttr(\"Obj\", modelVarsBinary), device=device)\n",
    "            \n",
    "            # get violated variables indices\n",
    "            # violated_vars_indices = [k for k, v in enumerate(model.getVars()) if (v.IISLB > 0 or v.IISUB > 0) and k in instanceBinaryIndices]\n",
    "\n",
    "\n",
    "            # t6 = time.time()\n",
    "            # for j, v in enumerate(model.getVars()):\n",
    "            #     # not violated\n",
    "            #     if (v.IISLB == 0 and v.IISUB == 0) and j in instanceBinaryIndices:\n",
    "            #         weights[j] = np.exp(np.dot(c[j], y_pred_binary[i, j].item()))\n",
    "            \n",
    "            IISLB = np.array(model.getAttr(\"IISLB\", modelVarsBinary))\n",
    "            IISUB = np.array(model.getAttr(\"IISUB\", modelVarsBinary))\n",
    "            violated_vars_indices = IISLB | IISUB\n",
    "            # violated_vars_indices = [k for k, v in enumerate(modelVarsBinary) if not (IISLB[k] == 0 and IISUB[k] == 0)]\n",
    "            # not_violated_vars_indices = [k for k, v in enumerate(modelVarsBinary) if (IISLB[k] == 0 and IISUB[k] == 0)]\n",
    "            \n",
    "            mask = torch.zeros_like(y_true[i], dtype=torch.bool)\n",
    "            mask[violated_vars_indices] = True\n",
    "                        \n",
    "            # t7 = time.time()        \n",
    "            weights_not_violated = torch.exp(torch.multiply(c.T, y_pred_binary[i]))\n",
    "            weights_not_violated[mask] = 0.0\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            # t8 = time.time()\n",
    "            # denominator = sum(  np.exp(np.dot(c[k], y_pred_binary[i, k].item())) for k in range(y_pred_binary[i].shape[0]) if not k in violated_vars_indices  )\n",
    "            denominator = sum(weights_not_violated)\n",
    "            \n",
    "            # convert denominator to torch float and move to cuda\n",
    "            denominator = torch.tensor([denominator], device=device)\n",
    "            # weights = weights.float().to(\"cpu\")\n",
    "            \n",
    "            # weights /= denominator    \n",
    "            weights_not_violated /= denominator\n",
    "        \n",
    "            # sort the time in descending order\n",
    "            \n",
    "            # t = {\"Time for reading model\": t2 - t1,\n",
    "            #         \"Time for getting model variables\": t3 - t2,\n",
    "            #             \"Time for setting variable bounds\": t4 - t3,\n",
    "            #                 \"Time for computing IIS\": t5 - t4,\n",
    "            #                     \"Time for getting violated variables\": t6 - t5,\n",
    "            #                         \"Time for computing weights\": t7 - t6,\n",
    "            #                             \"Time for calculating masked weights\": t8 - t7}\n",
    "            \n",
    "            # sort the time in descending order\n",
    "            # t = sorted(t.items(), key=lambda x: x[1], reverse=True)\n",
    "            # print(t)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # weights are all 1.0\n",
    "            weights_not_violated = torch.ones_like(y_true[i], dtype=torch.float32)\n",
    "            denominator = sum(weights_not_violated)\n",
    "            weights_not_violated /= denominator\n",
    "        \n",
    "        instance_weights.append(weights_not_violated)\n",
    "    \n",
    "    # convert list of tensors to tensor\n",
    "    instance_weights = torch.stack(instance_weights)\n",
    "           \n",
    "    return instance_weights\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss for neural network\n",
    "\n",
    "# @torch.compile\n",
    "def custom_loss(models: list, binary_indices, indices, y_pred: torch.tensor, y_true: torch.tensor):\n",
    "    \n",
    "    time_start = time.time()\n",
    "    instance_weights = get_feasibility_constrain_weights(models, binary_indices, indices, y_true, y_pred)\n",
    "    time_end = time.time()\n",
    "    \n",
    "    # print(\"Time for computing weights: \", time_end - time_start)\n",
    "    \n",
    "    loss_fn = nn.BCELoss(reduction='none')\n",
    "    \n",
    "    t1 = time.time()\n",
    "    loss = loss_fn(y_pred.float(), y_true.float())\n",
    "    \n",
    "    t2 = time.time()\n",
    "    # multiply the weights by the loss\n",
    "    loss = torch.multiply(loss, instance_weights)\n",
    "    \n",
    "    # sum the loss\n",
    "    loss = torch.mean(loss)\n",
    "    \n",
    "    t3 = time.time()\n",
    "    mean_loss = torch.tensor(loss, device=device, requires_grad=True)\n",
    "    \n",
    "    # print(\"Time for computing loss: \", t2 - t1)\n",
    "    # print(\"Time for multiplying weights: \", t3 - t2)\n",
    "    # print(\"Time for computing mean loss: \", time.time() - t3)\n",
    "    \n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_features//8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_features//8, n_features//16)\n",
    "        self.fc3 = nn.Linear(n_features//16, n_features//32)\n",
    "        self.fc4 = nn.Linear(n_features//32, out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # add regularization\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = (\n",
    "        False  # Force cuDNN to use a consistent convolution algorithm\n",
    "    )\n",
    "    torch.backends.cudnn.deterministic = (\n",
    "        True  # Force cuDNN to use deterministic algorithms if available\n",
    "    )\n",
    "    torch.use_deterministic_algorithms(\n",
    "        True\n",
    "    )  # Force torch to use deterministic algorithms if available\n",
    "\n",
    "\n",
    "config = {\n",
    "        'train_val_split': [0.80, 0.20], # These must sum to 1.0\n",
    "        'batch_size' : 32, # Num samples to average over for gradient updates\n",
    "        'EPOCHS' : 100, # Num times to iterate over the entire dataset\n",
    "        'LEARNING_RATE' : 1e-3, # Learning rate for the optimizer\n",
    "        'BETA1' : 0.9, # Beta1 parameter for the Adam optimizer\n",
    "        'BETA2' : 0.999, # Beta2 parameter for the Adam optimizer\n",
    "        'WEIGHT_DECAY' : 1e-4, # Weight decay parameter for the Adam optimizer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()\n",
    "net = torch.compile(net)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "# create the dataloader for X and solutions\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train), torch.tensor(y_train)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_test), torch.tensor(y_test)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "params = list(net.parameters())\n",
    "\n",
    "# optimizer = AdamW(params, lr=config['LEARNING_RATE'], weight_decay=1e-4)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "# optimizer = dadaptation.DAdaptAdam(params, lr=1, log_every=5, betas=(BETA1, BETA2), weight_decay=1e-4, decouple=True)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "total_steps = len(train_loader)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config['LEARNING_RATE'], steps_per_epoch=total_steps, epochs=config['EPOCHS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to cuda\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "# load the models\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "for i in range(len(model_files)):\n",
    "    models.append(gb.read(\"instances/mip/data/COR-LAT/\" + model_files[i], env=gurobi_env))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28533/415249718.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mean_loss = torch.tensor(loss, device=device, requires_grad=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to calculate loss:  25053.184173107147\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        # time to calculate the loss\n",
    "        start = time.time()\n",
    "        loss = custom_loss(models, \n",
    "                           binary_indices=indices,\n",
    "                           indices=train_indices,\n",
    "                            y_pred=outputs,\n",
    "                            y_true=labels)\n",
    "        end = time.time()\n",
    "        print(\"Time to calculate loss: \", end - start)\n",
    "                           \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d loss: %.3f lr: %.6f' % (epoch + 1, running_loss / len(train_loader), curr_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
