{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch.nn.functional import relu\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gurobipy as gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = (\n",
    "        False  # Force cuDNN to use a consistent convolution algorithm\n",
    "    )\n",
    "    torch.backends.cudnn.deterministic = (\n",
    "        True  # Force cuDNN to use deterministic algorithms if available\n",
    "    )\n",
    "    torch.use_deterministic_algorithms(\n",
    "        True\n",
    "    )  # Force torch to use deterministic algorithms if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/corlat_preprocessed.pickle\", \"rb\"))\n",
    "except:\n",
    "    # move dir to /ibm/gpfs/home/yjin0055/Project/DayAheadForecast\n",
    "    os.chdir(\"/ibm/gpfs/home/yjin0055/Project/DayAheadForecast\")\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/corlat_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each solution convert the dictionary to a list of values\n",
    "solutions = [\n",
    "    list(corlat_dataset[i][\"solution\"].values())\n",
    "    for i in range(len(corlat_dataset))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert solutions_list to numpy array\n",
    "solutions = np.array(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = os.listdir(\"instances/mip/data/COR-LAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices of the binary variables\n",
    "indices = []\n",
    "for i in range(len(corlat_dataset)):\n",
    "    indices.append(list(corlat_dataset[i][\"solution\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert indices to numpy array\n",
    "indices = np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X_train, X_test, y_train, y_test from Data/corlat/ using numpy.load\n",
    "X_train = np.load(\"Data/corlat/X_train.npy\")\n",
    "X_test = np.load(\"Data/corlat/X_test.npy\")\n",
    "y_train = np.load(\"Data/corlat/y_train.npy\")\n",
    "y_test = np.load(\"Data/corlat/y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test indices\n",
    "train_indices = np.load(\"Data/corlat/train_idx.npy\")\n",
    "test_indices = np.load(\"Data/corlat/test_idx.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the xgboost model\n",
    "with open(\"Models/Tabular/xgboost_model_corlat.pkl\", \"rb\") as f:\n",
    "    xgb_model = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.9237003241685676\n",
      "Precision:  0.9232569302772111\n",
      "Recall:  0.9241441441441441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22975/3270043010.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_test = y_test.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test.astype(np.int)\n",
    "print(\"F1 score: \", f1_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save xgboost model\n",
    "xgb_model.save_model(\"Models/Tabular/xgboost_model_corlat.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now test feasibility of the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Read LP format model from file instances/mip/data/COR-LAT/cor-lat-2f+r-u-10-10-10-5-100-3.462.b208.000000.prune2.lp\n",
      "Reading time = 0.01 seconds\n",
      "obj: 470 rows, 466 columns, 1751 nonzeros\n",
      "Set parameter Threads to value 1\n",
      "Gurobi Optimizer version 10.0.1 build v10.0.1rc0 (linux64)\n",
      "\n",
      "CPU model: AMD Ryzen Threadripper 1920X 12-Core Processor, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 12 physical cores, 24 logical processors, using up to 1 threads\n",
      "\n",
      "Optimize a model with 470 rows, 466 columns and 1751 nonzeros\n",
      "Model fingerprint: 0x3db7ce21\n",
      "Variable types: 366 continuous, 100 integer (100 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+02]\n",
      "  Objective range  [1e+00, 1e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+02]\n",
      "Presolve removed 17 rows and 9 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 453 rows, 457 columns, 1715 nonzeros\n",
      "Variable types: 360 continuous, 97 integer (97 binary)\n",
      "\n",
      "Root relaxation: objective 4.122460e+02, 474 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0  412.24598    0   11          -  412.24598      -     -    0s\n",
      "     0     0  407.74433    0   17          -  407.74433      -     -    0s\n",
      "     0     0  407.74433    0   17          -  407.74433      -     -    0s\n",
      "     0     0  406.53041    0   24          -  406.53041      -     -    0s\n",
      "     0     0  405.13437    0   18          -  405.13437      -     -    0s\n",
      "     0     0  405.07234    0   17          -  405.07234      -     -    0s\n",
      "     0     0  405.07234    0   17          -  405.07234      -     -    0s\n",
      "     0     0  402.10521    0   17          -  402.10521      -     -    0s\n",
      "     0     0  401.44774    0   18          -  401.44774      -     -    0s\n",
      "     0     0  401.44592    0   16          -  401.44592      -     -    0s\n",
      "     0     0  401.07083    0   19          -  401.07083      -     -    0s\n",
      "     0     0  400.70802    0   14          -  400.70802      -     -    0s\n",
      "     0     0  400.70802    0   15          -  400.70802      -     -    0s\n",
      "     0     0  400.48464    0   16          -  400.48464      -     -    0s\n",
      "     0     0  400.47799    0   18          -  400.47799      -     -    0s\n",
      "     0     0  400.47799    0   18          -  400.47799      -     -    0s\n",
      "     0     0  399.72899    0   23          -  399.72899      -     -    0s\n",
      "     0     0  399.67979    0   21          -  399.67979      -     -    0s\n",
      "     0     0  399.67979    0   21          -  399.67979      -     -    0s\n",
      "     0     0  398.90104    0   16          -  398.90104      -     -    0s\n",
      "     0     0  398.90101    0   17          -  398.90101      -     -    0s\n",
      "     0     0  398.55711    0   24          -  398.55711      -     -    0s\n",
      "     0     0  398.50732    0   28          -  398.50732      -     -    0s\n",
      "     0     0  398.50702    0   29          -  398.50702      -     -    0s\n",
      "     0     0  398.22469    0   23          -  398.22469      -     -    0s\n",
      "     0     0  398.21664    0   20          -  398.21664      -     -    0s\n",
      "     0     0  398.21351    0   20          -  398.21351      -     -    0s\n",
      "     0     0  398.19946    0   22          -  398.19946      -     -    0s\n",
      "     0     0  398.19946    0   22          -  398.19946      -     -    0s\n",
      "H    0     0                     300.0000000  398.19946  32.7%     -    0s\n",
      "     0     2  398.15771    0   22  300.00000  398.15771  32.7%     -    0s\n",
      "H  208   179                     302.0000000  397.77318  31.7%  15.2    0s\n",
      "H  234   202                     305.0000000  397.77318  30.4%  14.0    0s\n",
      "H  312   271                     307.0000000  397.71169  29.5%  13.7    0s\n",
      "H  468   400                     314.0000000  397.62075  26.6%  13.1    0s\n",
      "H  600   479                     387.0000000  394.92000  2.05%  17.6    1s\n",
      "*  817   499              40     389.0000000  394.47921  1.41%  22.7    1s\n",
      "* 1070   403              33     391.0000000  393.89688  0.74%  22.4    2s\n",
      "* 1270   234              31     392.0000000  393.33983  0.34%  22.3    2s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 2\n",
      "  Lift-and-project: 5\n",
      "  Cover: 16\n",
      "  Implied bound: 12\n",
      "  Projected implied bound: 2\n",
      "  MIR: 30\n",
      "  StrongCG: 1\n",
      "  Flow cover: 134\n",
      "  Inf proof: 3\n",
      "  Zero half: 1\n",
      "  Network: 3\n",
      "  Relax-and-lift: 2\n",
      "\n",
      "Explored 1316 nodes (30295 simplex iterations) in 2.36 seconds (1.51 work units)\n",
      "Thread count was 1 (of 24 available processors)\n",
      "\n",
      "Solution count 9: 392 391 389 ... 300\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.920000000000e+02, best bound 3.920000000000e+02, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# basic optimization solving time\n",
    "firstInstanceTest = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[0]])\n",
    "firstInstanceTest.Params.Threads = 1\n",
    "firstInstanceTest.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read LP format model from file instances/mip/data/COR-LAT/cor-lat-2f+r-u-10-10-10-5-100-3.462.b208.000000.prune2.lp\n",
      "Reading time = 0.01 seconds\n",
      "obj: 470 rows, 466 columns, 1751 nonzeros\n",
      "Set parameter Threads to value 1\n",
      "Iteration    Objective       Primal Inf.    Dual Inf.      Time\n",
      "       0    3.9700000e+02   2.880000e+02   0.000000e+00      0s\n",
      "\n",
      "IIS computed: 56 constraints and 73 bounds\n",
      "IIS runtime: 0.01 seconds (0.00 work units)\n",
      "Gurobi Optimizer version 10.0.1 build v10.0.1rc0 (linux64)\n",
      "\n",
      "CPU model: AMD Ryzen Threadripper 1920X 12-Core Processor, instruction set [SSE2|AVX|AVX2]\n",
      "Thread count: 12 physical cores, 24 logical processors, using up to 1 threads\n",
      "\n",
      "Optimize a model with 470 rows, 466 columns and 1751 nonzeros\n",
      "Model fingerprint: 0x499154c7\n",
      "Variable types: 366 continuous, 100 integer (100 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+02]\n",
      "  Objective range  [1e+00, 1e+01]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+02]\n",
      "\n",
      "User MIP start did not produce a new incumbent solution\n",
      "\n",
      "Presolve removed 17 rows and 9 columns\n",
      "Presolve time: 0.00s\n",
      "Presolved: 453 rows, 457 columns, 1715 nonzeros\n",
      "Variable types: 360 continuous, 97 integer (97 binary)\n",
      "\n",
      "Root relaxation: objective 4.122460e+02, 474 iterations, 0.00 seconds (0.00 work units)\n",
      "Another try with MIP start\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "H    0     0                     392.0000000  412.24598  5.16%     -    0s\n",
      "     0     0  412.24598    0   11  392.00000  412.24598  5.16%     -    0s\n",
      "     0     0  407.74433    0   17  392.00000  407.74433  4.02%     -    0s\n",
      "     0     0  407.74433    0   17  392.00000  407.74433  4.02%     -    0s\n",
      "     0     0  406.53041    0   24  392.00000  406.53041  3.71%     -    0s\n",
      "     0     0  405.13437    0   18  392.00000  405.13437  3.35%     -    0s\n",
      "     0     0  405.11859    0   20  392.00000  405.11859  3.35%     -    0s\n",
      "     0     0  405.11859    0   20  392.00000  405.11859  3.35%     -    0s\n",
      "     0     0  402.67029    0   18  392.00000  402.67029  2.72%     -    0s\n",
      "     0     0  402.66658    0   18  392.00000  402.66658  2.72%     -    0s\n",
      "     0     0  402.66658    0   18  392.00000  402.66658  2.72%     -    0s\n",
      "     0     0  402.28818    0   15  392.00000  402.28818  2.62%     -    0s\n",
      "     0     0  402.28727    0   17  392.00000  402.28727  2.62%     -    0s\n",
      "     0     0  401.46451    0   17  392.00000  401.46451  2.41%     -    0s\n",
      "     0     0  401.41814    0   20  392.00000  401.41814  2.40%     -    0s\n",
      "     0     0  401.39719    0   21  392.00000  401.39719  2.40%     -    0s\n",
      "     0     0  401.39578    0   21  392.00000  401.39578  2.40%     -    0s\n",
      "     0     0  400.86970    0   20  392.00000  400.86970  2.26%     -    0s\n",
      "     0     0  400.84105    0   19  392.00000  400.84105  2.26%     -    0s\n",
      "     0     0  400.81287    0   18  392.00000  400.81287  2.25%     -    0s\n",
      "     0     0  400.81253    0   20  392.00000  400.81253  2.25%     -    0s\n",
      "     0     0  400.02344    0   19  392.00000  400.02344  2.05%     -    0s\n",
      "     0     0  399.87352    0   19  392.00000  399.87352  2.01%     -    0s\n",
      "     0     0  399.87066    0   19  392.00000  399.87066  2.01%     -    0s\n",
      "     0     0  399.22949    0   22  392.00000  399.22949  1.84%     -    0s\n",
      "     0     0  399.07770    0   22  392.00000  399.07770  1.81%     -    0s\n",
      "     0     0  399.07770    0   22  392.00000  399.07770  1.81%     -    0s\n",
      "     0     0  398.79615    0   21  392.00000  398.79615  1.73%     -    0s\n",
      "     0     0  398.78726    0   21  392.00000  398.78726  1.73%     -    0s\n",
      "     0     0  398.67684    0   21  392.00000  398.67684  1.70%     -    0s\n",
      "     0     0  398.53102    0   19  392.00000  398.53102  1.67%     -    0s\n",
      "     0     0  398.52865    0   20  392.00000  398.52865  1.67%     -    0s\n",
      "     0     0  398.50625    0   23  392.00000  398.50625  1.66%     -    0s\n",
      "     0     0  398.50625    0   23  392.00000  398.50625  1.66%     -    0s\n",
      "     0     0  398.49631    0   24  392.00000  398.49631  1.66%     -    0s\n",
      "     0     0  398.49631    0   24  392.00000  398.49631  1.66%     -    0s\n",
      "     0     2  398.47352    0   24  392.00000  398.47352  1.65%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 3\n",
      "  Cover: 14\n",
      "  Implied bound: 29\n",
      "  MIR: 30\n",
      "  StrongCG: 1\n",
      "  Flow cover: 118\n",
      "  Inf proof: 4\n",
      "  Network: 22\n",
      "  Relax-and-lift: 1\n",
      "\n",
      "Explored 444 nodes (8608 simplex iterations) in 0.55 seconds (0.29 work units)\n",
      "Thread count was 1 (of 24 available processors)\n",
      "\n",
      "Solution count 1: 392 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.920000000000e+02, best bound 3.920000000000e+02, gap 0.0000%\n"
     ]
    }
   ],
   "source": [
    "# we are going to use first instance of test data\n",
    "firstInstanceTest = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[0]])\n",
    "firstInstanceTest.Params.Threads = 1\n",
    "\n",
    "# get indices of binary variables\n",
    "firstInstanceTestBinaryIndices = indices[test_indices[0]]\n",
    "\n",
    "# for this first instance of test data, we are going to use the xgboost prediction and fix the binary variables' values\n",
    "# to the values predicted by xgboost\n",
    "\n",
    "# get predictions from xgboost model\n",
    "xgb_pred = xgb_model.predict(X_test[0].reshape(1, -1)).reshape(-1)\n",
    "\n",
    "# get variables from the model\n",
    "modelVars = firstInstanceTest.getVars()\n",
    "\n",
    "# need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "for i in range(len(firstInstanceTestBinaryIndices)):\n",
    "    modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "    # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "    modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"LB\", xgb_pred[i])\n",
    "    modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"UB\", xgb_pred[i])\n",
    "    \n",
    "# After relaxing or fixing the binary variables, we can compute the IIS as before\n",
    "firstInstanceTest.computeIIS()\n",
    "# # Print the conflicting variables and constraints\n",
    "# for v in firstInstanceTest.getVars():\n",
    "#   if v.IISLB > 0 or v.IISUB > 0:\n",
    "#     print(v.varName, \"is part of the IIS\")\n",
    "# for c in firstInstanceTest.getConstrs():\n",
    "#   if c.IISConstr > 0:\n",
    "#     print(c.ConstrName, \"is part of the IIS\")\n",
    "\n",
    "# only assign the predicted variables that are not in the IIS to warm start the model\n",
    "for i, v in enumerate(firstInstanceTest.getVars()):\n",
    "    if v.IISLB == 0 and v.IISUB == 0:\n",
    "        if i in firstInstanceTestBinaryIndices:\n",
    "            # print(v.varName, \"is not part of the IIS\")\n",
    "            v.setAttr(\"VType\", \"B\")\n",
    "            v.setAttr(\"LB\", 0)\n",
    "            v.setAttr(\"UB\", 1)\n",
    "            v.setAttr(\"Start\", xgb_pred[i])\n",
    "            \n",
    "    \n",
    "    # else if the variable is in the IIS, \n",
    "    # get the relaxed variable and \n",
    "    # set the bounds to 0 and 1 for the relaxed binary variables\n",
    "    else:\n",
    "        if i in firstInstanceTestBinaryIndices:\n",
    "            # print(v.varName, \"is part of the IIS\")\n",
    "            v.setAttr(\"VType\", \"B\")\n",
    "            v.setAttr(\"LB\", 0)\n",
    "            v.setAttr(\"UB\", 1)\n",
    "            \n",
    "\n",
    "firstInstanceTest.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # we are going to use first instance of test data\n",
    "# firstInstanceTest = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[0]])\n",
    "# firstInstanceTest.Params.Threads = 1\n",
    "\n",
    "# # get indices of binary variables\n",
    "# firstInstanceTestBinaryIndices = indices[test_indices[0]]\n",
    "\n",
    "# # for this first instance of test data, we are going to use the xgboost prediction and fix the binary variables' values\n",
    "# # to the values predicted by xgboost\n",
    "\n",
    "# # get predictions from xgboost model\n",
    "# xgb_pred = xgb_model.predict(X_test[0].reshape(1, -1)).reshape(-1)\n",
    "\n",
    "# # get variables from the model\n",
    "# modelVars = firstInstanceTest.getVars()\n",
    "\n",
    "# # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "# for i in range(len(firstInstanceTestBinaryIndices)):\n",
    "#     modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "#     # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "#     modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"LB\", xgb_pred[i])\n",
    "#     modelVars[firstInstanceTestBinaryIndices[i]].setAttr(\"UB\", xgb_pred[i])\n",
    "    \n",
    "# # After relaxing or fixing the binary variables, we can compute the IIS as before\n",
    "# firstInstanceTest.computeIIS()\n",
    "\n",
    "# # for each constraint, if constraint in IIS, get the slack value. Implement this.\n",
    "# for c in firstInstanceTest.getConstrs():\n",
    "#     if c.IISConstr > 0:\n",
    "#         print(c.ConstrName, \"is part of the IIS\")\n",
    "#         print(c.Slack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firstInstanceTest.getVars()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only assign the predicted variables that are not in the IIS to warm start the model\n",
    "# for i, v in enumerate(firstInstanceTest.getVars()):\n",
    "#     if v.IISLB == 0 and v.IISUB == 0:\n",
    "#         if i in firstInstanceTestBinaryIndices:\n",
    "#             print(v.varName, \"is not part of the IIS\")\n",
    "#             v.setAttr(\"VType\", \"B\")\n",
    "#             v.setAttr(\"LB\", 0)\n",
    "#             v.setAttr(\"UB\", 1)\n",
    "            \n",
    "    \n",
    "#     # else if the variable is in the IIS, \n",
    "#     # get the relaxed variable and \n",
    "#     # set the bounds to 0 and 1 for the relaxed binary variables\n",
    "#     else:\n",
    "#         if i in firstInstanceTestBinaryIndices:\n",
    "#             print(v.varName, \"is part of the IIS\")\n",
    "#             v.setAttr(\"VType\", \"B\")\n",
    "#             v.setAttr(\"LB\", 0)\n",
    "#             v.setAttr(\"UB\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # continue solving the model\n",
    "# firstInstanceTest.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weights for each variable in the loss function should take the form of\n",
    "# w_{ij} = exp(-c_i^T x^{i, j}) / sum(exp(-c_i^T x^{i, k})) for k = 1, ..., N_i\n",
    "# where c_i is the vector of cost coefficient for training instance i, j is the index of the training instance, and N_i is the number of training instances\n",
    "\n",
    "\n",
    "# we are going to use first instance of test data\n",
    "# firstInstanceTest = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[0]])\n",
    "\n",
    "def custom_obj(model_files: list, indices, train_indices, y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    \n",
    "    instance_weights = []\n",
    "    \n",
    "    gurobi_env = gb.Env()\n",
    "    gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "    \n",
    "    # convert logit predictions to probabilities\n",
    "    y_pred = 1.0 / (1.0 + np.exp(-y_pred))\n",
    "\n",
    "    # convert predictions of N_samples, N_variables to binary\n",
    "    y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "    \n",
    "    # Compute the weights for each training instance\n",
    "    for i in range(y_true.shape[0]):\n",
    "        \n",
    "        model = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[train_indices[i]], env=gurobi_env)\n",
    "        \n",
    "        modelVars = model.getVars()\n",
    "        \n",
    "        instanceBinaryIndices = indices[train_indices[i]]\n",
    "\n",
    "        # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "        for j in range(len(instanceBinaryIndices)):\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "            # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "        \n",
    "        \n",
    "        # Compute the IIS to find the list of violated constraints and variables\n",
    "        model.computeIIS()\n",
    "        \n",
    "        # Initialize the weights\n",
    "        weights = np.zeros_like(y_true[i])\n",
    "        \n",
    "        c = model.getAttr(\"Obj\", model.getVars())\n",
    "        \n",
    "        # get violated variables indices\n",
    "        violated_vars_indices = [k for k, v in enumerate(model.getVars()) if (v.IISLB > 0 or v.IISUB > 0) and k in instanceBinaryIndices]\n",
    "        \n",
    "        for j, v in enumerate(model.getVars()):\n",
    "            # not violated\n",
    "            if (v.IISLB == 0 and v.IISUB == 0) and j in instanceBinaryIndices:\n",
    "                weights[j] = np.exp(np.dot(c[j], y_pred_binary[i, j]))\n",
    "        \n",
    "                \n",
    "        denominator = sum(  np.exp(np.dot(c[k], y_pred_binary[i, k])) for k in range(y_pred_binary[i].shape[0]) if not k in violated_vars_indices  )\n",
    "        \n",
    "        weights /= denominator    \n",
    "        \n",
    "        instance_weights.append(weights)\n",
    "    \n",
    "    \n",
    "    # y_pred is of shape (N_samples, N_binary_variables)\n",
    "    # weights is of shape (N_samples, N_binary_variables)\n",
    "    # each element in weights is the weight for the corresponding element in y_pred\n",
    "    # multiply the weights by the loss\n",
    "    \n",
    "    grad = y_pred - y_true\n",
    "    hess = y_pred * (1.0 - y_pred)\n",
    "    \n",
    "    # multiply the weights by the gradient and hessian\n",
    "    instance_weights = np.array(instance_weights)\n",
    "    grad = np.multiply(grad, instance_weights)\n",
    "    hess = np.multiply(hess, instance_weights)\n",
    "    \n",
    "    grad = grad.reshape(-1, 1)\n",
    "    hess = hess.reshape(-1, 1)\n",
    "\n",
    "    return grad, hess\n",
    "\n",
    "# Define a wrapper function that takes only y_true and y_pred as arguments\n",
    "def custom_obj_wrapper_train(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    # reshape y_true to be of shape (N_samples, N_binary_variables)\n",
    "    y_true = y_true.reshape(y_pred.shape)\n",
    "    return custom_obj(model_files, indices, train_indices, y_true, y_pred)\n",
    "\n",
    "# Initialize an XGBClassifier model with the custom objective function\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbmodel = XGBClassifier(objective=custom_obj_wrapper_train, tree_method=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22975/3213858680.py:1: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_train = y_train.astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-04-18\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype(np.int)\n",
    "xgbmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinyi12/miniconda3/envs/optimization/lib/python3.10/site-packages/xgboost/sklearn.py:761: UserWarning: objective is not saved in Scikit-Learn meta.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# save the feasibility constrained model\n",
    "xgbmodel.save_model(\"Models/Tabular/xgbmodel_feasibility_constrained.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load original xgboost model\n",
    "xgbmodel_ori = XGBClassifier(tree_method=\"hist\")\n",
    "xgbmodel_ori.load_model(\"Models/Tabular/xgboost_model_corlat.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data and calculate average number of infeasible assignments\n",
    "y_pred = xgbmodel.predict(X_test)\n",
    "y_pred_ori = xgbmodel_ori.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average number of infeasible assignments\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "n_infeasible = 0\n",
    "\n",
    "# Compute the weights for each training instance\n",
    "for i in range(y_pred_ori.shape[0]):\n",
    "    \n",
    "    model = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[test_indices[i]], env=gurobi_env)\n",
    "    \n",
    "    modelVars = model.getVars()\n",
    "    \n",
    "    instanceBinaryIndices = indices[test_indices[i]]\n",
    "\n",
    "    # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "    for j in range(len(instanceBinaryIndices)):\n",
    "        modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "        # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "        modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_ori[i, j])\n",
    "        modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_ori[i, j])\n",
    "    \n",
    "    \n",
    "    # Compute the IIS to find the list of violated constraints and variables\n",
    "    try:\n",
    "        model.computeIIS()\n",
    "        infeasible_flag = True\n",
    "    except:\n",
    "        infeasible_flag = False\n",
    "        continue\n",
    "    \n",
    "    if infeasible_flag:\n",
    "        # count the number of violated variables\n",
    "        for j, v in enumerate(model.getVars()):\n",
    "            # violated\n",
    "            if (v.IISLB > 0 or v.IISUB > 0) and j in instanceBinaryIndices:\n",
    "                n_infeasible += 1\n",
    "        \n",
    "    print(n_infeasible)\n",
    "    \n",
    "print(\"Average number of infeasible assignments: \", n_infeasible / y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        loss = -targets * torch.log(inputs) - (1 - targets) * torch.log(1 - inputs)\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feasibility_constrain_weights(model_files, indices, train_indices, y_true, y_pred):\n",
    "    \n",
    "    instance_weights = []\n",
    "    \n",
    "    gurobi_env = gb.Env()\n",
    "    gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "    \n",
    "    # convert logit predictions to probabilities\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "\n",
    "    # convert predictions of N_samples, N_variables to binary\n",
    "    y_pred_binary = torch.where(y_pred > 0.5, 1, 0)\n",
    "    \n",
    "    # Compute the weights for each training instance\n",
    "    for i in range(y_true.shape[0]):\n",
    "        \n",
    "        model = gb.read(\"instances/mip/data/COR-LAT/\" + model_files[i], env=gurobi_env)\n",
    "        \n",
    "        modelVars = model.getVars()\n",
    "        \n",
    "        instanceBinaryIndices = indices[i]\n",
    "\n",
    "        # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "        for j in range(len(instanceBinaryIndices)):\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "            # for each index in instanceBinaryIndices, set the value of the corresponding variable to the value predicted by the neural network\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j].item())\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j].item())\n",
    "        \n",
    "        \n",
    "        # Compute the IIS to find the list of violated constraints and variables\n",
    "        model.computeIIS()\n",
    "        \n",
    "        # Initialize the weights\n",
    "        weights = np.zeros_like(y_true[i].numpy())\n",
    "        \n",
    "        c = model.getAttr(\"Obj\", model.getVars())\n",
    "        \n",
    "        # get violated variables indices\n",
    "        violated_vars_indices = [k for k, v in enumerate(model.getVars()) if (v.IISLB > 0 or v.IISUB > 0) and k in instanceBinaryIndices]\n",
    "        \n",
    "        for j, v in enumerate(model.getVars()):\n",
    "            # not violated\n",
    "            if (v.IISLB == 0 and v.IISUB == 0) and j in instanceBinaryIndices:\n",
    "                weights[j] = np.exp(np.dot(c[j], y_pred_binary[i, j].item()))\n",
    "        \n",
    "                \n",
    "        denominator = sum(  np.exp(np.dot(c[k], y_pred_binary[i, k].item())) for k in range(y_pred_binary[i].shape[0]) if not k in violated_vars_indices  )\n",
    "        \n",
    "        weights /= denominator    \n",
    "        \n",
    "        instance_weights.append(weights)    \n",
    "        \n",
    "    instance_weights = np.array(instance_weights)\n",
    "    \n",
    "    return np.array(instance_weights)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss for neural network\n",
    "def custom_loss(model_files: list, indices, y_pred: torch.Tensor, y_true: torch.Tensor):\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    \n",
    "    instance_weights = get_feasibility_constrain_weights(model_files, indices, train_indices, y_true, y_pred)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y_true)\n",
    "    \n",
    "    # multiply the weights by the loss\n",
    "    loss = np.multiply(loss.numpy(), instance_weights)\n",
    "    \n",
    "    return torch.tensor(loss.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
