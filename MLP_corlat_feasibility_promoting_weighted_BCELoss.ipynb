{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network training with CORLAT dataset, using a feasibility promoting weighted BCE loss\n",
    "\n",
    "This notebook explores the training of a simple Multi Layer Perceptron (MLP) neural network for the CORLAT dataset.\n",
    "\n",
    "The MLP outputs assignments of binary variables for the CORLAT dataset. \n",
    "\n",
    "The idea behind the custom loss is to provide higher weights for assignments that results in a better objective value (depending on minimization or maximization). The training of MLP in this experiment differs from the majority of neural network training paradigms. The important thing to note here is that:\n",
    "\n",
    "$$\\color{lightblue}\\text{For each sample, we have multiple sets of assignments}$$\n",
    "\n",
    "For example:\n",
    "Sample 1, 100 solutions (each solution is a set of binary assignments).\n",
    "\n",
    "We train on every feasible solution gathered (up to `n_sols` specified during data collection using the `corlat.py` script).\n",
    "\n",
    "The idea is to establish the conditional probability distribution $$p(Y_{i} | X_{i}) \\quad \\text{for} \\quad i=0, 1, 2, \\dots, n $$\n",
    "\n",
    "for feasible assignments. $n$ is the number samples, and $i$ represents the $i$-th sample. i.e., $p(y^{i}_{j} = 1 | X^{i})$ is the probability of assigning a $1$ to binary variable $j$, of sample $i$, such that the assignment is feasible. \n",
    "\n",
    "Hence, it becomes clear now that the weights for each set of assignments is to encourage assignments with better objective values.\n",
    "\n",
    "The notebook ends with feasibility test for:\n",
    "1. Number of violated constraints.\n",
    "2. Optimization time for data-driven optimization using warm-start assignments.\n",
    "3. Optimization time for data-driven optimization using equality constraint assignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch.nn.functional import relu\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gurobipy as gb\n",
    "import time\n",
    "\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-4045b1e6-3428-f9e1-5643-862c4834363d)\n",
      "GPU 1: NVIDIA A100 80GB PCIe (UUID: GPU-35ac16d5-81e8-f772-b9cb-a681af1fd2b5)\n",
      "  MIG 2g.20gb     Device  0: (UUID: MIG-0447e452-22d5-5021-ab0b-6cc63b39ac83)\n",
      "GPU 2: NVIDIA A100 80GB PCIe (UUID: GPU-d949dd0a-b88e-ee87-9621-3a824f914f82)\n",
      "GPU 3: NVIDIA A100 80GB PCIe (UUID: GPU-8c13f3ad-24ee-bb68-5eff-7f73091e682a)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun  9 16:23:52 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100 80G...  On   | 00000000:17:00.0 Off |                   On |\n",
      "| N/A   57C    P0   134W / 300W |  19347MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80G...  On   | 00000000:65:00.0 Off |                   On |\n",
      "| N/A   42C    P0    46W / 300W |     24MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80G...  On   | 00000000:CA:00.0 Off |                   On |\n",
      "| N/A   58C    P0    81W / 300W |  13942MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80G...  On   | 00000000:E3:00.0 Off |                   On |\n",
      "| N/A   35C    P0    44W / 300W |     26MiB / 81920MiB |     N/A      Default |\n",
      "|                               |                      |              Enabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |         Memory-Usage |        Vol|         Shared        |\n",
      "|      ID  ID  Dev |           BAR1-Usage | SM     Unc| CE  ENC  DEC  OFA  JPG|\n",
      "|                  |                      |        ECC|                       |\n",
      "|==================+======================+===========+=======================|\n",
      "|  1    4   0   0  |      6MiB / 19968MiB | 28      0 |  2   0    1    0    0 |\n",
      "|                  |      0MiB / 32767MiB |           |                       |\n",
      "+------------------+----------------------+-----------+-----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0    4    0    1462980      C   .../python/3.10.5/bin/python    19319MiB |\n",
      "|    2    3    0    1545899      C   ...python/3.10.5/bin/python3    13911MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set CUDA to MIG-30c35cbb-1b1b-56b5-a681-575ef4494c6d\n",
    "# set CUDA_VISIBLE_DEVICES=0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = (\n",
    "        False  # Force cuDNN to use a consistent convolution algorithm\n",
    "    )\n",
    "    torch.backends.cudnn.deterministic = (\n",
    "        True  # Force cuDNN to use deterministic algorithms if available\n",
    "    )\n",
    "    torch.use_deterministic_algorithms(\n",
    "        True\n",
    "    )  # Force torch to use deterministic algorithms if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/processed_data/corlat_preprocessed.pickle\", \"rb\"))\n",
    "except:\n",
    "    # move dir to /ibm/gpfs/home/yjin0055/Project/DayAheadForecast\n",
    "    os.chdir(\"/ibm/gpfs/home/yjin0055/Project/DayAheadForecast\")\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/processed_data/corlat_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = corlat_dataset[0][\"var_node_features\"].shape[0]\n",
    "n_var_node_features = corlat_dataset[0][\"var_node_features\"].shape[1]\n",
    "max_constraint_size = corlat_dataset[0][\"constraint_node_features\"].shape[0]\n",
    "n_constraint_node_features = corlat_dataset[0][\"constraint_node_features\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all check the number of nodes and features\n",
    "for i in range(len(corlat_dataset)):\n",
    "    assert num_nodes == corlat_dataset[i][\"var_node_features\"].shape[0]\n",
    "    assert n_var_node_features == corlat_dataset[i][\"var_node_features\"].shape[1]\n",
    "    # assert max_constraint_size == corlat_dataset[i][\"constraint_node_features\"].shape[0]\n",
    "    assert n_constraint_node_features == corlat_dataset[i][\"constraint_node_features\"].shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "If cannot load, we `cd` to the respective project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corlat_presolved_dataset = pkl.load(open(\"Data/corlat_presolved/processed_data/corlat_presolved_preprocessed.pickle\", \"rb\"))\n",
    "except:\n",
    "    # move dir to /ibm/gpfs/home/yjin0055/Project/DayAheadForecast\n",
    "    os.chdir(\"/ibm/gpfs/home/yjin0055/Project/DayAheadForecast\")\n",
    "    corlat_presolved_dataset = pkl.load(open(\"Data/corlat_presolved/processed_data/corlat_presolved_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the binary variable indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary indices for each sample is the same\n",
    "binary_indices = corlat_dataset[0][\"indices\"][\"indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read X_train, X_test, y_train, y_test from Data/corlat/ using numpy.load\n",
    "X_train = np.load(\"Data/corlat/train_test_data/X_train.npy\")\n",
    "X_test = np.load(\"Data/corlat/train_test_data/X_test.npy\")\n",
    "y_train = np.load(\"Data/corlat/train_test_data/y_train.npy\", allow_pickle=True)\n",
    "y_test = np.load(\"Data/corlat/train_test_data/y_test.npy\", allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to binary and ensure there are no -0.0 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each instance in y_train and y_test, convert it to binary\n",
    "for i in range(y_train.shape[0]):\n",
    "    # make all values positive using abs\n",
    "    # y_train[i] is a tensor of shape (arbritary shape), num_vars\n",
    "    y_train[i] = np.abs(y_train[i])\n",
    "    \n",
    "    # use numpy where to convert values > 0.5 to 1, and values <= 0.5 to 0\n",
    "    y_train[i] = np.where(y_train[i] > 0.5, 1.0, 0.0)\n",
    "    \n",
    "for i in range(y_test.shape[0]):\n",
    "    # make all values positive using abs\n",
    "    # y_train[i] is a tensor of shape (arbritary shape), num_vars\n",
    "    y_test[i] = np.abs(y_test[i])\n",
    "    \n",
    "    # use numpy where to convert values > 0.5 to 1, and values <= 0.5 to 0\n",
    "    y_test[i] = np.where(y_test[i] > 0.5, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 0., 1.],\n",
       "       [1., 1., 1., ..., 1., 0., 1.],\n",
       "       [1., 1., 1., ..., 1., 0., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 0., 1.],\n",
       "       [1., 1., 1., ..., 1., 0., 1.],\n",
       "       [1., 1., 1., ..., 1., 0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the indices for training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test indices\n",
    "train_indices = np.load(\"Data/corlat/train_test_data/train_idx.npy\")\n",
    "test_indices = np.load(\"Data/corlat/train_test_data/test_idx.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "out_channels = y_train[0].shape[1]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features:  15847\n",
      "out_channels:  100\n"
     ]
    }
   ],
   "source": [
    "print(\"n_features: \", n_features)\n",
    "print(\"out_channels: \", out_channels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the weights for feasibility promoting weighted BCE loss loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load(\"Data/corlat/train_test_data/train_weights.npy\", allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_features//8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_features//8, n_features//16)\n",
    "        self.fc3 = nn.Linear(n_features//16, n_features//32)\n",
    "        self.fc4 = nn.Linear(n_features//32, out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # add regularization\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'train_val_split': [0.80, 0.20], # These must sum to 1.0\n",
    "        'batch_size' : 32, # Num samples to average over for gradient updates\n",
    "        'EPOCHS' : 1000, # Num times to iterate over the entire dataset\n",
    "        'LEARNING_RATE' : 5e-4, # Learning rate for the optimizer\n",
    "        'BETA1' : 0.9, # Beta1 parameter for the Adam optimizer\n",
    "        'BETA2' : 0.999, # Beta2 parameter for the Adam optimizer\n",
    "        'WEIGHT_DECAY' : 1e-4, # Weight decay parameter for the Adam optimizer\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom class for our dataset\n",
    "\n",
    "The custom class is needed as our `y` data is an `object` type tensor. This is because `y` is an array of `n` samples, where each `y[i]` is of `n_sols` x 100, where 100 is 100 binary outputs. `n_sols` can have a maximum of 100, due to our setting of collecting 100 possibl solutions. However `n_sols` varies from sample to sample, as for some sample there might not be 100 sols.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multipleTargetCORLATDataset(TensorDataset):\n",
    "    def __init__(self, X, y, weights=None, test=False):\n",
    "        super(multipleTargetCORLATDataset, self).__init__()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.weights = weights\n",
    "        self.test = test\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "    \n",
    "        if self.weights is None and self.test:\n",
    "            return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        weights = self.weights[index]\n",
    "        \n",
    "        \n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "        weights_tensor = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "        return X_tensor, y_tensor, weights_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "\n",
    "def collate_fn(data):\n",
    "    # data is a list of tuples (X, Y, weights)\n",
    "    \n",
    "    X = torch.stack([item[0] for item in data])\n",
    "    Y = [item[1] for item in data]\n",
    "    \n",
    "    \n",
    "    # only X and Y, no weights\n",
    "    if len(data[0]) == 2:\n",
    "        return X, Y    \n",
    "    \n",
    "    weights = [item[2] for item in data]\n",
    "\n",
    "    \n",
    "    return X, Y, weights\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = multipleTargetCORLATDataset(X_train, y_train, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = multipleTargetCORLATDataset(X_test, y_test, test=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize neural network, train loader, valid, loader, and optimizer.\n",
    "\n",
    "We used a one cycle learning rate in this case, where the learning rate warms up and peaks at `max_lr`. Then decreases to a small learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()\n",
    "# net = torch.compile(net)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "batch_size_test = 32\n",
    "valid_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "params = list(net.parameters())\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "total_steps = len(train_loader)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config['LEARNING_RATE'], steps_per_epoch=total_steps, epochs=config['EPOCHS'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition for feasibility promoting weighted BCELoss\n",
    "The custom loss calculates the BCEloss between the predicted binary targets and each (possible) output binary solutions (keep in mind that we have a maximum of 100 total solutions). \n",
    "\n",
    "For each sample, we take the mean across the losses of the 100 binary variables. Then for each sample, we multiply the calculated loss with the respective weight. We then sum all the losses of `n_sols_i`. \n",
    "\n",
    "For each batch, we then take the mean loss of the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss for neural network\n",
    "def feasibility_promoting_weighted_BCELoss(y_pred: torch.tensor, y_true: torch.tensor, weights: torch.tensor, device: torch.device):\n",
    "    \n",
    "    batch_loss = []\n",
    "    \n",
    "    loss_fn = nn.BCELoss(reduction='none')\n",
    "        \n",
    "    # sum over all targets\n",
    "    for i in range(len(y_true)):\n",
    "        loss = torch.mean(loss_fn(y_pred[i].expand(len(y_true[i]), -1), y_true[i].to(device)), dim=1)\n",
    "        loss = torch.mul(loss, weights[i].to(device))\n",
    "        batch_loss.append(torch.sum(loss))\n",
    "    \n",
    "    # sum over all samples\n",
    "    batch_loss = torch.mean(torch.stack(batch_loss))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The main training loop\n",
    "\n",
    "If the loss is smaller than the minimum loss, then save the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.691 lr: 0.000020\n",
      "Epoch 2 loss: 0.649 lr: 0.000020\n",
      "min loss:  0.6905858302116394\n",
      "Model saved\n",
      "Epoch 3 loss: 0.623 lr: 0.000020\n",
      "min loss:  0.648573921918869\n",
      "Model saved\n",
      "Epoch 4 loss: 0.590 lr: 0.000020\n",
      "min loss:  0.6225843667984009\n",
      "Model saved\n",
      "Epoch 5 loss: 0.562 lr: 0.000020\n",
      "min loss:  0.5899189734458923\n",
      "Model saved\n",
      "Epoch 6 loss: 0.542 lr: 0.000020\n",
      "min loss:  0.5617670226097107\n",
      "Model saved\n",
      "Epoch 7 loss: 0.533 lr: 0.000020\n",
      "min loss:  0.5422833728790283\n",
      "Model saved\n",
      "Epoch 8 loss: 0.527 lr: 0.000021\n",
      "min loss:  0.533007031083107\n",
      "Model saved\n",
      "Epoch 9 loss: 0.526 lr: 0.000021\n",
      "min loss:  0.5274606078863144\n",
      "Model saved\n",
      "Epoch 10 loss: 0.524 lr: 0.000021\n",
      "min loss:  0.5264981603622436\n",
      "Model saved\n",
      "Epoch 11 loss: 0.522 lr: 0.000021\n",
      "min loss:  0.5243312108516693\n",
      "Model saved\n",
      "Epoch 12 loss: 0.521 lr: 0.000022\n",
      "min loss:  0.5223673379421234\n",
      "Model saved\n",
      "Epoch 13 loss: 0.521 lr: 0.000022\n",
      "min loss:  0.5214317381381989\n",
      "Model saved\n",
      "Epoch 14 loss: 0.521 lr: 0.000022\n",
      "min loss:  0.5209495574235916\n",
      "Model saved\n",
      "Epoch 15 loss: 0.520 lr: 0.000023\n",
      "min loss:  0.5209391283988952\n",
      "Model saved\n",
      "Epoch 16 loss: 0.518 lr: 0.000023\n",
      "min loss:  0.5196742850542069\n",
      "Model saved\n",
      "Epoch 17 loss: 0.517 lr: 0.000023\n",
      "min loss:  0.5180762940645218\n",
      "Model saved\n",
      "Epoch 18 loss: 0.516 lr: 0.000024\n",
      "min loss:  0.5166444677114487\n",
      "Model saved\n",
      "Epoch 19 loss: 0.515 lr: 0.000024\n",
      "min loss:  0.5156788247823715\n",
      "Model saved\n",
      "Epoch 20 loss: 0.514 lr: 0.000025\n",
      "min loss:  0.5148635929822922\n",
      "Model saved\n",
      "Epoch 21 loss: 0.513 lr: 0.000025\n",
      "min loss:  0.513544515967369\n",
      "Model saved\n",
      "Epoch 22 loss: 0.511 lr: 0.000026\n",
      "min loss:  0.5126387351751327\n",
      "Model saved\n",
      "Epoch 23 loss: 0.510 lr: 0.000026\n",
      "min loss:  0.5112828081846237\n",
      "Model saved\n",
      "Epoch 24 loss: 0.508 lr: 0.000027\n",
      "min loss:  0.5098266535997391\n",
      "Model saved\n",
      "Epoch 25 loss: 0.506 lr: 0.000028\n",
      "min loss:  0.5080683028697968\n",
      "Model saved\n",
      "Epoch 26 loss: 0.505 lr: 0.000028\n",
      "min loss:  0.5057385855913162\n",
      "Model saved\n",
      "Epoch 27 loss: 0.505 lr: 0.000029\n",
      "min loss:  0.5054367542266845\n",
      "Model saved\n",
      "Epoch 28 loss: 0.503 lr: 0.000030\n",
      "min loss:  0.50520427942276\n",
      "Model saved\n",
      "Epoch 29 loss: 0.502 lr: 0.000030\n",
      "min loss:  0.503231691122055\n",
      "Model saved\n",
      "Epoch 30 loss: 0.499 lr: 0.000031\n",
      "min loss:  0.5016310501098633\n",
      "Model saved\n",
      "Epoch 31 loss: 0.498 lr: 0.000032\n",
      "min loss:  0.4991121870279312\n",
      "Model saved\n",
      "Epoch 32 loss: 0.494 lr: 0.000033\n",
      "min loss:  0.498180713057518\n",
      "Model saved\n",
      "Epoch 33 loss: 0.495 lr: 0.000033\n",
      "min loss:  0.4943943059444427\n",
      "Epoch 34 loss: 0.492 lr: 0.000034\n",
      "min loss:  0.4943943059444427\n",
      "Model saved\n",
      "Epoch 35 loss: 0.495 lr: 0.000035\n",
      "min loss:  0.4919853961467743\n",
      "Epoch 36 loss: 0.495 lr: 0.000036\n",
      "min loss:  0.4919853961467743\n",
      "Epoch 37 loss: 0.489 lr: 0.000037\n",
      "min loss:  0.4919853961467743\n",
      "Model saved\n",
      "Epoch 38 loss: 0.487 lr: 0.000038\n",
      "min loss:  0.4894162213802338\n",
      "Model saved\n",
      "Epoch 39 loss: 0.484 lr: 0.000039\n",
      "min loss:  0.4870547330379486\n",
      "Model saved\n",
      "Epoch 40 loss: 0.481 lr: 0.000040\n",
      "min loss:  0.48397700011730194\n",
      "Model saved\n",
      "Epoch 41 loss: 0.484 lr: 0.000041\n",
      "min loss:  0.4814646941423416\n",
      "Epoch 42 loss: 0.481 lr: 0.000042\n",
      "min loss:  0.4814646941423416\n",
      "Model saved\n",
      "Epoch 43 loss: 0.481 lr: 0.000043\n",
      "min loss:  0.4814331030845642\n",
      "Model saved\n",
      "Epoch 44 loss: 0.484 lr: 0.000044\n",
      "min loss:  0.48096761882305145\n",
      "Epoch 45 loss: 0.476 lr: 0.000045\n",
      "min loss:  0.48096761882305145\n",
      "Model saved\n",
      "Epoch 46 loss: 0.475 lr: 0.000046\n",
      "min loss:  0.47595839977264404\n",
      "Model saved\n",
      "Epoch 47 loss: 0.478 lr: 0.000047\n",
      "min loss:  0.47500148475170134\n",
      "Epoch 48 loss: 0.478 lr: 0.000048\n",
      "min loss:  0.47500148475170134\n",
      "Epoch 49 loss: 0.478 lr: 0.000050\n",
      "min loss:  0.47500148475170134\n",
      "Epoch 50 loss: 0.475 lr: 0.000051\n",
      "min loss:  0.47500148475170134\n",
      "Model saved\n",
      "Epoch 51 loss: 0.473 lr: 0.000052\n",
      "min loss:  0.4749426341056824\n",
      "Model saved\n",
      "Epoch 52 loss: 0.471 lr: 0.000053\n",
      "min loss:  0.4730196523666382\n",
      "Model saved\n",
      "Epoch 53 loss: 0.467 lr: 0.000055\n",
      "min loss:  0.47134048998355865\n",
      "Model saved\n",
      "Epoch 54 loss: 0.470 lr: 0.000056\n",
      "min loss:  0.46735002517700197\n",
      "Epoch 55 loss: 0.469 lr: 0.000057\n",
      "min loss:  0.46735002517700197\n",
      "Epoch 56 loss: 0.473 lr: 0.000059\n",
      "min loss:  0.46735002517700197\n",
      "Epoch 57 loss: 0.474 lr: 0.000060\n",
      "min loss:  0.46735002517700197\n",
      "Epoch 58 loss: 0.471 lr: 0.000062\n",
      "min loss:  0.46735002517700197\n",
      "Epoch 59 loss: 0.468 lr: 0.000063\n",
      "min loss:  0.46735002517700197\n",
      "Epoch 60 loss: 0.470 lr: 0.000064\n",
      "min loss:  0.46735002517700197\n",
      "Epoch 61 loss: 0.470 lr: 0.000066\n",
      "min loss:  0.46735002517700197\n",
      "Epoch 62 loss: 0.470 lr: 0.000067\n",
      "min loss:  0.46735002517700197\n",
      "Epoch 63 loss: 0.466 lr: 0.000069\n",
      "min loss:  0.46735002517700197\n",
      "Model saved\n",
      "Epoch 64 loss: 0.464 lr: 0.000070\n",
      "min loss:  0.4664230853319168\n",
      "Model saved\n",
      "Epoch 65 loss: 0.459 lr: 0.000072\n",
      "min loss:  0.46359620451927186\n",
      "Model saved\n",
      "Epoch 66 loss: 0.462 lr: 0.000073\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 67 loss: 0.468 lr: 0.000075\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 68 loss: 0.471 lr: 0.000077\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 69 loss: 0.472 lr: 0.000078\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 70 loss: 0.472 lr: 0.000080\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 71 loss: 0.467 lr: 0.000082\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 72 loss: 0.468 lr: 0.000083\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 73 loss: 0.468 lr: 0.000085\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 74 loss: 0.468 lr: 0.000087\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 75 loss: 0.470 lr: 0.000089\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 76 loss: 0.479 lr: 0.000090\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 77 loss: 0.478 lr: 0.000092\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 78 loss: 0.477 lr: 0.000094\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 79 loss: 0.476 lr: 0.000096\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 80 loss: 0.478 lr: 0.000098\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 81 loss: 0.482 lr: 0.000099\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 82 loss: 0.481 lr: 0.000101\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 83 loss: 0.478 lr: 0.000103\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 84 loss: 0.482 lr: 0.000105\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 85 loss: 0.484 lr: 0.000107\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 86 loss: 0.478 lr: 0.000109\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 87 loss: 0.477 lr: 0.000111\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 88 loss: 0.485 lr: 0.000113\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 89 loss: 0.489 lr: 0.000115\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 90 loss: 0.490 lr: 0.000117\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 91 loss: 0.494 lr: 0.000119\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 92 loss: 0.477 lr: 0.000121\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 93 loss: 0.479 lr: 0.000123\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 94 loss: 0.481 lr: 0.000125\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 95 loss: 0.483 lr: 0.000127\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 96 loss: 0.478 lr: 0.000129\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 97 loss: 0.477 lr: 0.000131\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 98 loss: 0.475 lr: 0.000134\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 99 loss: 0.474 lr: 0.000136\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 100 loss: 0.479 lr: 0.000138\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 101 loss: 0.477 lr: 0.000140\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 102 loss: 0.478 lr: 0.000142\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 103 loss: 0.473 lr: 0.000144\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 104 loss: 0.476 lr: 0.000147\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 105 loss: 0.479 lr: 0.000149\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 106 loss: 0.475 lr: 0.000151\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 107 loss: 0.480 lr: 0.000153\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 108 loss: 0.487 lr: 0.000156\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 109 loss: 0.483 lr: 0.000158\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 110 loss: 0.485 lr: 0.000160\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 111 loss: 0.480 lr: 0.000162\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 112 loss: 0.477 lr: 0.000165\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 113 loss: 0.477 lr: 0.000167\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 114 loss: 0.483 lr: 0.000169\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 115 loss: 0.494 lr: 0.000172\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 116 loss: 0.499 lr: 0.000174\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 117 loss: 0.491 lr: 0.000176\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 118 loss: 0.493 lr: 0.000179\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 119 loss: 0.501 lr: 0.000181\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 120 loss: 0.521 lr: 0.000183\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 121 loss: 0.520 lr: 0.000186\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 122 loss: 0.514 lr: 0.000188\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 123 loss: 0.515 lr: 0.000191\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 124 loss: 0.515 lr: 0.000193\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 125 loss: 0.520 lr: 0.000195\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 126 loss: 0.515 lr: 0.000198\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 127 loss: 0.519 lr: 0.000200\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 128 loss: 0.509 lr: 0.000203\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 129 loss: 0.511 lr: 0.000205\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 130 loss: 0.507 lr: 0.000208\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 131 loss: 0.514 lr: 0.000210\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 132 loss: 0.514 lr: 0.000213\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 133 loss: 0.510 lr: 0.000215\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 134 loss: 0.507 lr: 0.000218\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 135 loss: 0.512 lr: 0.000220\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 136 loss: 0.510 lr: 0.000222\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 137 loss: 0.515 lr: 0.000225\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 138 loss: 0.515 lr: 0.000227\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 139 loss: 0.508 lr: 0.000230\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 140 loss: 0.509 lr: 0.000232\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 141 loss: 0.507 lr: 0.000235\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 142 loss: 0.512 lr: 0.000237\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 143 loss: 0.503 lr: 0.000240\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 144 loss: 0.506 lr: 0.000242\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 145 loss: 0.503 lr: 0.000245\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 146 loss: 0.500 lr: 0.000247\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 147 loss: 0.504 lr: 0.000250\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 148 loss: 0.501 lr: 0.000252\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 149 loss: 0.501 lr: 0.000255\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 150 loss: 0.511 lr: 0.000258\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 151 loss: 0.506 lr: 0.000260\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 152 loss: 0.498 lr: 0.000263\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 153 loss: 0.500 lr: 0.000265\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 154 loss: 0.506 lr: 0.000268\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 155 loss: 0.507 lr: 0.000270\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 156 loss: 0.506 lr: 0.000273\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 157 loss: 0.502 lr: 0.000275\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 158 loss: 0.499 lr: 0.000278\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 159 loss: 0.499 lr: 0.000280\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 160 loss: 0.501 lr: 0.000283\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 161 loss: 0.500 lr: 0.000285\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 162 loss: 0.502 lr: 0.000288\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 163 loss: 0.500 lr: 0.000290\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 164 loss: 0.494 lr: 0.000293\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 165 loss: 0.500 lr: 0.000295\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 166 loss: 0.501 lr: 0.000298\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 167 loss: 0.497 lr: 0.000300\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 168 loss: 0.505 lr: 0.000303\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 169 loss: 0.496 lr: 0.000305\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 170 loss: 0.495 lr: 0.000307\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 171 loss: 0.497 lr: 0.000310\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 172 loss: 0.495 lr: 0.000312\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 173 loss: 0.493 lr: 0.000315\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 174 loss: 0.496 lr: 0.000317\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 175 loss: 0.512 lr: 0.000320\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 176 loss: 0.499 lr: 0.000322\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 177 loss: 0.502 lr: 0.000325\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 178 loss: 0.500 lr: 0.000327\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 179 loss: 0.505 lr: 0.000329\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 180 loss: 0.498 lr: 0.000332\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 181 loss: 0.494 lr: 0.000334\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 182 loss: 0.498 lr: 0.000337\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 183 loss: 0.498 lr: 0.000339\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 184 loss: 0.497 lr: 0.000341\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 185 loss: 0.505 lr: 0.000344\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 186 loss: 0.496 lr: 0.000346\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 187 loss: 0.496 lr: 0.000348\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 188 loss: 0.513 lr: 0.000351\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 189 loss: 0.502 lr: 0.000353\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 190 loss: 0.496 lr: 0.000355\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 191 loss: 0.499 lr: 0.000358\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 192 loss: 0.506 lr: 0.000360\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 193 loss: 0.492 lr: 0.000362\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 194 loss: 0.504 lr: 0.000364\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 195 loss: 0.497 lr: 0.000367\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 196 loss: 0.501 lr: 0.000369\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 197 loss: 0.493 lr: 0.000371\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 198 loss: 0.501 lr: 0.000373\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 199 loss: 0.492 lr: 0.000376\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 200 loss: 0.496 lr: 0.000378\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 201 loss: 0.496 lr: 0.000380\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 202 loss: 0.506 lr: 0.000382\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 203 loss: 0.498 lr: 0.000384\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 204 loss: 0.501 lr: 0.000386\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 205 loss: 0.499 lr: 0.000389\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 206 loss: 0.500 lr: 0.000391\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 207 loss: 0.503 lr: 0.000393\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 208 loss: 0.501 lr: 0.000395\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 209 loss: 0.495 lr: 0.000397\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 210 loss: 0.507 lr: 0.000399\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 211 loss: 0.503 lr: 0.000401\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 212 loss: 0.505 lr: 0.000403\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 213 loss: 0.495 lr: 0.000405\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 214 loss: 0.491 lr: 0.000407\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 215 loss: 0.497 lr: 0.000409\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 216 loss: 0.499 lr: 0.000411\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 217 loss: 0.495 lr: 0.000413\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 218 loss: 0.497 lr: 0.000415\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 219 loss: 0.494 lr: 0.000417\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 220 loss: 0.498 lr: 0.000419\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 221 loss: 0.497 lr: 0.000421\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 222 loss: 0.501 lr: 0.000422\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 223 loss: 0.494 lr: 0.000424\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 224 loss: 0.492 lr: 0.000426\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 225 loss: 0.496 lr: 0.000428\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 226 loss: 0.492 lr: 0.000430\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 227 loss: 0.494 lr: 0.000431\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 228 loss: 0.501 lr: 0.000433\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 229 loss: 0.511 lr: 0.000435\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 230 loss: 0.498 lr: 0.000437\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 231 loss: 0.509 lr: 0.000438\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 232 loss: 0.491 lr: 0.000440\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 233 loss: 0.500 lr: 0.000442\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 234 loss: 0.496 lr: 0.000443\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 235 loss: 0.491 lr: 0.000445\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 236 loss: 0.496 lr: 0.000447\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 237 loss: 0.510 lr: 0.000448\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 238 loss: 0.540 lr: 0.000450\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 239 loss: 0.518 lr: 0.000451\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 240 loss: 0.510 lr: 0.000453\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 241 loss: 0.511 lr: 0.000454\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 242 loss: 0.512 lr: 0.000456\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 243 loss: 0.517 lr: 0.000457\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 244 loss: 0.510 lr: 0.000459\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 245 loss: 0.509 lr: 0.000460\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 246 loss: 0.505 lr: 0.000461\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 247 loss: 0.515 lr: 0.000463\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 248 loss: 0.517 lr: 0.000464\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 249 loss: 0.531 lr: 0.000465\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 250 loss: 0.519 lr: 0.000467\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 251 loss: 0.511 lr: 0.000468\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 252 loss: 0.510 lr: 0.000469\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 253 loss: 0.509 lr: 0.000470\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 254 loss: 0.540 lr: 0.000472\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 255 loss: 0.522 lr: 0.000473\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 256 loss: 0.525 lr: 0.000474\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 257 loss: 0.525 lr: 0.000475\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 258 loss: 0.520 lr: 0.000476\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 259 loss: 0.513 lr: 0.000477\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 260 loss: 0.508 lr: 0.000478\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 261 loss: 0.513 lr: 0.000479\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 262 loss: 0.507 lr: 0.000480\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 263 loss: 0.512 lr: 0.000481\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 264 loss: 0.530 lr: 0.000482\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 265 loss: 0.538 lr: 0.000483\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 266 loss: 0.521 lr: 0.000484\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 267 loss: 0.509 lr: 0.000485\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 268 loss: 0.514 lr: 0.000486\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 269 loss: 0.515 lr: 0.000487\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 270 loss: 0.509 lr: 0.000487\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 271 loss: 0.519 lr: 0.000488\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 272 loss: 0.517 lr: 0.000489\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 273 loss: 0.517 lr: 0.000490\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 274 loss: 0.516 lr: 0.000490\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 275 loss: 0.522 lr: 0.000491\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 276 loss: 0.514 lr: 0.000492\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 277 loss: 0.511 lr: 0.000492\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 278 loss: 0.528 lr: 0.000493\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 279 loss: 0.515 lr: 0.000494\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 280 loss: 0.509 lr: 0.000494\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 281 loss: 0.508 lr: 0.000495\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 282 loss: 0.508 lr: 0.000495\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 283 loss: 0.508 lr: 0.000496\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 284 loss: 0.504 lr: 0.000496\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 285 loss: 0.515 lr: 0.000497\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 286 loss: 0.505 lr: 0.000497\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 287 loss: 0.512 lr: 0.000497\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 288 loss: 0.507 lr: 0.000498\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 289 loss: 0.508 lr: 0.000498\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 290 loss: 0.503 lr: 0.000498\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 291 loss: 0.509 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 292 loss: 0.505 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 293 loss: 0.518 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 294 loss: 0.510 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 295 loss: 0.514 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 296 loss: 0.511 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 297 loss: 0.504 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 298 loss: 0.505 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 299 loss: 0.509 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 300 loss: 0.505 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 301 loss: 0.511 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 302 loss: 0.520 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 303 loss: 0.507 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 304 loss: 0.504 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 305 loss: 0.508 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 306 loss: 0.507 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 307 loss: 0.515 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 308 loss: 0.517 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 309 loss: 0.511 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 310 loss: 0.504 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 311 loss: 0.516 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 312 loss: 0.515 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 313 loss: 0.513 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 314 loss: 0.505 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 315 loss: 0.506 lr: 0.000500\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 316 loss: 0.506 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 317 loss: 0.501 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 318 loss: 0.513 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 319 loss: 0.535 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 320 loss: 0.522 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 321 loss: 0.522 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 322 loss: 0.519 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 323 loss: 0.541 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 324 loss: 0.521 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 325 loss: 0.516 lr: 0.000499\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 326 loss: 0.523 lr: 0.000498\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 327 loss: 0.523 lr: 0.000498\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 328 loss: 0.526 lr: 0.000498\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 329 loss: 0.516 lr: 0.000498\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 330 loss: 0.520 lr: 0.000498\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 331 loss: 0.536 lr: 0.000498\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 332 loss: 0.513 lr: 0.000498\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 333 loss: 0.511 lr: 0.000497\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 334 loss: 0.513 lr: 0.000497\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 335 loss: 0.512 lr: 0.000497\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 336 loss: 0.518 lr: 0.000497\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 337 loss: 0.523 lr: 0.000497\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 338 loss: 0.530 lr: 0.000497\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 339 loss: 0.518 lr: 0.000496\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 340 loss: 0.522 lr: 0.000496\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 341 loss: 0.544 lr: 0.000496\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 342 loss: 0.518 lr: 0.000496\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 343 loss: 0.520 lr: 0.000496\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 344 loss: 0.527 lr: 0.000495\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 345 loss: 0.518 lr: 0.000495\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 346 loss: 0.516 lr: 0.000495\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 347 loss: 0.540 lr: 0.000495\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 348 loss: 0.533 lr: 0.000494\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 349 loss: 0.538 lr: 0.000494\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 350 loss: 0.527 lr: 0.000494\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 351 loss: 0.528 lr: 0.000494\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 352 loss: 0.524 lr: 0.000493\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 353 loss: 0.520 lr: 0.000493\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 354 loss: 0.521 lr: 0.000493\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 355 loss: 0.518 lr: 0.000493\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 356 loss: 0.524 lr: 0.000492\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 357 loss: 0.519 lr: 0.000492\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 358 loss: 0.523 lr: 0.000492\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 359 loss: 0.520 lr: 0.000492\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 360 loss: 0.518 lr: 0.000491\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 361 loss: 0.522 lr: 0.000491\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 362 loss: 0.515 lr: 0.000491\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 363 loss: 0.523 lr: 0.000490\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 364 loss: 0.514 lr: 0.000490\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 365 loss: 0.517 lr: 0.000490\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 366 loss: 0.517 lr: 0.000489\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 367 loss: 0.518 lr: 0.000489\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 368 loss: 0.516 lr: 0.000489\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 369 loss: 0.515 lr: 0.000488\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 370 loss: 0.516 lr: 0.000488\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 371 loss: 0.512 lr: 0.000488\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 372 loss: 0.519 lr: 0.000487\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 373 loss: 0.515 lr: 0.000487\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 374 loss: 0.517 lr: 0.000487\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 375 loss: 0.511 lr: 0.000486\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 376 loss: 0.509 lr: 0.000486\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 377 loss: 0.537 lr: 0.000486\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 378 loss: 0.526 lr: 0.000485\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 379 loss: 0.523 lr: 0.000485\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 380 loss: 0.518 lr: 0.000484\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 381 loss: 0.521 lr: 0.000484\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 382 loss: 0.518 lr: 0.000484\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 383 loss: 0.517 lr: 0.000483\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 384 loss: 0.533 lr: 0.000483\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 385 loss: 0.518 lr: 0.000482\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 386 loss: 0.532 lr: 0.000482\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 387 loss: 0.513 lr: 0.000482\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 388 loss: 0.519 lr: 0.000481\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 389 loss: 0.515 lr: 0.000481\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 390 loss: 0.521 lr: 0.000480\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 391 loss: 0.515 lr: 0.000480\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 392 loss: 0.518 lr: 0.000479\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 393 loss: 0.516 lr: 0.000479\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 394 loss: 0.519 lr: 0.000479\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 395 loss: 0.514 lr: 0.000478\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 396 loss: 0.513 lr: 0.000478\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 397 loss: 0.512 lr: 0.000477\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 398 loss: 0.512 lr: 0.000477\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 399 loss: 0.529 lr: 0.000476\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 400 loss: 0.516 lr: 0.000476\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 401 loss: 0.517 lr: 0.000475\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 402 loss: 0.520 lr: 0.000475\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 403 loss: 0.510 lr: 0.000474\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 404 loss: 0.511 lr: 0.000474\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 405 loss: 0.514 lr: 0.000473\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 406 loss: 0.515 lr: 0.000473\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 407 loss: 0.512 lr: 0.000472\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 408 loss: 0.512 lr: 0.000472\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 409 loss: 0.509 lr: 0.000471\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 410 loss: 0.509 lr: 0.000471\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 411 loss: 0.514 lr: 0.000470\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 412 loss: 0.518 lr: 0.000470\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 413 loss: 0.509 lr: 0.000469\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 414 loss: 0.509 lr: 0.000469\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 415 loss: 0.515 lr: 0.000468\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 416 loss: 0.512 lr: 0.000467\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 417 loss: 0.509 lr: 0.000467\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 418 loss: 0.522 lr: 0.000466\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 419 loss: 0.514 lr: 0.000466\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 420 loss: 0.508 lr: 0.000465\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 421 loss: 0.513 lr: 0.000465\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 422 loss: 0.511 lr: 0.000464\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 423 loss: 0.510 lr: 0.000463\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 424 loss: 0.511 lr: 0.000463\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 425 loss: 0.504 lr: 0.000462\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 426 loss: 0.509 lr: 0.000462\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 427 loss: 0.511 lr: 0.000461\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 428 loss: 0.515 lr: 0.000460\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 429 loss: 0.519 lr: 0.000460\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 430 loss: 0.512 lr: 0.000459\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 431 loss: 0.512 lr: 0.000459\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 432 loss: 0.518 lr: 0.000458\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 433 loss: 0.512 lr: 0.000457\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 434 loss: 0.511 lr: 0.000457\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 435 loss: 0.508 lr: 0.000456\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 436 loss: 0.517 lr: 0.000455\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 437 loss: 0.533 lr: 0.000455\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 438 loss: 0.511 lr: 0.000454\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 439 loss: 0.512 lr: 0.000454\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 440 loss: 0.506 lr: 0.000453\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 441 loss: 0.506 lr: 0.000452\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 442 loss: 0.510 lr: 0.000452\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 443 loss: 0.508 lr: 0.000451\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 444 loss: 0.510 lr: 0.000450\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 445 loss: 0.519 lr: 0.000450\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 446 loss: 0.513 lr: 0.000449\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 447 loss: 0.528 lr: 0.000448\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 448 loss: 0.526 lr: 0.000448\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 449 loss: 0.513 lr: 0.000447\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 450 loss: 0.515 lr: 0.000446\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 451 loss: 0.506 lr: 0.000445\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 452 loss: 0.511 lr: 0.000445\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 453 loss: 0.510 lr: 0.000444\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 454 loss: 0.510 lr: 0.000443\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 455 loss: 0.523 lr: 0.000443\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 456 loss: 0.519 lr: 0.000442\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 457 loss: 0.509 lr: 0.000441\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 458 loss: 0.535 lr: 0.000440\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 459 loss: 0.529 lr: 0.000440\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 460 loss: 0.515 lr: 0.000439\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 461 loss: 0.515 lr: 0.000438\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 462 loss: 0.520 lr: 0.000438\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 463 loss: 0.518 lr: 0.000437\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 464 loss: 0.519 lr: 0.000436\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 465 loss: 0.515 lr: 0.000435\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 466 loss: 0.510 lr: 0.000435\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 467 loss: 0.508 lr: 0.000434\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 468 loss: 0.513 lr: 0.000433\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 469 loss: 0.509 lr: 0.000432\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 470 loss: 0.510 lr: 0.000431\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 471 loss: 0.512 lr: 0.000431\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 472 loss: 0.519 lr: 0.000430\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 473 loss: 0.512 lr: 0.000429\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 474 loss: 0.506 lr: 0.000428\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 475 loss: 0.520 lr: 0.000428\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 476 loss: 0.526 lr: 0.000427\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 477 loss: 0.524 lr: 0.000426\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 478 loss: 0.511 lr: 0.000425\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 479 loss: 0.511 lr: 0.000424\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 480 loss: 0.507 lr: 0.000424\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 481 loss: 0.505 lr: 0.000423\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 482 loss: 0.509 lr: 0.000422\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 483 loss: 0.506 lr: 0.000421\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 484 loss: 0.506 lr: 0.000420\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 485 loss: 0.519 lr: 0.000419\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 486 loss: 0.544 lr: 0.000419\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 487 loss: 0.529 lr: 0.000418\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 488 loss: 0.529 lr: 0.000417\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 489 loss: 0.526 lr: 0.000416\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 490 loss: 0.530 lr: 0.000415\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 491 loss: 0.525 lr: 0.000414\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 492 loss: 0.521 lr: 0.000414\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 493 loss: 0.519 lr: 0.000413\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 494 loss: 0.517 lr: 0.000412\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 495 loss: 0.517 lr: 0.000411\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 496 loss: 0.518 lr: 0.000410\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 497 loss: 0.512 lr: 0.000409\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 498 loss: 0.534 lr: 0.000408\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 499 loss: 0.548 lr: 0.000408\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 500 loss: 0.535 lr: 0.000407\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 501 loss: 0.525 lr: 0.000406\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 502 loss: 0.531 lr: 0.000405\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 503 loss: 0.532 lr: 0.000404\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 504 loss: 0.522 lr: 0.000403\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 505 loss: 0.517 lr: 0.000402\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 506 loss: 0.514 lr: 0.000401\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 507 loss: 0.536 lr: 0.000401\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 508 loss: 0.528 lr: 0.000400\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 509 loss: 0.522 lr: 0.000399\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 510 loss: 0.519 lr: 0.000398\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 511 loss: 0.518 lr: 0.000397\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 512 loss: 0.517 lr: 0.000396\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 513 loss: 0.516 lr: 0.000395\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 514 loss: 0.517 lr: 0.000394\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 515 loss: 0.534 lr: 0.000393\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 516 loss: 0.552 lr: 0.000392\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 517 loss: 0.533 lr: 0.000391\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 518 loss: 0.531 lr: 0.000391\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 519 loss: 0.525 lr: 0.000390\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 520 loss: 0.523 lr: 0.000389\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 521 loss: 0.518 lr: 0.000388\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 522 loss: 0.521 lr: 0.000387\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 523 loss: 0.516 lr: 0.000386\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 524 loss: 0.513 lr: 0.000385\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 525 loss: 0.523 lr: 0.000384\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 526 loss: 0.523 lr: 0.000383\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 527 loss: 0.519 lr: 0.000382\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 528 loss: 0.520 lr: 0.000381\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 529 loss: 0.517 lr: 0.000380\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 530 loss: 0.524 lr: 0.000379\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 531 loss: 0.518 lr: 0.000378\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 532 loss: 0.515 lr: 0.000377\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 533 loss: 0.512 lr: 0.000376\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 534 loss: 0.510 lr: 0.000375\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 535 loss: 0.527 lr: 0.000374\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 536 loss: 0.509 lr: 0.000373\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 537 loss: 0.513 lr: 0.000372\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 538 loss: 0.520 lr: 0.000371\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 539 loss: 0.507 lr: 0.000370\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 540 loss: 0.516 lr: 0.000369\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 541 loss: 0.509 lr: 0.000368\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 542 loss: 0.518 lr: 0.000367\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 543 loss: 0.513 lr: 0.000366\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 544 loss: 0.506 lr: 0.000365\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 545 loss: 0.522 lr: 0.000364\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 546 loss: 0.507 lr: 0.000363\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 547 loss: 0.511 lr: 0.000362\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 548 loss: 0.517 lr: 0.000361\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 549 loss: 0.525 lr: 0.000360\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 550 loss: 0.511 lr: 0.000359\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 551 loss: 0.519 lr: 0.000358\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 552 loss: 0.510 lr: 0.000357\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 553 loss: 0.513 lr: 0.000356\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 554 loss: 0.511 lr: 0.000355\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 555 loss: 0.544 lr: 0.000354\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 556 loss: 0.532 lr: 0.000353\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 557 loss: 0.521 lr: 0.000352\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 558 loss: 0.519 lr: 0.000351\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 559 loss: 0.520 lr: 0.000350\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 560 loss: 0.512 lr: 0.000349\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 561 loss: 0.510 lr: 0.000348\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 562 loss: 0.528 lr: 0.000347\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 563 loss: 0.511 lr: 0.000346\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 564 loss: 0.510 lr: 0.000345\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 565 loss: 0.511 lr: 0.000344\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 566 loss: 0.509 lr: 0.000343\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 567 loss: 0.517 lr: 0.000342\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 568 loss: 0.519 lr: 0.000341\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 569 loss: 0.507 lr: 0.000340\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 570 loss: 0.507 lr: 0.000339\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 571 loss: 0.508 lr: 0.000338\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 572 loss: 0.511 lr: 0.000337\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 573 loss: 0.507 lr: 0.000336\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 574 loss: 0.517 lr: 0.000335\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 575 loss: 0.504 lr: 0.000334\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 576 loss: 0.514 lr: 0.000333\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 577 loss: 0.508 lr: 0.000331\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 578 loss: 0.509 lr: 0.000330\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 579 loss: 0.548 lr: 0.000329\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 580 loss: 0.543 lr: 0.000328\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 581 loss: 0.521 lr: 0.000327\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 582 loss: 0.511 lr: 0.000326\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 583 loss: 0.511 lr: 0.000325\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 584 loss: 0.505 lr: 0.000324\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 585 loss: 0.509 lr: 0.000323\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 586 loss: 0.506 lr: 0.000322\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 587 loss: 0.516 lr: 0.000321\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 588 loss: 0.520 lr: 0.000320\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 589 loss: 0.509 lr: 0.000319\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 590 loss: 0.506 lr: 0.000318\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 591 loss: 0.504 lr: 0.000316\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 592 loss: 0.520 lr: 0.000315\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 593 loss: 0.517 lr: 0.000314\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 594 loss: 0.508 lr: 0.000313\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 595 loss: 0.517 lr: 0.000312\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 596 loss: 0.509 lr: 0.000311\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 597 loss: 0.524 lr: 0.000310\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 598 loss: 0.529 lr: 0.000309\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 599 loss: 0.522 lr: 0.000308\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 600 loss: 0.532 lr: 0.000307\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 601 loss: 0.530 lr: 0.000306\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 602 loss: 0.530 lr: 0.000305\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 603 loss: 0.527 lr: 0.000303\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 604 loss: 0.527 lr: 0.000302\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 605 loss: 0.526 lr: 0.000301\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 606 loss: 0.517 lr: 0.000300\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 607 loss: 0.513 lr: 0.000299\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 608 loss: 0.527 lr: 0.000298\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 609 loss: 0.522 lr: 0.000297\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 610 loss: 0.516 lr: 0.000296\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 611 loss: 0.516 lr: 0.000295\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 612 loss: 0.528 lr: 0.000294\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 613 loss: 0.536 lr: 0.000292\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 614 loss: 0.515 lr: 0.000291\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 615 loss: 0.506 lr: 0.000290\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 616 loss: 0.514 lr: 0.000289\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 617 loss: 0.510 lr: 0.000288\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 618 loss: 0.507 lr: 0.000287\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 619 loss: 0.507 lr: 0.000286\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 620 loss: 0.539 lr: 0.000285\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 621 loss: 0.520 lr: 0.000284\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 622 loss: 0.511 lr: 0.000282\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 623 loss: 0.506 lr: 0.000281\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 624 loss: 0.509 lr: 0.000280\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 625 loss: 0.508 lr: 0.000279\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 626 loss: 0.507 lr: 0.000278\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 627 loss: 0.507 lr: 0.000277\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 628 loss: 0.510 lr: 0.000276\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 629 loss: 0.539 lr: 0.000275\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 630 loss: 0.518 lr: 0.000274\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 631 loss: 0.513 lr: 0.000272\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 632 loss: 0.503 lr: 0.000271\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 633 loss: 0.505 lr: 0.000270\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 634 loss: 0.505 lr: 0.000269\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 635 loss: 0.503 lr: 0.000268\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 636 loss: 0.516 lr: 0.000267\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 637 loss: 0.505 lr: 0.000266\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 638 loss: 0.504 lr: 0.000265\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 639 loss: 0.503 lr: 0.000263\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 640 loss: 0.504 lr: 0.000262\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 641 loss: 0.504 lr: 0.000261\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 642 loss: 0.502 lr: 0.000260\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 643 loss: 0.504 lr: 0.000259\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 644 loss: 0.520 lr: 0.000258\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 645 loss: 0.503 lr: 0.000257\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 646 loss: 0.509 lr: 0.000256\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 647 loss: 0.504 lr: 0.000254\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 648 loss: 0.500 lr: 0.000253\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 649 loss: 0.502 lr: 0.000252\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 650 loss: 0.503 lr: 0.000251\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 651 loss: 0.505 lr: 0.000250\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 652 loss: 0.508 lr: 0.000249\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 653 loss: 0.509 lr: 0.000248\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 654 loss: 0.505 lr: 0.000247\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 655 loss: 0.501 lr: 0.000245\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 656 loss: 0.506 lr: 0.000244\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 657 loss: 0.506 lr: 0.000243\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 658 loss: 0.506 lr: 0.000242\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 659 loss: 0.506 lr: 0.000241\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 660 loss: 0.504 lr: 0.000240\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 661 loss: 0.500 lr: 0.000239\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 662 loss: 0.504 lr: 0.000238\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 663 loss: 0.517 lr: 0.000237\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 664 loss: 0.508 lr: 0.000235\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 665 loss: 0.504 lr: 0.000234\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 666 loss: 0.500 lr: 0.000233\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 667 loss: 0.501 lr: 0.000232\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 668 loss: 0.518 lr: 0.000231\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 669 loss: 0.503 lr: 0.000230\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 670 loss: 0.518 lr: 0.000229\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 671 loss: 0.502 lr: 0.000228\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 672 loss: 0.504 lr: 0.000226\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 673 loss: 0.500 lr: 0.000225\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 674 loss: 0.505 lr: 0.000224\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 675 loss: 0.506 lr: 0.000223\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 676 loss: 0.503 lr: 0.000222\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 677 loss: 0.504 lr: 0.000221\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 678 loss: 0.504 lr: 0.000220\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 679 loss: 0.510 lr: 0.000219\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 680 loss: 0.511 lr: 0.000218\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 681 loss: 0.503 lr: 0.000216\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 682 loss: 0.502 lr: 0.000215\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 683 loss: 0.501 lr: 0.000214\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 684 loss: 0.505 lr: 0.000213\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 685 loss: 0.503 lr: 0.000212\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 686 loss: 0.502 lr: 0.000211\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 687 loss: 0.503 lr: 0.000210\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 688 loss: 0.501 lr: 0.000209\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 689 loss: 0.504 lr: 0.000208\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 690 loss: 0.502 lr: 0.000206\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 691 loss: 0.515 lr: 0.000205\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 692 loss: 0.505 lr: 0.000204\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 693 loss: 0.510 lr: 0.000203\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 694 loss: 0.502 lr: 0.000202\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 695 loss: 0.502 lr: 0.000201\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 696 loss: 0.506 lr: 0.000200\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 697 loss: 0.504 lr: 0.000199\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 698 loss: 0.503 lr: 0.000198\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 699 loss: 0.501 lr: 0.000197\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 700 loss: 0.502 lr: 0.000195\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 701 loss: 0.513 lr: 0.000194\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 702 loss: 0.506 lr: 0.000193\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 703 loss: 0.504 lr: 0.000192\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 704 loss: 0.504 lr: 0.000191\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 705 loss: 0.503 lr: 0.000190\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 706 loss: 0.502 lr: 0.000189\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 707 loss: 0.526 lr: 0.000188\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 708 loss: 0.525 lr: 0.000187\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 709 loss: 0.511 lr: 0.000186\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 710 loss: 0.507 lr: 0.000185\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 711 loss: 0.503 lr: 0.000183\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 712 loss: 0.503 lr: 0.000182\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 713 loss: 0.501 lr: 0.000181\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 714 loss: 0.502 lr: 0.000180\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 715 loss: 0.501 lr: 0.000179\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 716 loss: 0.500 lr: 0.000178\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 717 loss: 0.501 lr: 0.000177\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 718 loss: 0.502 lr: 0.000176\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 719 loss: 0.503 lr: 0.000175\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 720 loss: 0.504 lr: 0.000174\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 721 loss: 0.500 lr: 0.000173\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 722 loss: 0.501 lr: 0.000172\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 723 loss: 0.509 lr: 0.000171\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 724 loss: 0.511 lr: 0.000170\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 725 loss: 0.506 lr: 0.000168\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 726 loss: 0.501 lr: 0.000167\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 727 loss: 0.502 lr: 0.000166\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 728 loss: 0.503 lr: 0.000165\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 729 loss: 0.502 lr: 0.000164\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 730 loss: 0.501 lr: 0.000163\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 731 loss: 0.500 lr: 0.000162\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 732 loss: 0.498 lr: 0.000161\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 733 loss: 0.511 lr: 0.000160\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 734 loss: 0.503 lr: 0.000159\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 735 loss: 0.499 lr: 0.000158\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 736 loss: 0.505 lr: 0.000157\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 737 loss: 0.513 lr: 0.000156\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 738 loss: 0.504 lr: 0.000155\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 739 loss: 0.501 lr: 0.000154\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 740 loss: 0.505 lr: 0.000153\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 741 loss: 0.501 lr: 0.000152\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 742 loss: 0.504 lr: 0.000151\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 743 loss: 0.499 lr: 0.000150\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 744 loss: 0.499 lr: 0.000149\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 745 loss: 0.499 lr: 0.000148\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 746 loss: 0.499 lr: 0.000147\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 747 loss: 0.503 lr: 0.000146\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 748 loss: 0.500 lr: 0.000145\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 749 loss: 0.500 lr: 0.000144\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 750 loss: 0.499 lr: 0.000143\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 751 loss: 0.500 lr: 0.000142\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 752 loss: 0.503 lr: 0.000141\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 753 loss: 0.506 lr: 0.000139\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 754 loss: 0.511 lr: 0.000138\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 755 loss: 0.500 lr: 0.000137\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 756 loss: 0.500 lr: 0.000136\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 757 loss: 0.500 lr: 0.000135\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 758 loss: 0.503 lr: 0.000134\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 759 loss: 0.503 lr: 0.000133\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 760 loss: 0.500 lr: 0.000133\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 761 loss: 0.497 lr: 0.000132\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 762 loss: 0.505 lr: 0.000131\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 763 loss: 0.498 lr: 0.000130\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 764 loss: 0.498 lr: 0.000129\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 765 loss: 0.498 lr: 0.000128\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 766 loss: 0.500 lr: 0.000127\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 767 loss: 0.500 lr: 0.000126\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 768 loss: 0.499 lr: 0.000125\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 769 loss: 0.499 lr: 0.000124\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 770 loss: 0.500 lr: 0.000123\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 771 loss: 0.502 lr: 0.000122\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 772 loss: 0.497 lr: 0.000121\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 773 loss: 0.497 lr: 0.000120\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 774 loss: 0.498 lr: 0.000119\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 775 loss: 0.498 lr: 0.000118\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 776 loss: 0.497 lr: 0.000117\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 777 loss: 0.500 lr: 0.000116\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 778 loss: 0.499 lr: 0.000115\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 779 loss: 0.498 lr: 0.000114\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 780 loss: 0.498 lr: 0.000113\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 781 loss: 0.500 lr: 0.000112\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 782 loss: 0.497 lr: 0.000111\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 783 loss: 0.497 lr: 0.000110\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 784 loss: 0.496 lr: 0.000109\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 785 loss: 0.498 lr: 0.000109\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 786 loss: 0.494 lr: 0.000108\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 787 loss: 0.497 lr: 0.000107\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 788 loss: 0.497 lr: 0.000106\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 789 loss: 0.497 lr: 0.000105\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 790 loss: 0.498 lr: 0.000104\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 791 loss: 0.498 lr: 0.000103\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 792 loss: 0.497 lr: 0.000102\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 793 loss: 0.496 lr: 0.000101\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 794 loss: 0.494 lr: 0.000100\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 795 loss: 0.495 lr: 0.000099\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 796 loss: 0.498 lr: 0.000099\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 797 loss: 0.498 lr: 0.000098\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 798 loss: 0.496 lr: 0.000097\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 799 loss: 0.497 lr: 0.000096\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 800 loss: 0.496 lr: 0.000095\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 801 loss: 0.498 lr: 0.000094\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 802 loss: 0.497 lr: 0.000093\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 803 loss: 0.497 lr: 0.000092\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 804 loss: 0.494 lr: 0.000091\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 805 loss: 0.495 lr: 0.000091\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 806 loss: 0.496 lr: 0.000090\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 807 loss: 0.496 lr: 0.000089\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 808 loss: 0.496 lr: 0.000088\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 809 loss: 0.497 lr: 0.000087\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 810 loss: 0.495 lr: 0.000086\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 811 loss: 0.495 lr: 0.000086\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 812 loss: 0.494 lr: 0.000085\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 813 loss: 0.493 lr: 0.000084\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 814 loss: 0.494 lr: 0.000083\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 815 loss: 0.496 lr: 0.000082\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 816 loss: 0.495 lr: 0.000081\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 817 loss: 0.496 lr: 0.000080\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 818 loss: 0.496 lr: 0.000080\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 819 loss: 0.497 lr: 0.000079\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 820 loss: 0.497 lr: 0.000078\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 821 loss: 0.495 lr: 0.000077\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 822 loss: 0.495 lr: 0.000076\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 823 loss: 0.495 lr: 0.000076\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 824 loss: 0.497 lr: 0.000075\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 825 loss: 0.495 lr: 0.000074\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 826 loss: 0.494 lr: 0.000073\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 827 loss: 0.496 lr: 0.000072\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 828 loss: 0.491 lr: 0.000072\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 829 loss: 0.496 lr: 0.000071\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 830 loss: 0.496 lr: 0.000070\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 831 loss: 0.496 lr: 0.000069\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 832 loss: 0.499 lr: 0.000069\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 833 loss: 0.494 lr: 0.000068\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 834 loss: 0.496 lr: 0.000067\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 835 loss: 0.495 lr: 0.000066\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 836 loss: 0.492 lr: 0.000065\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 837 loss: 0.495 lr: 0.000065\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 838 loss: 0.494 lr: 0.000064\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 839 loss: 0.493 lr: 0.000063\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 840 loss: 0.492 lr: 0.000062\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 841 loss: 0.494 lr: 0.000062\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 842 loss: 0.491 lr: 0.000061\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 843 loss: 0.495 lr: 0.000060\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 844 loss: 0.494 lr: 0.000060\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 845 loss: 0.492 lr: 0.000059\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 846 loss: 0.494 lr: 0.000058\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 847 loss: 0.493 lr: 0.000057\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 848 loss: 0.496 lr: 0.000057\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 849 loss: 0.497 lr: 0.000056\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 850 loss: 0.495 lr: 0.000055\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 851 loss: 0.493 lr: 0.000055\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 852 loss: 0.494 lr: 0.000054\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 853 loss: 0.495 lr: 0.000053\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 854 loss: 0.495 lr: 0.000052\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 855 loss: 0.495 lr: 0.000052\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 856 loss: 0.494 lr: 0.000051\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 857 loss: 0.494 lr: 0.000050\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 858 loss: 0.492 lr: 0.000050\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 859 loss: 0.495 lr: 0.000049\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 860 loss: 0.491 lr: 0.000048\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 861 loss: 0.492 lr: 0.000048\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 862 loss: 0.492 lr: 0.000047\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 863 loss: 0.495 lr: 0.000046\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 864 loss: 0.493 lr: 0.000046\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 865 loss: 0.496 lr: 0.000045\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 866 loss: 0.491 lr: 0.000044\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 867 loss: 0.494 lr: 0.000044\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 868 loss: 0.491 lr: 0.000043\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 869 loss: 0.495 lr: 0.000043\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 870 loss: 0.493 lr: 0.000042\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 871 loss: 0.500 lr: 0.000041\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 872 loss: 0.503 lr: 0.000041\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 873 loss: 0.498 lr: 0.000040\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 874 loss: 0.494 lr: 0.000040\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 875 loss: 0.495 lr: 0.000039\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 876 loss: 0.492 lr: 0.000038\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 877 loss: 0.497 lr: 0.000038\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 878 loss: 0.493 lr: 0.000037\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 879 loss: 0.492 lr: 0.000037\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 880 loss: 0.493 lr: 0.000036\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 881 loss: 0.493 lr: 0.000035\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 882 loss: 0.492 lr: 0.000035\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 883 loss: 0.496 lr: 0.000034\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 884 loss: 0.494 lr: 0.000034\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 885 loss: 0.495 lr: 0.000033\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 886 loss: 0.491 lr: 0.000033\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 887 loss: 0.493 lr: 0.000032\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 888 loss: 0.493 lr: 0.000031\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 889 loss: 0.495 lr: 0.000031\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 890 loss: 0.492 lr: 0.000030\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 891 loss: 0.494 lr: 0.000030\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 892 loss: 0.493 lr: 0.000029\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 893 loss: 0.491 lr: 0.000029\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 894 loss: 0.492 lr: 0.000028\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 895 loss: 0.493 lr: 0.000028\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 896 loss: 0.495 lr: 0.000027\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 897 loss: 0.495 lr: 0.000027\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 898 loss: 0.492 lr: 0.000026\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 899 loss: 0.492 lr: 0.000026\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 900 loss: 0.491 lr: 0.000025\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 901 loss: 0.491 lr: 0.000025\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 902 loss: 0.495 lr: 0.000024\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 903 loss: 0.489 lr: 0.000024\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 904 loss: 0.495 lr: 0.000023\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 905 loss: 0.491 lr: 0.000023\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 906 loss: 0.492 lr: 0.000022\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 907 loss: 0.494 lr: 0.000022\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 908 loss: 0.493 lr: 0.000021\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 909 loss: 0.496 lr: 0.000021\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 910 loss: 0.491 lr: 0.000021\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 911 loss: 0.494 lr: 0.000020\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 912 loss: 0.493 lr: 0.000020\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 913 loss: 0.493 lr: 0.000019\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 914 loss: 0.493 lr: 0.000019\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 915 loss: 0.495 lr: 0.000018\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 916 loss: 0.492 lr: 0.000018\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 917 loss: 0.494 lr: 0.000018\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 918 loss: 0.496 lr: 0.000017\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 919 loss: 0.493 lr: 0.000017\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 920 loss: 0.495 lr: 0.000016\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 921 loss: 0.490 lr: 0.000016\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 922 loss: 0.490 lr: 0.000016\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 923 loss: 0.492 lr: 0.000015\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 924 loss: 0.492 lr: 0.000015\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 925 loss: 0.494 lr: 0.000014\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 926 loss: 0.494 lr: 0.000014\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 927 loss: 0.493 lr: 0.000014\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 928 loss: 0.492 lr: 0.000013\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 929 loss: 0.492 lr: 0.000013\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 930 loss: 0.495 lr: 0.000013\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 931 loss: 0.493 lr: 0.000012\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 932 loss: 0.492 lr: 0.000012\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 933 loss: 0.493 lr: 0.000012\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 934 loss: 0.491 lr: 0.000011\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 935 loss: 0.491 lr: 0.000011\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 936 loss: 0.492 lr: 0.000011\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 937 loss: 0.495 lr: 0.000010\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 938 loss: 0.495 lr: 0.000010\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 939 loss: 0.490 lr: 0.000010\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 940 loss: 0.492 lr: 0.000009\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 941 loss: 0.492 lr: 0.000009\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 942 loss: 0.494 lr: 0.000009\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 943 loss: 0.490 lr: 0.000008\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 944 loss: 0.490 lr: 0.000008\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 945 loss: 0.492 lr: 0.000008\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 946 loss: 0.494 lr: 0.000008\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 947 loss: 0.494 lr: 0.000007\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 948 loss: 0.492 lr: 0.000007\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 949 loss: 0.492 lr: 0.000007\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 950 loss: 0.492 lr: 0.000007\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 951 loss: 0.494 lr: 0.000006\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 952 loss: 0.494 lr: 0.000006\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 953 loss: 0.493 lr: 0.000006\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 954 loss: 0.495 lr: 0.000006\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 955 loss: 0.492 lr: 0.000005\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 956 loss: 0.490 lr: 0.000005\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 957 loss: 0.488 lr: 0.000005\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 958 loss: 0.492 lr: 0.000005\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 959 loss: 0.494 lr: 0.000004\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 960 loss: 0.492 lr: 0.000004\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 961 loss: 0.492 lr: 0.000004\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 962 loss: 0.492 lr: 0.000004\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 963 loss: 0.492 lr: 0.000004\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 964 loss: 0.491 lr: 0.000003\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 965 loss: 0.493 lr: 0.000003\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 966 loss: 0.492 lr: 0.000003\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 967 loss: 0.492 lr: 0.000003\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 968 loss: 0.493 lr: 0.000003\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 969 loss: 0.492 lr: 0.000003\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 970 loss: 0.494 lr: 0.000002\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 971 loss: 0.491 lr: 0.000002\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 972 loss: 0.492 lr: 0.000002\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 973 loss: 0.493 lr: 0.000002\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 974 loss: 0.493 lr: 0.000002\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 975 loss: 0.492 lr: 0.000002\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 976 loss: 0.493 lr: 0.000002\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 977 loss: 0.495 lr: 0.000001\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 978 loss: 0.489 lr: 0.000001\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 979 loss: 0.491 lr: 0.000001\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 980 loss: 0.494 lr: 0.000001\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 981 loss: 0.491 lr: 0.000001\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 982 loss: 0.491 lr: 0.000001\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 983 loss: 0.494 lr: 0.000001\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 984 loss: 0.489 lr: 0.000001\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 985 loss: 0.491 lr: 0.000001\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 986 loss: 0.493 lr: 0.000001\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 987 loss: 0.492 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 988 loss: 0.494 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 989 loss: 0.489 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 990 loss: 0.493 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 991 loss: 0.492 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 992 loss: 0.491 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 993 loss: 0.494 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 994 loss: 0.492 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 995 loss: 0.491 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 996 loss: 0.491 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 997 loss: 0.492 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 998 loss: 0.493 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 999 loss: 0.490 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n",
      "Epoch 1000 loss: 0.492 lr: 0.000000\n",
      "min loss:  0.45939733743667605\n"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "\n",
    "for epoch in range(config[\"EPOCHS\"]):\n",
    "    running_loss = 0.0\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels, weights = data\n",
    "        \n",
    "        inputs = inputs.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        \n",
    "        loss = feasibility_promoting_weighted_BCELoss(outputs, labels, weights, device=device)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d loss: %.3f lr: %.6f' % (epoch + 1, running_loss / len(train_loader), curr_lr))\n",
    "    \n",
    "    if len(loss_list) > 0:\n",
    "        print(\"min loss: \", min(loss_list))\n",
    "        if (running_loss / len(train_loader)) < min(loss_list):\n",
    "            torch.save(net.state_dict(), \"Data/corlat/models/MLP_corlat_feasibility_promoting_weighted_BCELoss.pth\")\n",
    "            print(\"Model saved\")\n",
    "    \n",
    "    loss_list.append(running_loss / len(train_loader))\n",
    "    \n",
    "    # if training loss is lower than previous loss, save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model\n",
    "net = NeuralNetwork()\n",
    "net.load_state_dict(torch.load(\"Data/corlat/models/MLP_corlat_feasibility_promoting_weighted_BCELoss.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=15847, out_features=1980, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=1980, out_features=990, bias=True)\n",
       "  (fc3): Linear(in_features=990, out_features=495, bias=True)\n",
       "  (fc4): Linear(in_features=495, out_features=100, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test number of feasible solutions\n",
    "# test the model on the test set\n",
    "net.eval()\n",
    "net.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing feasibility and time needed for optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for testing feasibility.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feasibility_test(batch_size, y_pred, test_models, indices):\n",
    "    \"\"\"\n",
    "    Function to test the feasibility of the solution predicted by the neural network.\n",
    "    For each instance in the test set, we will relax the binary variables to continuous variables with bounds of 0 and 1, and set the value of the binary variables to the value predicted by the neural network.\n",
    "    We will then compute the IIS to find the list of violated constraints and variables.\n",
    "    The number of violated constraints is the number of non zero elements in IISConstr.\n",
    "    \n",
    "    Args:\n",
    "    batch_size (int): batch size\n",
    "    y_pred (npt.NDArray): predictions of the neural network\n",
    "    test_models (List): list of gurobi models for each instance in the test set\n",
    "    indices (List): list of indices of binary variables\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    n_violated_constraints (List): list of number of violated constraints for each instance in the test set    \n",
    "    \"\"\"\n",
    "    \n",
    "    n_violated_constraints = []\n",
    "\n",
    "    # convert predictions of N_samples, N_variables to binary\n",
    "    y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "    \n",
    "    # Compute the weights for each training instance\n",
    "    for i in range(len(test_models)):\n",
    "        \n",
    "        model = test_models[i]\n",
    "        \n",
    "        modelVars = model.getVars()\n",
    "        \n",
    "        instanceBinaryIndices = indices\n",
    "\n",
    "        # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "        for j in range(len(instanceBinaryIndices)):\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "            # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "        \n",
    "        \n",
    "        # Compute the IIS to find the list of violated constraints and variables\n",
    "        try:\n",
    "            model.computeIIS()\n",
    "        except gb.GurobiError:\n",
    "            print(\"Model is feasible\")\n",
    "            n_violated_constraints.append(0)\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        # get number of violated constraints\n",
    "        IISConstr = model.getAttr(\"IISConstr\", model.getConstrs())\n",
    "\n",
    "        # count number of non zero elements in IISConstr        \n",
    "        n_violated_constraints.append(np.count_nonzero(IISConstr))\n",
    "        \n",
    "    return n_violated_constraints"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append the gurobi test instances into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-06-02\n"
     ]
    }
   ],
   "source": [
    "test_models = []\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "# model_files = os.listdir(\"instances/mip/data/COR-LAT\")\n",
    "model_files = pkl.load(open(\"Data/corlat/train_test_data/pickle_filenames.pkl\", \"rb\"))\n",
    "# rename from .pickle to .lp\n",
    "filename = []\n",
    "for i in range(len(model_files)):\n",
    "    filename.append(model_files[i].replace(\".pickle\", \".lp\"))\n",
    "\n",
    "for i in range(len(test_indices)):\n",
    "    model = gb.read(\"Data/corlat/instances/\" + filename[test_indices[i]], env=gurobi_env)\n",
    "    test_models.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model objective sense:  -1\n"
     ]
    }
   ],
   "source": [
    "model.ModelSense\n",
    "# if -1, minimize, if 1, maximize\n",
    "print(\"Model objective sense: \", model.ModelSense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', '=', '=', '=', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '<', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '=', '<', '=', '=', '=', '=', '=']\n"
     ]
    }
   ],
   "source": [
    "obj = model.getObjective()\n",
    "print(model.getAttr(\"Sense\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get number of violated constraints for each test instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "Model is feasible\n",
      "2\n",
      "Model is feasible\n",
      "3\n",
      "Model is feasible\n",
      "Model is feasible\n",
      "Model is feasible\n",
      "4\n",
      "Model is feasible\n",
      "5\n",
      "6\n",
      "Model is feasible\n",
      "Model is feasible\n",
      "7\n",
      "Model is feasible\n",
      "8\n",
      "9\n",
      "10\n",
      "Model is feasible\n",
      "11\n",
      "Model is feasible\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "n_violated_constraints = []\n",
    "for i, data in enumerate(valid_loader):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # get slices of test_models according to batch size\n",
    "    len_test_models = len(test_models)\n",
    "    print(i)\n",
    "    test_models_batch = test_models[i*batch_size: min((i+1)*batch_size, len_test_models)]\n",
    "    \n",
    "    n_violated_constraints_batch = feasibility_test(batch_size_test, outputs.detach().cpu().numpy(), test_models_batch, binary_indices)\n",
    "    \n",
    "    n_violated_constraints.append(n_violated_constraints_batch)\n",
    "    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten n_violated_constraints\n",
    "n_violated_constraints = [item for sublist in n_violated_constraints for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of violated constraints:  2.3575\n",
      "Length of n_violated_constraints:  400\n",
      "[24, 28, 1, 1, 1, 1, 1, 1, 27, 2, 2, 1, 1, 27, 36, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 27, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 77, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 14, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 11, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 47, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 14, 21, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 16, 1, 1, 2, 1, 1, 1, 2, 17, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 4, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 6, 1, 1, 1, 1, 21, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 22, 1, 23, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 38, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 13, 2, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Average number of violated constraints: \", np.mean(n_violated_constraints))\n",
    "print(\"Length of n_violated_constraints: \", len(n_violated_constraints))\n",
    "print(n_violated_constraints)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the average optimization time without any warm start or equality constrain set.\n",
    "\n",
    "The naive optimization routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-06-02\n",
      "Optimization time for model  0 :  0.20464301109313965\n",
      "Optimization time for model  1 :  2.814563035964966\n",
      "Optimization time for model  2 :  2.0997819900512695\n",
      "Optimization time for model  3 :  2.0235610008239746\n",
      "Optimization time for model  4 :  1.327219009399414\n",
      "Optimization time for model  5 :  0.9989018440246582\n",
      "Optimization time for model  6 :  1.5220320224761963\n",
      "Optimization time for model  7 :  0.9181039333343506\n",
      "Optimization time for model  8 :  0.00653386116027832\n",
      "Optimization time for model  9 :  2.900223970413208\n",
      "Optimization time for model  10 :  0.19867801666259766\n",
      "Optimization time for model  11 :  0.06766915321350098\n",
      "Optimization time for model  12 :  0.0037429332733154297\n",
      "Optimization time for model  13 :  0.22120189666748047\n",
      "Optimization time for model  14 :  0.0034461021423339844\n",
      "Optimization time for model  15 :  0.49202394485473633\n",
      "Optimization time for model  16 :  0.19829583168029785\n",
      "Optimization time for model  17 :  0.14036822319030762\n",
      "Optimization time for model  18 :  0.13608312606811523\n",
      "Optimization time for model  19 :  2.4442849159240723\n",
      "Optimization time for model  20 :  0.008024215698242188\n",
      "Optimization time for model  21 :  1.8369359970092773\n",
      "Optimization time for model  22 :  0.0036458969116210938\n",
      "Optimization time for model  23 :  0.015760183334350586\n",
      "Optimization time for model  24 :  0.457003116607666\n",
      "Optimization time for model  25 :  1.2788159847259521\n",
      "Optimization time for model  26 :  0.1430339813232422\n",
      "Optimization time for model  27 :  0.0007328987121582031\n",
      "Optimization time for model  28 :  1.4634819030761719\n",
      "Optimization time for model  29 :  1.0973949432373047\n",
      "Optimization time for model  30 :  1.4298348426818848\n",
      "Optimization time for model  31 :  13.045676946640015\n",
      "Optimization time for model  32 :  2.727139949798584\n",
      "Optimization time for model  33 :  0.08913588523864746\n",
      "Optimization time for model  34 :  1.698288917541504\n",
      "Optimization time for model  35 :  0.4097590446472168\n",
      "Optimization time for model  36 :  0.31667399406433105\n",
      "Optimization time for model  37 :  0.3603200912475586\n",
      "Optimization time for model  38 :  0.003506898880004883\n",
      "Optimization time for model  39 :  15.979418992996216\n",
      "Optimization time for model  40 :  1.176854133605957\n",
      "Optimization time for model  41 :  0.5088701248168945\n",
      "Optimization time for model  42 :  0.36824917793273926\n",
      "Optimization time for model  43 :  1.5931148529052734\n",
      "Optimization time for model  44 :  0.3648848533630371\n",
      "Optimization time for model  45 :  0.015681028366088867\n",
      "Optimization time for model  46 :  3.717355966567993\n",
      "Optimization time for model  47 :  1.2421560287475586\n",
      "Optimization time for model  48 :  4.661022901535034\n",
      "Optimization time for model  49 :  0.2486720085144043\n",
      "Optimization time for model  50 :  2.568063974380493\n",
      "Optimization time for model  51 :  0.023752927780151367\n",
      "Optimization time for model  52 :  0.0014979839324951172\n",
      "Optimization time for model  53 :  1.2156238555908203\n",
      "Optimization time for model  54 :  0.09454894065856934\n",
      "Optimization time for model  55 :  1.8125028610229492\n",
      "Optimization time for model  56 :  0.0016429424285888672\n",
      "Optimization time for model  57 :  0.162492036819458\n",
      "Optimization time for model  58 :  0.06314396858215332\n",
      "Optimization time for model  59 :  0.030852079391479492\n",
      "Optimization time for model  60 :  0.022285938262939453\n",
      "Optimization time for model  61 :  0.44489502906799316\n",
      "Optimization time for model  62 :  1.3135590553283691\n",
      "Optimization time for model  63 :  0.0007619857788085938\n",
      "Optimization time for model  64 :  3.5798439979553223\n",
      "Optimization time for model  65 :  2.595250129699707\n",
      "Optimization time for model  66 :  0.43692493438720703\n",
      "Optimization time for model  67 :  0.20810699462890625\n",
      "Optimization time for model  68 :  2.548197031021118\n",
      "Optimization time for model  69 :  0.00185394287109375\n",
      "Optimization time for model  70 :  5.430135011672974\n",
      "Optimization time for model  71 :  0.008466958999633789\n",
      "Optimization time for model  72 :  2.968801975250244\n",
      "Optimization time for model  73 :  1.0501749515533447\n",
      "Optimization time for model  74 :  0.9847538471221924\n",
      "Optimization time for model  75 :  0.0060079097747802734\n",
      "Optimization time for model  76 :  0.021263837814331055\n",
      "Optimization time for model  77 :  7.810209035873413\n",
      "Optimization time for model  78 :  6.535961151123047\n",
      "Optimization time for model  79 :  0.0030040740966796875\n",
      "Optimization time for model  80 :  7.487995862960815\n",
      "Optimization time for model  81 :  2.2190399169921875\n",
      "Optimization time for model  82 :  3.0151751041412354\n",
      "Optimization time for model  83 :  0.004706144332885742\n",
      "Optimization time for model  84 :  2.4114990234375\n",
      "Optimization time for model  85 :  1.2124481201171875\n",
      "Optimization time for model  86 :  2.975942850112915\n",
      "Optimization time for model  87 :  0.01734304428100586\n",
      "Optimization time for model  88 :  0.27487897872924805\n",
      "Optimization time for model  89 :  3.8729610443115234\n",
      "Optimization time for model  90 :  6.049504995346069\n",
      "Optimization time for model  91 :  2.180323839187622\n",
      "Optimization time for model  92 :  0.08720898628234863\n",
      "Optimization time for model  93 :  0.003498077392578125\n",
      "Optimization time for model  94 :  0.014939069747924805\n",
      "Optimization time for model  95 :  2.5213098526000977\n",
      "Optimization time for model  96 :  0.0008881092071533203\n",
      "Optimization time for model  97 :  0.07825899124145508\n",
      "Optimization time for model  98 :  0.00740814208984375\n",
      "Optimization time for model  99 :  6.92946982383728\n",
      "Optimization time for model  100 :  0.032132863998413086\n",
      "Optimization time for model  101 :  0.07506203651428223\n",
      "Optimization time for model  102 :  1.1250770092010498\n",
      "Optimization time for model  103 :  0.0036890506744384766\n",
      "Optimization time for model  104 :  0.0035669803619384766\n",
      "Optimization time for model  105 :  1.277177095413208\n",
      "Optimization time for model  106 :  4.114269971847534\n",
      "Optimization time for model  107 :  6.058897018432617\n",
      "Optimization time for model  108 :  2.689655065536499\n",
      "Optimization time for model  109 :  0.13527894020080566\n",
      "Optimization time for model  110 :  1.2533230781555176\n",
      "Optimization time for model  111 :  2.4991841316223145\n",
      "Optimization time for model  112 :  0.1422569751739502\n",
      "Optimization time for model  113 :  2.3405940532684326\n",
      "Optimization time for model  114 :  0.17609596252441406\n",
      "Optimization time for model  115 :  0.7508158683776855\n",
      "Optimization time for model  116 :  0.0035800933837890625\n",
      "Optimization time for model  117 :  0.22583818435668945\n",
      "Optimization time for model  118 :  5.04919695854187\n",
      "Optimization time for model  119 :  1.3915648460388184\n",
      "Optimization time for model  120 :  0.003676891326904297\n",
      "Optimization time for model  121 :  0.10744810104370117\n",
      "Optimization time for model  122 :  3.654611110687256\n",
      "Optimization time for model  123 :  0.02281498908996582\n",
      "Optimization time for model  124 :  0.6035652160644531\n",
      "Optimization time for model  125 :  6.527489900588989\n",
      "Optimization time for model  126 :  1.896967887878418\n",
      "Optimization time for model  127 :  0.003480195999145508\n",
      "Optimization time for model  128 :  1.2393670082092285\n",
      "Optimization time for model  129 :  0.3290669918060303\n",
      "Optimization time for model  130 :  0.0030879974365234375\n",
      "Optimization time for model  131 :  0.05424809455871582\n",
      "Optimization time for model  132 :  2.0672600269317627\n",
      "Optimization time for model  133 :  4.8844990730285645\n",
      "Optimization time for model  134 :  9.998366117477417\n",
      "Optimization time for model  135 :  0.16545605659484863\n",
      "Optimization time for model  136 :  0.08407807350158691\n",
      "Optimization time for model  137 :  0.003670930862426758\n",
      "Optimization time for model  138 :  3.4791760444641113\n",
      "Optimization time for model  139 :  0.24076604843139648\n",
      "Optimization time for model  140 :  0.3185770511627197\n",
      "Optimization time for model  141 :  0.05488300323486328\n",
      "Optimization time for model  142 :  0.2349851131439209\n",
      "Optimization time for model  143 :  3.7244760990142822\n",
      "Optimization time for model  144 :  0.0036559104919433594\n",
      "Optimization time for model  145 :  0.12520909309387207\n",
      "Optimization time for model  146 :  1.1404931545257568\n",
      "Optimization time for model  147 :  5.898929834365845\n",
      "Optimization time for model  148 :  0.025256872177124023\n",
      "Optimization time for model  149 :  6.694936990737915\n",
      "Optimization time for model  150 :  0.08206486701965332\n",
      "Optimization time for model  151 :  0.4317638874053955\n",
      "Optimization time for model  152 :  0.12450480461120605\n",
      "Optimization time for model  153 :  0.12073993682861328\n",
      "Optimization time for model  154 :  0.24941396713256836\n",
      "Optimization time for model  155 :  1.7860901355743408\n",
      "Optimization time for model  156 :  1.0816409587860107\n",
      "Optimization time for model  157 :  1.8485841751098633\n",
      "Optimization time for model  158 :  2.1587488651275635\n",
      "Optimization time for model  159 :  0.8993978500366211\n",
      "Optimization time for model  160 :  1.9633920192718506\n",
      "Optimization time for model  161 :  0.1088869571685791\n",
      "Optimization time for model  162 :  0.1260240077972412\n",
      "Optimization time for model  163 :  0.514693021774292\n",
      "Optimization time for model  164 :  0.035262107849121094\n",
      "Optimization time for model  165 :  1.0911979675292969\n",
      "Optimization time for model  166 :  0.04214787483215332\n",
      "Optimization time for model  167 :  0.6922461986541748\n",
      "Optimization time for model  168 :  0.09640288352966309\n",
      "Optimization time for model  169 :  0.8291940689086914\n",
      "Optimization time for model  170 :  0.2269430160522461\n",
      "Optimization time for model  171 :  0.3587000370025635\n",
      "Optimization time for model  172 :  0.29331111907958984\n",
      "Optimization time for model  173 :  0.7467460632324219\n",
      "Optimization time for model  174 :  0.1147007942199707\n",
      "Optimization time for model  175 :  1.7701258659362793\n",
      "Optimization time for model  176 :  0.07084417343139648\n",
      "Optimization time for model  177 :  1.7924602031707764\n",
      "Optimization time for model  178 :  1.863619089126587\n",
      "Optimization time for model  179 :  0.09949111938476562\n",
      "Optimization time for model  180 :  0.9915370941162109\n",
      "Optimization time for model  181 :  0.3464040756225586\n",
      "Optimization time for model  182 :  0.37911391258239746\n",
      "Optimization time for model  183 :  4.039459943771362\n",
      "Optimization time for model  184 :  0.4786570072174072\n",
      "Optimization time for model  185 :  0.2249290943145752\n",
      "Optimization time for model  186 :  0.31346893310546875\n",
      "Optimization time for model  187 :  0.0007278919219970703\n",
      "Optimization time for model  188 :  29.901158094406128\n",
      "Optimization time for model  189 :  1.0515670776367188\n",
      "Optimization time for model  190 :  0.08463692665100098\n",
      "Optimization time for model  191 :  3.945935010910034\n",
      "Optimization time for model  192 :  0.0022330284118652344\n",
      "Optimization time for model  193 :  1.308812141418457\n",
      "Optimization time for model  194 :  0.37274599075317383\n",
      "Optimization time for model  195 :  4.7643067836761475\n",
      "Optimization time for model  196 :  0.26540708541870117\n",
      "Optimization time for model  197 :  0.18911004066467285\n",
      "Optimization time for model  198 :  0.003412008285522461\n",
      "Optimization time for model  199 :  0.011054039001464844\n",
      "Optimization time for model  200 :  0.3612642288208008\n",
      "Optimization time for model  201 :  0.11721301078796387\n",
      "Optimization time for model  202 :  0.015696048736572266\n",
      "Optimization time for model  203 :  0.9390349388122559\n",
      "Optimization time for model  204 :  3.04201078414917\n",
      "Optimization time for model  205 :  0.0034999847412109375\n",
      "Optimization time for model  206 :  0.003531932830810547\n",
      "Optimization time for model  207 :  0.9456210136413574\n",
      "Optimization time for model  208 :  0.9957921504974365\n",
      "Optimization time for model  209 :  0.3541140556335449\n",
      "Optimization time for model  210 :  3.180820941925049\n",
      "Optimization time for model  211 :  0.23028802871704102\n",
      "Optimization time for model  212 :  1.8574130535125732\n",
      "Optimization time for model  213 :  0.02401113510131836\n",
      "Optimization time for model  214 :  0.009009838104248047\n",
      "Optimization time for model  215 :  0.012987852096557617\n",
      "Optimization time for model  216 :  1.345376968383789\n",
      "Optimization time for model  217 :  0.09118986129760742\n",
      "Optimization time for model  218 :  0.17208003997802734\n",
      "Optimization time for model  219 :  2.315512180328369\n",
      "Optimization time for model  220 :  2.925420045852661\n",
      "Optimization time for model  221 :  1.8242299556732178\n",
      "Optimization time for model  222 :  0.00341796875\n",
      "Optimization time for model  223 :  0.8366930484771729\n",
      "Optimization time for model  224 :  0.20630502700805664\n",
      "Optimization time for model  225 :  0.02934885025024414\n",
      "Optimization time for model  226 :  0.3902769088745117\n",
      "Optimization time for model  227 :  3.465736150741577\n",
      "Optimization time for model  228 :  0.44793105125427246\n",
      "Optimization time for model  229 :  5.8802101612091064\n",
      "Optimization time for model  230 :  0.09810495376586914\n",
      "Optimization time for model  231 :  3.490989923477173\n",
      "Optimization time for model  232 :  0.28637099266052246\n",
      "Optimization time for model  233 :  0.019433975219726562\n",
      "Optimization time for model  234 :  0.2910940647125244\n",
      "Optimization time for model  235 :  0.07666516304016113\n",
      "Optimization time for model  236 :  6.050060987472534\n",
      "Optimization time for model  237 :  0.21895623207092285\n",
      "Optimization time for model  238 :  0.010516881942749023\n",
      "Optimization time for model  239 :  0.6575760841369629\n",
      "Optimization time for model  240 :  6.488870143890381\n",
      "Optimization time for model  241 :  0.003564119338989258\n",
      "Optimization time for model  242 :  3.62808895111084\n",
      "Optimization time for model  243 :  5.905509948730469\n",
      "Optimization time for model  244 :  1.4209380149841309\n",
      "Optimization time for model  245 :  0.0014901161193847656\n",
      "Optimization time for model  246 :  0.003498077392578125\n",
      "Optimization time for model  247 :  0.17128705978393555\n",
      "Optimization time for model  248 :  3.677799940109253\n",
      "Optimization time for model  249 :  3.769973039627075\n",
      "Optimization time for model  250 :  0.4921150207519531\n",
      "Optimization time for model  251 :  5.0343029499053955\n",
      "Optimization time for model  252 :  0.5909938812255859\n",
      "Optimization time for model  253 :  2.072736978530884\n",
      "Optimization time for model  254 :  0.12016487121582031\n",
      "Optimization time for model  255 :  0.0012578964233398438\n",
      "Optimization time for model  256 :  0.2018439769744873\n",
      "Optimization time for model  257 :  1.3220820426940918\n",
      "Optimization time for model  258 :  9.213742971420288\n",
      "Optimization time for model  259 :  0.003554105758666992\n",
      "Optimization time for model  260 :  0.05176687240600586\n",
      "Optimization time for model  261 :  0.3563401699066162\n",
      "Optimization time for model  262 :  2.4694581031799316\n",
      "Optimization time for model  263 :  11.716548919677734\n",
      "Optimization time for model  264 :  0.10667991638183594\n",
      "Optimization time for model  265 :  2.1640870571136475\n",
      "Optimization time for model  266 :  1.3246848583221436\n",
      "Optimization time for model  267 :  8.226633071899414\n",
      "Optimization time for model  268 :  0.24899506568908691\n",
      "Optimization time for model  269 :  10.672904014587402\n",
      "Optimization time for model  270 :  1.29681396484375\n",
      "Optimization time for model  271 :  10.185012102127075\n",
      "Optimization time for model  272 :  3.039367914199829\n",
      "Optimization time for model  273 :  0.25582098960876465\n",
      "Optimization time for model  274 :  0.6921241283416748\n",
      "Optimization time for model  275 :  17.540693998336792\n",
      "Optimization time for model  276 :  0.17953109741210938\n",
      "Optimization time for model  277 :  0.7154648303985596\n",
      "Optimization time for model  278 :  0.6841199398040771\n",
      "Optimization time for model  279 :  0.13105201721191406\n",
      "Optimization time for model  280 :  2.254155158996582\n",
      "Optimization time for model  281 :  0.5428750514984131\n",
      "Optimization time for model  282 :  0.06567811965942383\n",
      "Optimization time for model  283 :  0.12237787246704102\n",
      "Optimization time for model  284 :  2.1661489009857178\n",
      "Optimization time for model  285 :  0.17792820930480957\n",
      "Optimization time for model  286 :  0.3395090103149414\n",
      "Optimization time for model  287 :  9.084203004837036\n",
      "Optimization time for model  288 :  0.42910003662109375\n",
      "Optimization time for model  289 :  0.3749082088470459\n",
      "Optimization time for model  290 :  0.3970930576324463\n",
      "Optimization time for model  291 :  5.024651050567627\n",
      "Optimization time for model  292 :  0.003606081008911133\n",
      "Optimization time for model  293 :  0.003556966781616211\n",
      "Optimization time for model  294 :  0.3434131145477295\n",
      "Optimization time for model  295 :  2.5936098098754883\n",
      "Optimization time for model  296 :  1.2378218173980713\n",
      "Optimization time for model  297 :  0.5022599697113037\n",
      "Optimization time for model  298 :  0.2659618854522705\n",
      "Optimization time for model  299 :  0.0757601261138916\n",
      "Optimization time for model  300 :  3.1075549125671387\n",
      "Optimization time for model  301 :  0.4151880741119385\n",
      "Optimization time for model  302 :  1.326479196548462\n",
      "Optimization time for model  303 :  3.508314847946167\n",
      "Optimization time for model  304 :  0.17868709564208984\n",
      "Optimization time for model  305 :  2.5230720043182373\n",
      "Optimization time for model  306 :  0.45166802406311035\n",
      "Optimization time for model  307 :  0.1373729705810547\n",
      "Optimization time for model  308 :  2.7135770320892334\n",
      "Optimization time for model  309 :  1.7691969871520996\n",
      "Optimization time for model  310 :  0.0035479068756103516\n",
      "Optimization time for model  311 :  0.5105991363525391\n",
      "Optimization time for model  312 :  0.29491281509399414\n",
      "Optimization time for model  313 :  0.042050838470458984\n",
      "Optimization time for model  314 :  2.568142890930176\n",
      "Optimization time for model  315 :  0.1320340633392334\n",
      "Optimization time for model  316 :  1.701150894165039\n",
      "Optimization time for model  317 :  0.45258092880249023\n",
      "Optimization time for model  318 :  1.405791997909546\n",
      "Optimization time for model  319 :  0.3214390277862549\n",
      "Optimization time for model  320 :  0.3191108703613281\n",
      "Optimization time for model  321 :  0.44655299186706543\n",
      "Optimization time for model  322 :  0.3310530185699463\n",
      "Optimization time for model  323 :  10.905503988265991\n",
      "Optimization time for model  324 :  0.05990195274353027\n",
      "Optimization time for model  325 :  1.2646880149841309\n",
      "Optimization time for model  326 :  0.5662000179290771\n",
      "Optimization time for model  327 :  0.3949849605560303\n",
      "Optimization time for model  328 :  0.003590106964111328\n",
      "Optimization time for model  329 :  0.38983607292175293\n",
      "Optimization time for model  330 :  2.269451141357422\n",
      "Optimization time for model  331 :  4.972701072692871\n",
      "Optimization time for model  332 :  0.26058292388916016\n",
      "Optimization time for model  333 :  0.12932205200195312\n",
      "Optimization time for model  334 :  0.1377241611480713\n",
      "Optimization time for model  335 :  0.08694696426391602\n",
      "Optimization time for model  336 :  0.18683099746704102\n",
      "Optimization time for model  337 :  0.015312910079956055\n",
      "Optimization time for model  338 :  0.002140045166015625\n",
      "Optimization time for model  339 :  0.13040399551391602\n",
      "Optimization time for model  340 :  0.23082494735717773\n",
      "Optimization time for model  341 :  1.5398471355438232\n",
      "Optimization time for model  342 :  1.7425940036773682\n",
      "Optimization time for model  343 :  0.0009100437164306641\n",
      "Optimization time for model  344 :  0.1482841968536377\n",
      "Optimization time for model  345 :  1.6948199272155762\n",
      "Optimization time for model  346 :  2.3753180503845215\n",
      "Optimization time for model  347 :  1.308152198791504\n",
      "Optimization time for model  348 :  0.15901899337768555\n",
      "Optimization time for model  349 :  2.0029189586639404\n",
      "Optimization time for model  350 :  2.6125540733337402\n",
      "Optimization time for model  351 :  11.98791790008545\n",
      "Optimization time for model  352 :  0.6743090152740479\n",
      "Optimization time for model  353 :  0.13386106491088867\n",
      "Optimization time for model  354 :  2.328718900680542\n",
      "Optimization time for model  355 :  2.166275978088379\n",
      "Optimization time for model  356 :  3.0955898761749268\n",
      "Optimization time for model  357 :  1.9389770030975342\n",
      "Optimization time for model  358 :  0.0014100074768066406\n",
      "Optimization time for model  359 :  0.006575107574462891\n",
      "Optimization time for model  360 :  0.14113998413085938\n",
      "Optimization time for model  361 :  2.3193230628967285\n",
      "Optimization time for model  362 :  0.22391295433044434\n",
      "Optimization time for model  363 :  0.029752016067504883\n",
      "Optimization time for model  364 :  2.7483091354370117\n",
      "Optimization time for model  365 :  0.0067441463470458984\n",
      "Optimization time for model  366 :  6.546271085739136\n",
      "Optimization time for model  367 :  0.10036897659301758\n",
      "Optimization time for model  368 :  0.05138397216796875\n",
      "Optimization time for model  369 :  0.4285120964050293\n",
      "Optimization time for model  370 :  0.1042790412902832\n",
      "Optimization time for model  371 :  0.2611398696899414\n",
      "Optimization time for model  372 :  2.457645893096924\n",
      "Optimization time for model  373 :  0.003656148910522461\n",
      "Optimization time for model  374 :  0.08979916572570801\n",
      "Optimization time for model  375 :  0.061170101165771484\n",
      "Optimization time for model  376 :  0.6142160892486572\n",
      "Optimization time for model  377 :  6.779881000518799\n",
      "Optimization time for model  378 :  3.2992329597473145\n",
      "Optimization time for model  379 :  0.003475189208984375\n",
      "Optimization time for model  380 :  0.3051719665527344\n",
      "Optimization time for model  381 :  2.3125739097595215\n",
      "Optimization time for model  382 :  0.2590830326080322\n",
      "Optimization time for model  383 :  1.1316440105438232\n",
      "Optimization time for model  384 :  1.9388439655303955\n",
      "Optimization time for model  385 :  2.208111047744751\n",
      "Optimization time for model  386 :  2.829714059829712\n",
      "Optimization time for model  387 :  0.003506898880004883\n",
      "Optimization time for model  388 :  0.07355380058288574\n",
      "Optimization time for model  389 :  1.746734857559204\n",
      "Optimization time for model  390 :  0.09434700012207031\n",
      "Optimization time for model  391 :  1.5879220962524414\n",
      "Optimization time for model  392 :  5.281826019287109\n",
      "Optimization time for model  393 :  0.02679920196533203\n",
      "Optimization time for model  394 :  0.06595706939697266\n",
      "Optimization time for model  395 :  0.018085956573486328\n",
      "Optimization time for model  396 :  0.11737489700317383\n",
      "Optimization time for model  397 :  0.9257421493530273\n",
      "Optimization time for model  398 :  0.41670799255371094\n",
      "Optimization time for model  399 :  0.6013689041137695\n",
      "Average optimization time:  1.5693416231870652\n"
     ]
    }
   ],
   "source": [
    "test_models = []\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "model_files = pkl.load(open(\"Data/corlat/train_test_data/pickle_filenames.pkl\", \"rb\"))\n",
    "# rename from .pickle to .lp\n",
    "filename = []\n",
    "for i in range(len(model_files)):\n",
    "    filename.append(model_files[i].replace(\".pickle\", \".lp\"))\n",
    "    \n",
    "for i in range(len(test_indices)):\n",
    "    model = gb.read(\"Data/corlat/instances/\" + filename[test_indices[i]], env=gurobi_env)\n",
    "    test_models.append(model)\n",
    "\n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time = []\n",
    "for i in range(len(test_models)):\n",
    "    model = test_models[i]\n",
    "    model.Params.Threads = 1\n",
    "    model.optimize()\n",
    "    print(\"Optimization time for model \", i, \": \", model.Runtime)\n",
    "    opt_time.append(model.Runtime)\n",
    "\n",
    "print(\"Average optimization time: \", np.mean(opt_time))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate the optimization time for each instance in the test set if we use $\\color{lightblue}\\text{warm start}$ to find optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_diving_opt_time(models, binary_indices, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to calculate the optimization time for each instance in the test set if we use warm start to find the optimal solution.\n",
    "    For each instance in the test set, we will relax the binary variables to continuous variables with bounds of 0 and 1, and set the value of the binary variables to the value predicted by the neural network.\n",
    "    We will then compute the IIS to find the list of violated constraints and variables.\n",
    "    If the model is infeasible, we will set the bounds of the binary variables to 0 and 1, and set the starting value of the binary variables to the value predicted by the neural network.\n",
    "    We will then optimize the model and record the optimization time.\n",
    "\n",
    "    Args:\n",
    "    models: list of gurobi models for each instance in the test set\n",
    "    binary_indices: list of indices of binary variables\n",
    "    y_pred: predictions of the neural network\n",
    "    \n",
    "    Returns:\n",
    "    opt_time: list of optimization time for each instance in the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    opt_time = []\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        \n",
    "        modelVars = model.getVars()\n",
    "        \n",
    "        instanceBinaryIndices = binary_indices\n",
    "        \n",
    "        y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "        \n",
    "        # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "        for j in range(len(instanceBinaryIndices)):\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "            # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "        \n",
    "        \n",
    "        # Compute the IIS to find the list of violated constraints and variables\n",
    "        try:\n",
    "            model.computeIIS()\n",
    "            infeasible_flag = True\n",
    "        except gb.GurobiError:\n",
    "            print(\"Model is feasible\")\n",
    "            infeasible_flag = False\n",
    "            continue\n",
    "        \n",
    "        if infeasible_flag:\n",
    "            for j in range(len(instanceBinaryIndices)):\n",
    "                if modelVars[instanceBinaryIndices[j]].IISLB == 0 and modelVars[instanceBinaryIndices[j]].IISUB == 0:\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                    # for each index in binary_indices, set the value of the corresponding variable to the value predicted by model\n",
    "                    # modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "                    # modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])                 \n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", 0)\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", 1)\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"Start\", y_pred_binary[i, j])\n",
    "                    \n",
    "                    # else if the variable is in the IIS, \n",
    "                    # get the relaxed variable and \n",
    "                    # set the bounds to 0 and 1 for the relaxed binary variables\n",
    "                else:\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", 0)\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", 1)\n",
    "        \n",
    "        else:\n",
    "            for j in range(len(instanceBinaryIndices)):\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                # modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "                # modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", 0)\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", 1)\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"Start\", y_pred_binary[i, j])\n",
    "        \n",
    "        model.Params.Threads = 1\n",
    "        model.optimize()\n",
    "        print(\"Optimization time for model \", i, \": \", model.Runtime)\n",
    "        opt_time.append(model.Runtime)\n",
    "        \n",
    "    return opt_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-06-02\n",
      "Optimization time for model  0 :  0.21126794815063477\n",
      "Optimization time for model  1 :  2.7527899742126465\n",
      "Optimization time for model  2 :  1.5212550163269043\n",
      "Optimization time for model  3 :  1.369001865386963\n",
      "Optimization time for model  4 :  0.42718982696533203\n",
      "Optimization time for model  5 :  0.2759220600128174\n",
      "Optimization time for model  6 :  0.396716833114624\n",
      "Optimization time for model  7 :  0.9202249050140381\n",
      "Optimization time for model  8 :  0.0068509578704833984\n",
      "Optimization time for model  9 :  1.1727099418640137\n",
      "Optimization time for model  10 :  0.48308396339416504\n",
      "Optimization time for model  11 :  0.01904892921447754\n",
      "Optimization time for model  12 :  0.003921985626220703\n",
      "Optimization time for model  13 :  0.221466064453125\n",
      "Optimization time for model  14 :  0.003690004348754883\n",
      "Optimization time for model  15 :  0.7606759071350098\n",
      "Optimization time for model  16 :  0.1981639862060547\n",
      "Optimization time for model  17 :  0.141448974609375\n",
      "Optimization time for model  18 :  0.19209718704223633\n",
      "Optimization time for model  19 :  3.3983869552612305\n",
      "Optimization time for model  20 :  0.008459806442260742\n",
      "Optimization time for model  21 :  1.9073188304901123\n",
      "Optimization time for model  22 :  0.0039730072021484375\n",
      "Optimization time for model  23 :  0.017010927200317383\n",
      "Optimization time for model  24 :  0.42633509635925293\n",
      "Optimization time for model  25 :  1.2779490947723389\n",
      "Optimization time for model  26 :  0.14335012435913086\n",
      "Optimization time for model  27 :  0.0009319782257080078\n",
      "Optimization time for model  28 :  0.5147459506988525\n",
      "Optimization time for model  29 :  0.2823679447174072\n",
      "Optimization time for model  30 :  0.5328309535980225\n",
      "Optimization time for model  31 :  5.873476028442383\n",
      "Optimization time for model  0 :  2.7584068775177\n",
      "Optimization time for model  1 :  0.041532039642333984\n",
      "Optimization time for model  2 :  0.3397650718688965\n",
      "Optimization time for model  3 :  0.34227991104125977\n",
      "Optimization time for model  4 :  0.31000208854675293\n",
      "Optimization time for model  5 :  0.4623739719390869\n",
      "Optimization time for model  6 :  0.0036978721618652344\n",
      "Optimization time for model  7 :  1.4358279705047607\n",
      "Optimization time for model  8 :  1.2289681434631348\n",
      "Optimization time for model  9 :  0.2734370231628418\n",
      "Optimization time for model  10 :  0.3658919334411621\n",
      "Optimization time for model  11 :  1.3693568706512451\n",
      "Optimization time for model  12 :  0.3756711483001709\n",
      "Optimization time for model  13 :  0.020766019821166992\n",
      "Optimization time for model  14 :  2.7445361614227295\n",
      "Optimization time for model  15 :  1.2454469203948975\n",
      "Optimization time for model  16 :  4.8228490352630615\n",
      "Optimization time for model  17 :  0.2555539608001709\n",
      "Optimization time for model  18 :  3.0048940181732178\n",
      "Optimization time for model  19 :  0.03842306137084961\n",
      "Optimization time for model  20 :  0.0016760826110839844\n",
      "Optimization time for model  21 :  0.7253730297088623\n",
      "Optimization time for model  22 :  0.09456300735473633\n",
      "Optimization time for model  23 :  1.8348560333251953\n",
      "Optimization time for model  24 :  0.0018069744110107422\n",
      "Optimization time for model  25 :  0.16359806060791016\n",
      "Optimization time for model  26 :  0.07838296890258789\n",
      "Optimization time for model  27 :  0.03374886512756348\n",
      "Optimization time for model  28 :  0.0225980281829834\n",
      "Optimization time for model  29 :  0.8409340381622314\n",
      "Optimization time for model  30 :  2.1252260208129883\n",
      "Optimization time for model  31 :  0.0009229183197021484\n",
      "Optimization time for model  0 :  4.042893886566162\n",
      "Optimization time for model  1 :  1.2136750221252441\n",
      "Optimization time for model  2 :  0.4434661865234375\n",
      "Optimization time for model  3 :  0.3298830986022949\n",
      "Optimization time for model  4 :  2.6485209465026855\n",
      "Optimization time for model  5 :  0.0021119117736816406\n",
      "Optimization time for model  6 :  7.222941160202026\n",
      "Optimization time for model  7 :  0.009005069732666016\n",
      "Optimization time for model  8 :  0.9662277698516846\n",
      "Optimization time for model  9 :  0.5494341850280762\n",
      "Optimization time for model  10 :  0.3748900890350342\n",
      "Optimization time for model  11 :  0.006257057189941406\n",
      "Optimization time for model  12 :  0.04775094985961914\n",
      "Optimization time for model  13 :  10.172420978546143\n",
      "Optimization time for model  14 :  3.9145970344543457\n",
      "Optimization time for model  15 :  0.0032269954681396484\n",
      "Optimization time for model  16 :  4.767776012420654\n",
      "Optimization time for model  17 :  2.2005460262298584\n",
      "Optimization time for model  18 :  3.012688159942627\n",
      "Optimization time for model  19 :  0.005017995834350586\n",
      "Optimization time for model  20 :  1.86924409866333\n",
      "Optimization time for model  21 :  0.6594669818878174\n",
      "Optimization time for model  22 :  3.0596559047698975\n",
      "Optimization time for model  23 :  0.017718076705932617\n",
      "Optimization time for model  24 :  0.24749994277954102\n",
      "Optimization time for model  25 :  4.216149806976318\n",
      "Optimization time for model  26 :  6.145474195480347\n",
      "Optimization time for model  27 :  2.232358932495117\n",
      "Optimization time for model  28 :  0.11606216430664062\n",
      "Model is feasible\n",
      "Optimization time for model  30 :  0.01534891128540039\n",
      "Optimization time for model  31 :  2.5171661376953125\n",
      "Optimization time for model  0 :  0.0010950565338134766\n",
      "Optimization time for model  1 :  0.0784611701965332\n",
      "Optimization time for model  2 :  0.010625123977661133\n",
      "Optimization time for model  3 :  7.593122959136963\n",
      "Optimization time for model  4 :  0.032370805740356445\n",
      "Optimization time for model  5 :  0.07490706443786621\n",
      "Optimization time for model  6 :  0.6393280029296875\n",
      "Optimization time for model  7 :  0.003997087478637695\n",
      "Model is feasible\n",
      "Optimization time for model  9 :  1.278264045715332\n",
      "Optimization time for model  10 :  1.9154679775238037\n",
      "Optimization time for model  11 :  4.858265161514282\n",
      "Optimization time for model  12 :  1.693734884262085\n",
      "Optimization time for model  13 :  0.18566012382507324\n",
      "Optimization time for model  14 :  0.5134880542755127\n",
      "Optimization time for model  15 :  1.0084559917449951\n",
      "Optimization time for model  16 :  0.13573980331420898\n",
      "Optimization time for model  17 :  1.3799889087677002\n",
      "Optimization time for model  18 :  0.17748403549194336\n",
      "Optimization time for model  19 :  0.812432050704956\n",
      "Optimization time for model  20 :  0.003710031509399414\n",
      "Optimization time for model  21 :  0.2671821117401123\n",
      "Optimization time for model  22 :  3.350605010986328\n",
      "Optimization time for model  23 :  1.747596025466919\n",
      "Optimization time for model  24 :  0.00393986701965332\n",
      "Optimization time for model  25 :  0.10783720016479492\n",
      "Optimization time for model  26 :  3.869870901107788\n",
      "Optimization time for model  27 :  0.023304224014282227\n",
      "Optimization time for model  28 :  0.5710878372192383\n",
      "Optimization time for model  29 :  5.4305198192596436\n",
      "Optimization time for model  30 :  1.9002258777618408\n",
      "Optimization time for model  31 :  0.0037221908569335938\n",
      "Optimization time for model  0 :  1.1210050582885742\n",
      "Optimization time for model  1 :  0.3383481502532959\n",
      "Optimization time for model  2 :  0.0032858848571777344\n",
      "Optimization time for model  3 :  0.05485677719116211\n",
      "Optimization time for model  4 :  0.7782468795776367\n",
      "Optimization time for model  5 :  4.879684925079346\n",
      "Optimization time for model  6 :  10.026161909103394\n",
      "Optimization time for model  7 :  0.19668102264404297\n",
      "Optimization time for model  8 :  0.08370780944824219\n",
      "Optimization time for model  9 :  0.0032110214233398438\n",
      "Optimization time for model  10 :  3.1887621879577637\n",
      "Optimization time for model  11 :  0.24126410484313965\n",
      "Optimization time for model  12 :  0.31920504570007324\n",
      "Optimization time for model  13 :  0.06529903411865234\n",
      "Optimization time for model  14 :  0.13743209838867188\n",
      "Optimization time for model  15 :  1.359490156173706\n",
      "Optimization time for model  16 :  0.0038640499114990234\n",
      "Optimization time for model  17 :  0.13772988319396973\n",
      "Optimization time for model  18 :  1.4748458862304688\n",
      "Optimization time for model  19 :  3.708127975463867\n",
      "Optimization time for model  20 :  0.0256040096282959\n",
      "Optimization time for model  21 :  8.219336032867432\n",
      "Optimization time for model  22 :  0.08054709434509277\n",
      "Optimization time for model  23 :  0.29457712173461914\n",
      "Optimization time for model  24 :  0.1263420581817627\n",
      "Optimization time for model  25 :  0.12142801284790039\n",
      "Optimization time for model  26 :  0.6191771030426025\n",
      "Optimization time for model  27 :  0.6728320121765137\n",
      "Optimization time for model  28 :  1.1003777980804443\n",
      "Optimization time for model  29 :  1.8466370105743408\n",
      "Optimization time for model  30 :  0.9588971138000488\n",
      "Optimization time for model  31 :  0.9794671535491943\n",
      "Optimization time for model  0 :  1.005608081817627\n",
      "Optimization time for model  1 :  0.11068201065063477\n",
      "Optimization time for model  2 :  0.08577990531921387\n",
      "Optimization time for model  3 :  0.5233609676361084\n",
      "Optimization time for model  4 :  0.037194013595581055\n",
      "Optimization time for model  5 :  0.5865650177001953\n",
      "Optimization time for model  6 :  0.04236292839050293\n",
      "Optimization time for model  7 :  0.6906259059906006\n",
      "Optimization time for model  8 :  0.09652304649353027\n",
      "Optimization time for model  9 :  0.34648609161376953\n",
      "Optimization time for model  10 :  0.13724994659423828\n",
      "Optimization time for model  11 :  0.42117905616760254\n",
      "Optimization time for model  12 :  0.45366811752319336\n",
      "Optimization time for model  13 :  0.684567928314209\n",
      "Optimization time for model  14 :  0.11488008499145508\n",
      "Optimization time for model  15 :  0.5719571113586426\n",
      "Optimization time for model  16 :  0.0715029239654541\n",
      "Optimization time for model  17 :  0.9364540576934814\n",
      "Optimization time for model  18 :  1.8628480434417725\n",
      "Optimization time for model  19 :  0.10031914710998535\n",
      "Optimization time for model  20 :  0.30400896072387695\n",
      "Optimization time for model  21 :  0.2625250816345215\n",
      "Optimization time for model  22 :  0.3793158531188965\n",
      "Optimization time for model  23 :  4.0445380210876465\n",
      "Optimization time for model  24 :  0.3615901470184326\n",
      "Optimization time for model  25 :  0.4002978801727295\n",
      "Optimization time for model  26 :  0.3393409252166748\n",
      "Optimization time for model  27 :  0.0008840560913085938\n",
      "Optimization time for model  28 :  5.517841815948486\n",
      "Optimization time for model  29 :  1.0504930019378662\n",
      "Optimization time for model  30 :  0.08522200584411621\n",
      "Optimization time for model  31 :  2.682781934738159\n",
      "Optimization time for model  0 :  0.002413034439086914\n",
      "Optimization time for model  1 :  0.789283037185669\n",
      "Optimization time for model  2 :  0.4771089553833008\n",
      "Optimization time for model  3 :  4.937772989273071\n",
      "Optimization time for model  4 :  0.2644941806793213\n",
      "Optimization time for model  5 :  0.22072100639343262\n",
      "Optimization time for model  6 :  0.00458979606628418\n",
      "Optimization time for model  7 :  0.0064239501953125\n",
      "Optimization time for model  8 :  0.504951000213623\n",
      "Optimization time for model  9 :  0.11767196655273438\n",
      "Optimization time for model  10 :  0.013895034790039062\n",
      "Optimization time for model  11 :  0.9385440349578857\n",
      "Optimization time for model  12 :  4.029528856277466\n",
      "Optimization time for model  13 :  0.0037238597869873047\n",
      "Optimization time for model  14 :  0.0037369728088378906\n",
      "Optimization time for model  15 :  0.8828768730163574\n",
      "Optimization time for model  16 :  0.3098909854888916\n",
      "Optimization time for model  17 :  0.3761160373687744\n",
      "Optimization time for model  18 :  3.329496145248413\n",
      "Optimization time for model  19 :  0.22956418991088867\n",
      "Optimization time for model  20 :  0.5318140983581543\n",
      "Optimization time for model  21 :  0.026834964752197266\n",
      "Optimization time for model  22 :  0.009510040283203125\n",
      "Optimization time for model  23 :  0.014732122421264648\n",
      "Optimization time for model  24 :  0.7387261390686035\n",
      "Optimization time for model  25 :  0.09144711494445801\n",
      "Optimization time for model  26 :  0.19437003135681152\n",
      "Optimization time for model  27 :  0.6146130561828613\n",
      "Optimization time for model  28 :  2.7948498725891113\n",
      "Optimization time for model  29 :  0.6033439636230469\n",
      "Optimization time for model  30 :  0.0036580562591552734\n",
      "Optimization time for model  31 :  0.8355820178985596\n",
      "Optimization time for model  0 :  0.08670186996459961\n",
      "Optimization time for model  1 :  0.04775118827819824\n",
      "Optimization time for model  2 :  0.6324801445007324\n",
      "Optimization time for model  3 :  1.5267369747161865\n",
      "Optimization time for model  4 :  0.5910859107971191\n",
      "Optimization time for model  5 :  2.2248170375823975\n",
      "Optimization time for model  6 :  0.09739208221435547\n",
      "Optimization time for model  7 :  3.4885799884796143\n",
      "Optimization time for model  8 :  0.6205539703369141\n",
      "Optimization time for model  9 :  0.019732952117919922\n",
      "Optimization time for model  10 :  0.2914600372314453\n",
      "Optimization time for model  11 :  0.07892107963562012\n",
      "Optimization time for model  12 :  4.1112120151519775\n",
      "Optimization time for model  13 :  0.22125506401062012\n",
      "Optimization time for model  14 :  0.011131048202514648\n",
      "Optimization time for model  15 :  0.4578108787536621\n",
      "Optimization time for model  16 :  6.484759092330933\n",
      "Optimization time for model  17 :  0.005952119827270508\n",
      "Optimization time for model  18 :  3.6388731002807617\n",
      "Optimization time for model  19 :  5.968250036239624\n",
      "Optimization time for model  20 :  1.0625760555267334\n",
      "Optimization time for model  21 :  0.0016391277313232422\n",
      "Optimization time for model  22 :  0.004991054534912109\n",
      "Optimization time for model  23 :  0.17165780067443848\n",
      "Optimization time for model  24 :  3.7791969776153564\n",
      "Optimization time for model  25 :  1.909294843673706\n",
      "Optimization time for model  26 :  0.37096500396728516\n",
      "Optimization time for model  27 :  3.039489984512329\n",
      "Optimization time for model  28 :  0.5911850929260254\n",
      "Optimization time for model  29 :  2.072175979614258\n",
      "Optimization time for model  30 :  0.12035703659057617\n",
      "Optimization time for model  31 :  0.001458883285522461\n",
      "Optimization time for model  0 :  0.20267200469970703\n",
      "Optimization time for model  1 :  0.7539920806884766\n",
      "Optimization time for model  2 :  3.9227750301361084\n",
      "Optimization time for model  3 :  0.0037190914154052734\n",
      "Optimization time for model  4 :  0.05149698257446289\n",
      "Optimization time for model  5 :  0.24106693267822266\n",
      "Optimization time for model  6 :  2.4641878604888916\n",
      "Optimization time for model  7 :  2.2624359130859375\n",
      "Optimization time for model  8 :  0.05530810356140137\n",
      "Optimization time for model  9 :  2.0010530948638916\n",
      "Optimization time for model  10 :  1.3422369956970215\n",
      "Optimization time for model  11 :  3.1056501865386963\n",
      "Optimization time for model  12 :  0.21358799934387207\n",
      "Optimization time for model  13 :  9.182940006256104\n",
      "Optimization time for model  14 :  1.2980759143829346\n",
      "Optimization time for model  15 :  7.493740797042847\n",
      "Optimization time for model  16 :  0.7270820140838623\n",
      "Optimization time for model  17 :  0.3056340217590332\n",
      "Optimization time for model  18 :  0.30469417572021484\n",
      "Optimization time for model  19 :  17.55607008934021\n",
      "Optimization time for model  20 :  0.23335504531860352\n",
      "Optimization time for model  21 :  0.7146267890930176\n",
      "Optimization time for model  22 :  0.5177991390228271\n",
      "Optimization time for model  23 :  0.13142800331115723\n",
      "Optimization time for model  24 :  2.909950017929077\n",
      "Optimization time for model  25 :  0.5437180995941162\n",
      "Optimization time for model  26 :  0.10086989402770996\n",
      "Optimization time for model  27 :  0.12334680557250977\n",
      "Optimization time for model  28 :  1.7548670768737793\n",
      "Optimization time for model  29 :  0.17796802520751953\n",
      "Optimization time for model  30 :  0.6918289661407471\n",
      "Optimization time for model  31 :  6.039050102233887\n",
      "Optimization time for model  0 :  0.5839400291442871\n",
      "Optimization time for model  1 :  0.3737959861755371\n",
      "Optimization time for model  2 :  0.3948249816894531\n",
      "Optimization time for model  3 :  2.3506641387939453\n",
      "Optimization time for model  4 :  0.0037670135498046875\n",
      "Optimization time for model  5 :  0.0048160552978515625\n",
      "Optimization time for model  6 :  0.3798809051513672\n",
      "Optimization time for model  7 :  1.4044599533081055\n",
      "Optimization time for model  8 :  1.271245002746582\n",
      "Optimization time for model  9 :  0.5483269691467285\n",
      "Optimization time for model  10 :  0.2717170715332031\n",
      "Optimization time for model  11 :  0.07596802711486816\n",
      "Optimization time for model  12 :  1.7807960510253906\n",
      "Model is feasible\n",
      "Optimization time for model  14 :  1.9444911479949951\n",
      "Optimization time for model  15 :  1.7264389991760254\n",
      "Optimization time for model  16 :  0.23290586471557617\n",
      "Optimization time for model  17 :  2.519871950149536\n",
      "Optimization time for model  18 :  0.5390148162841797\n",
      "Optimization time for model  19 :  0.13748908042907715\n",
      "Optimization time for model  20 :  1.6707470417022705\n",
      "Optimization time for model  21 :  1.4451019763946533\n",
      "Optimization time for model  22 :  0.0037431716918945312\n",
      "Optimization time for model  23 :  0.8359560966491699\n",
      "Optimization time for model  24 :  0.2105100154876709\n",
      "Optimization time for model  25 :  0.0423130989074707\n",
      "Optimization time for model  26 :  2.392902135848999\n",
      "Optimization time for model  27 :  0.13274693489074707\n",
      "Optimization time for model  28 :  1.8569438457489014\n",
      "Optimization time for model  29 :  0.45271897315979004\n",
      "Optimization time for model  30 :  1.4014489650726318\n",
      "Optimization time for model  31 :  0.32253193855285645\n",
      "Optimization time for model  0 :  0.3939321041107178\n",
      "Optimization time for model  1 :  0.3536369800567627\n",
      "Optimization time for model  2 :  0.4491748809814453\n",
      "Optimization time for model  3 :  10.933807134628296\n",
      "Optimization time for model  4 :  0.060072898864746094\n",
      "Optimization time for model  5 :  0.4159989356994629\n",
      "Optimization time for model  6 :  0.8973782062530518\n",
      "Optimization time for model  7 :  0.575653076171875\n",
      "Optimization time for model  8 :  0.003732919692993164\n",
      "Optimization time for model  9 :  0.4102439880371094\n",
      "Optimization time for model  10 :  1.414247989654541\n",
      "Optimization time for model  11 :  4.971402168273926\n",
      "Optimization time for model  12 :  0.40192103385925293\n",
      "Optimization time for model  13 :  0.15082716941833496\n",
      "Optimization time for model  14 :  0.20287394523620605\n",
      "Optimization time for model  15 :  0.0873420238494873\n",
      "Optimization time for model  16 :  0.18670392036437988\n",
      "Optimization time for model  17 :  0.14796805381774902\n",
      "Optimization time for model  18 :  0.002355813980102539\n",
      "Optimization time for model  19 :  0.22005796432495117\n",
      "Optimization time for model  20 :  0.20703792572021484\n",
      "Optimization time for model  21 :  0.8218851089477539\n",
      "Optimization time for model  22 :  0.8958959579467773\n",
      "Optimization time for model  23 :  0.0010938644409179688\n",
      "Optimization time for model  24 :  0.2508559226989746\n",
      "Optimization time for model  25 :  0.4329981803894043\n",
      "Optimization time for model  26 :  3.5889248847961426\n",
      "Optimization time for model  27 :  0.9598429203033447\n",
      "Optimization time for model  28 :  0.25258421897888184\n",
      "Optimization time for model  29 :  2.0320029258728027\n",
      "Optimization time for model  30 :  1.9583430290222168\n",
      "Optimization time for model  31 :  9.415879011154175\n",
      "Optimization time for model  0 :  0.2870209217071533\n",
      "Optimization time for model  1 :  0.051722049713134766\n",
      "Optimization time for model  2 :  1.2118799686431885\n",
      "Optimization time for model  3 :  1.1500461101531982\n",
      "Optimization time for model  4 :  3.1063389778137207\n",
      "Optimization time for model  5 :  1.232192039489746\n",
      "Optimization time for model  6 :  0.0015909671783447266\n",
      "Optimization time for model  7 :  0.006842851638793945\n",
      "Optimization time for model  8 :  0.21567201614379883\n",
      "Optimization time for model  9 :  1.1675069332122803\n",
      "Optimization time for model  10 :  0.22425222396850586\n",
      "Optimization time for model  11 :  0.0684349536895752\n",
      "Optimization time for model  12 :  1.7113580703735352\n",
      "Optimization time for model  13 :  0.006906032562255859\n",
      "Optimization time for model  14 :  6.544683933258057\n",
      "Optimization time for model  15 :  0.1004490852355957\n",
      "Optimization time for model  16 :  0.05148506164550781\n",
      "Optimization time for model  17 :  0.4296300411224365\n",
      "Optimization time for model  18 :  0.1332409381866455\n",
      "Optimization time for model  19 :  0.2721700668334961\n",
      "Optimization time for model  20 :  3.83304500579834\n",
      "Optimization time for model  21 :  0.003931999206542969\n",
      "Optimization time for model  22 :  0.08998489379882812\n",
      "Optimization time for model  23 :  0.08760595321655273\n",
      "Optimization time for model  24 :  0.5330648422241211\n",
      "Optimization time for model  25 :  6.032813787460327\n",
      "Optimization time for model  26 :  3.5497238636016846\n",
      "Optimization time for model  27 :  0.0037260055541992188\n",
      "Optimization time for model  28 :  0.305433988571167\n",
      "Optimization time for model  29 :  4.135885953903198\n",
      "Optimization time for model  30 :  0.6494908332824707\n",
      "Optimization time for model  31 :  1.129951000213623\n",
      "Optimization time for model  0 :  1.9280869960784912\n",
      "Optimization time for model  1 :  0.7524969577789307\n",
      "Optimization time for model  2 :  1.5915300846099854\n",
      "Optimization time for model  3 :  0.003715991973876953\n",
      "Optimization time for model  4 :  0.0739130973815918\n",
      "Optimization time for model  5 :  1.7470009326934814\n",
      "Optimization time for model  6 :  0.09491610527038574\n",
      "Optimization time for model  7 :  0.4105541706085205\n",
      "Optimization time for model  8 :  8.58452582359314\n",
      "Optimization time for model  9 :  0.019862890243530273\n",
      "Optimization time for model  10 :  0.10287189483642578\n",
      "Optimization time for model  11 :  0.01758599281311035\n",
      "Optimization time for model  12 :  0.11748409271240234\n",
      "Optimization time for model  13 :  0.9255449771881104\n",
      "Optimization time for model  14 :  0.3088650703430176\n",
      "Optimization time for model  15 :  0.183074951171875\n"
     ]
    }
   ],
   "source": [
    "test_models = []\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "model_files = pkl.load(open(\"Data/corlat/train_test_data/pickle_filenames.pkl\", \"rb\"))\n",
    "# rename from .pickle to .lp\n",
    "filename = []\n",
    "for i in range(len(model_files)):\n",
    "    filename.append(model_files[i].replace(\".pickle\", \".lp\"))\n",
    "    \n",
    "for i in range(len(test_indices)):\n",
    "    model = gb.read(\"Data/corlat/instances/\" + filename[test_indices[i]], env=gurobi_env)\n",
    "    test_models.append(model)\n",
    "    \n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time = []\n",
    "for i, data in enumerate(valid_loader):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # get slices of test_models according to batch size\n",
    "    len_test_models = len(test_models)\n",
    "\n",
    "    test_models_batch = test_models[i*batch_size: min((i+1)*batch_size, len_test_models)]\n",
    "    \n",
    "    opt_time_batch = calculate_diving_opt_time(test_models_batch, binary_indices, outputs.detach().cpu().numpy())\n",
    "    \n",
    "    opt_time.append(opt_time_batch)\n",
    "    \n",
    "# save opt_time\n",
    "with open(\"Data/corlat/opt_time.pickle\", \"wb\") as f:\n",
    "    pkl.dump(opt_time, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average optimization time:  1.2182969812782345\n"
     ]
    }
   ],
   "source": [
    "# flatten opt_time\n",
    "opt_time_flat = [item for sublist in opt_time for item in sublist]\n",
    "print(\"Average optimization time: \", np.mean(opt_time_flat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the optimization time for each instance in the test set if we use $\\color{lightblue}\\text{equality constraint}$ to find optimal solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_equality_constraint_opt_time(models, binary_indices, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate the optimization time for each instance in the test set if we use equality constraint to find the optimal solution.\n",
    "    For each instance in the test set, we will relax the binary variables to continuous variables with bounds of 0 and 1, and set the value of the binary variables to the value predicted by the neural network.\n",
    "    We will then compute the IIS to find the list of violated constraints and variables.\n",
    "    If the model is infeasible, we will set the bounds of the binary variables to 0 and 1, and set the starting value of the binary variables to the value predicted by the neural network.\n",
    "    We will then optimize the model and record the optimization time.\n",
    "\n",
    "    Args:\n",
    "        models: list of gurobi models for each instance in the test set\n",
    "        binary_indices: list of indices of binary variables\n",
    "        y_pred: predictions of the neural network\n",
    "\n",
    "    Returns:\n",
    "        opt_time: list of optimization time for each instance in the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    opt_time = []\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        model = models[i]\n",
    "        \n",
    "        modelVars = model.getVars()\n",
    "        \n",
    "        instanceBinaryIndices = binary_indices\n",
    "        \n",
    "        y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "        \n",
    "        # need to relax the binary variables to continuous variables with bounds of 0 and 1, we can use the setAttr method to change their vtype attribute\n",
    "        for j in range(len(instanceBinaryIndices)):\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"C\")\n",
    "\n",
    "            # for each index in firstInstanceTestBinaryIndices, set the value of the corresponding variable to the value predicted by xgboost\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "            modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "        \n",
    "        \n",
    "        # Compute the IIS to find the list of violated constraints and variables\n",
    "        try:\n",
    "            model.computeIIS()\n",
    "            infeasible_flag = True\n",
    "        except gb.GurobiError:\n",
    "            print(\"Model is feasible\")\n",
    "            infeasible_flag = False\n",
    "            continue\n",
    "        \n",
    "        if infeasible_flag:\n",
    "            for j in range(len(instanceBinaryIndices)):\n",
    "                if modelVars[instanceBinaryIndices[j]].IISLB == 0 and modelVars[instanceBinaryIndices[j]].IISUB == 0:\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                    # for each index in binary_indices, set the value of the corresponding variable to the value predicted by model\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])                 \n",
    "                    \n",
    "                    # else if the variable is in the IIS, \n",
    "                    # get the relaxed variable and \n",
    "                    # set the bounds to 0 and 1 for the relaxed binary variables\n",
    "                else:\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", 0)\n",
    "                    modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", 1)\n",
    "        \n",
    "        else:\n",
    "            for j in range(len(instanceBinaryIndices)):\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"VType\", \"B\")\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"LB\", y_pred_binary[i, j])\n",
    "                modelVars[instanceBinaryIndices[j]].setAttr(\"UB\", y_pred_binary[i, j])\n",
    "        \n",
    "        model.Params.Threads = 1\n",
    "        model.optimize()\n",
    "        print(\"Optimization time for model \", i, \": \", model.Runtime)\n",
    "        opt_time.append(model.Runtime)\n",
    "        \n",
    "    return opt_time\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equality Constraint test optimization time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2024-06-02\n",
      "Optimization time for model  0 :  0.0006139278411865234\n",
      "Optimization time for model  1 :  0.0001938343048095703\n",
      "Optimization time for model  2 :  0.0001881122589111328\n",
      "Optimization time for model  3 :  0.00017189979553222656\n",
      "Optimization time for model  4 :  0.0001499652862548828\n",
      "Optimization time for model  5 :  0.00014209747314453125\n",
      "Optimization time for model  6 :  0.00011801719665527344\n",
      "Optimization time for model  7 :  0.00017309188842773438\n",
      "Optimization time for model  8 :  0.006464958190917969\n",
      "Optimization time for model  9 :  0.0001800060272216797\n",
      "Optimization time for model  10 :  0.25037503242492676\n",
      "Optimization time for model  11 :  0.03884005546569824\n",
      "Model is feasible\n",
      "Optimization time for model  13 :  0.0001780986785888672\n",
      "Model is feasible\n",
      "Optimization time for model  15 :  0.00022411346435546875\n",
      "Optimization time for model  16 :  0.00014090538024902344\n",
      "Optimization time for model  17 :  0.00013113021850585938\n",
      "Optimization time for model  18 :  0.00018405914306640625\n",
      "Optimization time for model  19 :  2.4253361225128174\n",
      "Optimization time for model  20 :  0.00018095970153808594\n",
      "Optimization time for model  21 :  0.00012183189392089844\n",
      "Optimization time for model  22 :  0.0001709461212158203\n",
      "Optimization time for model  23 :  0.00016307830810546875\n",
      "Optimization time for model  24 :  0.000186920166015625\n",
      "Optimization time for model  25 :  0.0001800060272216797\n",
      "Optimization time for model  26 :  0.00012302398681640625\n",
      "Optimization time for model  27 :  0.00012111663818359375\n",
      "Optimization time for model  28 :  0.00016999244689941406\n",
      "Optimization time for model  29 :  1.0867910385131836\n",
      "Optimization time for model  30 :  0.00013113021850585938\n",
      "Optimization time for model  31 :  0.0001819133758544922\n",
      "Optimization time for model  0 :  0.0001270771026611328\n",
      "Optimization time for model  1 :  0.0002040863037109375\n",
      "Optimization time for model  2 :  0.0001690387725830078\n",
      "Optimization time for model  3 :  0.00018095970153808594\n",
      "Optimization time for model  4 :  0.00016689300537109375\n",
      "Optimization time for model  5 :  0.00016689300537109375\n",
      "Optimization time for model  6 :  0.00016498565673828125\n",
      "Optimization time for model  7 :  0.00016379356384277344\n",
      "Optimization time for model  8 :  0.00017595291137695312\n",
      "Optimization time for model  9 :  0.0001628398895263672\n",
      "Optimization time for model  10 :  0.00011801719665527344\n",
      "Optimization time for model  11 :  0.000164031982421875\n",
      "Optimization time for model  12 :  0.0001430511474609375\n",
      "Optimization time for model  13 :  0.0001659393310546875\n",
      "Optimization time for model  14 :  0.00013780593872070312\n",
      "Optimization time for model  15 :  0.00011610984802246094\n",
      "Optimization time for model  16 :  0.00016307830810546875\n",
      "Optimization time for model  17 :  0.00011801719665527344\n",
      "Optimization time for model  18 :  0.00016307830810546875\n",
      "Optimization time for model  19 :  0.023613929748535156\n",
      "Optimization time for model  20 :  0.00013017654418945312\n",
      "Optimization time for model  21 :  1.2068312168121338\n",
      "Optimization time for model  22 :  0.0001327991485595703\n",
      "Optimization time for model  23 :  0.00012111663818359375\n",
      "Optimization time for model  24 :  0.00012493133544921875\n",
      "Optimization time for model  25 :  0.0001728534698486328\n",
      "Optimization time for model  26 :  0.00017404556274414062\n",
      "Optimization time for model  27 :  0.00016307830810546875\n",
      "Optimization time for model  28 :  0.00012183189392089844\n",
      "Optimization time for model  29 :  0.44056010246276855\n",
      "Optimization time for model  30 :  0.0001800060272216797\n",
      "Optimization time for model  31 :  0.000125885009765625\n",
      "Optimization time for model  0 :  0.00013709068298339844\n",
      "Optimization time for model  1 :  0.00013208389282226562\n",
      "Optimization time for model  2 :  0.00015807151794433594\n",
      "Optimization time for model  3 :  0.0001811981201171875\n",
      "Optimization time for model  4 :  2.5281810760498047\n",
      "Optimization time for model  5 :  0.00013399124145507812\n",
      "Optimization time for model  6 :  5.377208948135376\n",
      "Optimization time for model  7 :  0.00013494491577148438\n",
      "Optimization time for model  8 :  0.0001499652862548828\n",
      "Optimization time for model  9 :  0.00011992454528808594\n",
      "Optimization time for model  10 :  0.000186920166015625\n",
      "Optimization time for model  11 :  0.0010259151458740234\n",
      "Optimization time for model  12 :  0.021049022674560547\n",
      "Optimization time for model  13 :  7.751461982727051\n",
      "Optimization time for model  14 :  0.0001780986785888672\n",
      "Optimization time for model  15 :  0.00014090538024902344\n",
      "Optimization time for model  16 :  5.849881887435913\n",
      "Optimization time for model  17 :  2.1956241130828857\n",
      "Optimization time for model  18 :  2.996596097946167\n",
      "Optimization time for model  19 :  0.0001881122589111328\n",
      "Optimization time for model  20 :  0.0001728534698486328\n",
      "Optimization time for model  21 :  0.00017380714416503906\n",
      "Optimization time for model  22 :  0.00011682510375976562\n",
      "Optimization time for model  23 :  0.00012302398681640625\n",
      "Optimization time for model  24 :  0.0001430511474609375\n",
      "Optimization time for model  25 :  0.0001399517059326172\n",
      "Optimization time for model  26 :  6.00424599647522\n",
      "Optimization time for model  27 :  0.00019979476928710938\n",
      "Optimization time for model  28 :  0.08687710762023926\n",
      "Optimization time for model  29 :  0.0001800060272216797\n",
      "Optimization time for model  30 :  0.014806032180786133\n",
      "Optimization time for model  31 :  0.00017881393432617188\n",
      "Optimization time for model  0 :  0.00014901161193847656\n",
      "Optimization time for model  1 :  0.00014209747314453125\n",
      "Optimization time for model  2 :  0.00017595291137695312\n",
      "Optimization time for model  3 :  0.00014591217041015625\n",
      "Optimization time for model  4 :  0.0001239776611328125\n",
      "Optimization time for model  5 :  0.00012302398681640625\n",
      "Optimization time for model  6 :  0.00011897087097167969\n",
      "Optimization time for model  7 :  0.0016388893127441406\n",
      "Optimization time for model  8 :  0.0007588863372802734\n",
      "Optimization time for model  9 :  0.0003590583801269531\n",
      "Optimization time for model  10 :  0.0001838207244873047\n",
      "Optimization time for model  11 :  0.00015115737915039062\n",
      "Optimization time for model  12 :  0.0001709461212158203\n",
      "Optimization time for model  13 :  0.00041985511779785156\n",
      "Optimization time for model  14 :  1.2594599723815918\n",
      "Optimization time for model  15 :  0.0001900196075439453\n",
      "Optimization time for model  16 :  0.14136600494384766\n",
      "Optimization time for model  17 :  2.3219380378723145\n",
      "Optimization time for model  18 :  0.00013518333435058594\n",
      "Optimization time for model  19 :  0.00015211105346679688\n",
      "Optimization time for model  20 :  0.00017499923706054688\n",
      "Optimization time for model  21 :  0.22336292266845703\n",
      "Optimization time for model  22 :  5.010430097579956\n",
      "Optimization time for model  23 :  1.3792040348052979\n",
      "Optimization time for model  24 :  0.00018596649169921875\n",
      "Optimization time for model  25 :  0.00012803077697753906\n",
      "Optimization time for model  26 :  0.00017499923706054688\n",
      "Optimization time for model  27 :  0.00012183189392089844\n",
      "Optimization time for model  28 :  0.00016498565673828125\n",
      "Optimization time for model  29 :  6.480931043624878\n",
      "Optimization time for model  30 :  0.00015807151794433594\n",
      "Optimization time for model  31 :  0.00017499923706054688\n",
      "Optimization time for model  0 :  0.00017499923706054688\n",
      "Optimization time for model  1 :  0.00018787384033203125\n",
      "Optimization time for model  2 :  0.000186920166015625\n",
      "Optimization time for model  3 :  0.0001289844512939453\n",
      "Optimization time for model  4 :  0.00014495849609375\n",
      "Optimization time for model  5 :  0.0001201629638671875\n",
      "Optimization time for model  6 :  9.936229228973389\n",
      "Optimization time for model  7 :  0.00022602081298828125\n",
      "Optimization time for model  8 :  0.00014209747314453125\n",
      "Optimization time for model  9 :  0.00017499923706054688\n",
      "Optimization time for model  10 :  3.4579389095306396\n",
      "Optimization time for model  11 :  0.00020503997802734375\n",
      "Optimization time for model  12 :  0.00013589859008789062\n",
      "Optimization time for model  13 :  0.00012612342834472656\n",
      "Optimization time for model  14 :  0.0001728534698486328\n",
      "Optimization time for model  15 :  3.701200008392334\n",
      "Model is feasible\n",
      "Optimization time for model  17 :  0.12450408935546875\n",
      "Optimization time for model  18 :  1.0790040493011475\n",
      "Optimization time for model  19 :  1.7863759994506836\n",
      "Optimization time for model  20 :  0.00015115737915039062\n",
      "Optimization time for model  21 :  0.00015997886657714844\n",
      "Optimization time for model  22 :  0.00012803077697753906\n",
      "Optimization time for model  23 :  0.1927189826965332\n",
      "Optimization time for model  24 :  0.00012993812561035156\n",
      "Optimization time for model  25 :  0.00012302398681640625\n",
      "Optimization time for model  26 :  0.0001499652862548828\n",
      "Optimization time for model  27 :  0.00017404556274414062\n",
      "Optimization time for model  28 :  0.00011801719665527344\n",
      "Optimization time for model  29 :  0.00016808509826660156\n",
      "Optimization time for model  30 :  0.0001709461212158203\n",
      "Optimization time for model  31 :  0.00017404556274414062\n",
      "Optimization time for model  0 :  0.00016307830810546875\n",
      "Optimization time for model  1 :  0.00014090538024902344\n",
      "Optimization time for model  2 :  0.00015091896057128906\n",
      "Optimization time for model  3 :  2.0558829307556152\n",
      "Optimization time for model  4 :  0.000431060791015625\n",
      "Optimization time for model  5 :  0.00015211105346679688\n",
      "Optimization time for model  6 :  0.0001289844512939453\n",
      "Optimization time for model  7 :  0.0001220703125\n",
      "Optimization time for model  8 :  0.0001227855682373047\n",
      "Optimization time for model  9 :  0.00018715858459472656\n",
      "Optimization time for model  10 :  0.00016880035400390625\n",
      "Optimization time for model  11 :  0.0001621246337890625\n",
      "Optimization time for model  12 :  0.0001621246337890625\n",
      "Optimization time for model  13 :  0.0001659393310546875\n",
      "Optimization time for model  14 :  0.00012111663818359375\n",
      "Optimization time for model  15 :  0.00011706352233886719\n",
      "Optimization time for model  16 :  0.00012183189392089844\n",
      "Optimization time for model  17 :  0.00013899803161621094\n",
      "Optimization time for model  18 :  1.8567349910736084\n",
      "Optimization time for model  19 :  0.0001518726348876953\n",
      "Optimization time for model  20 :  0.9852678775787354\n",
      "Optimization time for model  21 :  0.0002079010009765625\n",
      "Optimization time for model  22 :  0.4438290596008301\n",
      "Optimization time for model  23 :  0.0001399517059326172\n",
      "Optimization time for model  24 :  0.0001900196075439453\n",
      "Optimization time for model  25 :  0.00017404556274414062\n",
      "Optimization time for model  26 :  0.0001201629638671875\n",
      "Optimization time for model  27 :  0.0001220703125\n",
      "Optimization time for model  28 :  0.00017404556274414062\n",
      "Optimization time for model  29 :  0.00014400482177734375\n",
      "Optimization time for model  30 :  0.0001227855682373047\n",
      "Optimization time for model  31 :  0.00016880035400390625\n",
      "Optimization time for model  0 :  0.00014901161193847656\n",
      "Optimization time for model  1 :  0.00018596649169921875\n",
      "Optimization time for model  2 :  0.3714108467102051\n",
      "Optimization time for model  3 :  4.735655069351196\n",
      "Optimization time for model  4 :  0.00014281272888183594\n",
      "Optimization time for model  5 :  0.00013518333435058594\n",
      "Optimization time for model  6 :  0.00017905235290527344\n",
      "Optimization time for model  7 :  0.00016808509826660156\n",
      "Optimization time for model  8 :  0.0001671314239501953\n",
      "Optimization time for model  9 :  0.00012111663818359375\n",
      "Optimization time for model  10 :  0.015347957611083984\n",
      "Optimization time for model  11 :  0.941256046295166\n",
      "Optimization time for model  12 :  0.00018286705017089844\n",
      "Optimization time for model  13 :  0.00017595291137695312\n",
      "Model is feasible\n",
      "Optimization time for model  15 :  0.9390959739685059\n",
      "Optimization time for model  16 :  0.00015687942504882812\n",
      "Optimization time for model  17 :  0.0001590251922607422\n",
      "Optimization time for model  18 :  0.00014591217041015625\n",
      "Optimization time for model  19 :  0.00013208389282226562\n",
      "Optimization time for model  20 :  0.00017595291137695312\n",
      "Optimization time for model  21 :  0.0001690387725830078\n",
      "Optimization time for model  22 :  0.00016498565673828125\n",
      "Optimization time for model  23 :  0.0001811981201171875\n",
      "Optimization time for model  24 :  1.334907054901123\n",
      "Optimization time for model  25 :  0.00020503997802734375\n",
      "Optimization time for model  26 :  0.0001327991485595703\n",
      "Optimization time for model  27 :  0.00015091896057128906\n",
      "Optimization time for model  28 :  0.0001728534698486328\n",
      "Optimization time for model  29 :  0.00019884109497070312\n",
      "Model is feasible\n",
      "Optimization time for model  31 :  0.00013494491577148438\n",
      "Optimization time for model  0 :  0.20569896697998047\n",
      "Optimization time for model  1 :  0.00019407272338867188\n",
      "Optimization time for model  2 :  0.00013399124145507812\n",
      "Optimization time for model  3 :  2.402513027191162\n",
      "Optimization time for model  4 :  0.00020885467529296875\n",
      "Optimization time for model  5 :  0.00016188621520996094\n",
      "Optimization time for model  6 :  0.00013494491577148438\n",
      "Optimization time for model  7 :  0.00015091896057128906\n",
      "Optimization time for model  8 :  0.0009989738464355469\n",
      "Optimization time for model  9 :  0.00013113021850585938\n",
      "Optimization time for model  10 :  0.00019097328186035156\n",
      "Optimization time for model  11 :  0.00015497207641601562\n",
      "Optimization time for model  12 :  0.00014901161193847656\n",
      "Optimization time for model  13 :  0.00016808509826660156\n",
      "Optimization time for model  14 :  0.00016808509826660156\n",
      "Optimization time for model  15 :  0.0008389949798583984\n",
      "Optimization time for model  16 :  0.0001227855682373047\n",
      "Model is feasible\n",
      "Optimization time for model  18 :  0.00017976760864257812\n",
      "Optimization time for model  19 :  0.00018596649169921875\n",
      "Optimization time for model  20 :  1.456563949584961\n",
      "Optimization time for model  21 :  0.0001590251922607422\n",
      "Optimization time for model  22 :  0.00020384788513183594\n",
      "Optimization time for model  23 :  0.00012803077697753906\n",
      "Optimization time for model  24 :  0.00017404556274414062\n",
      "Optimization time for model  25 :  3.748418092727661\n",
      "Optimization time for model  26 :  0.00018095970153808594\n",
      "Optimization time for model  27 :  0.0001628398895263672\n",
      "Optimization time for model  28 :  0.00018715858459472656\n",
      "Optimization time for model  29 :  0.0001800060272216797\n",
      "Optimization time for model  30 :  0.0001277923583984375\n",
      "Optimization time for model  31 :  0.00012087821960449219\n",
      "Optimization time for model  0 :  0.0001308917999267578\n",
      "Optimization time for model  1 :  0.00014901161193847656\n",
      "Optimization time for model  2 :  9.16350793838501\n",
      "Optimization time for model  3 :  0.0002079010009765625\n",
      "Optimization time for model  4 :  0.0001418590545654297\n",
      "Optimization time for model  5 :  0.0001308917999267578\n",
      "Optimization time for model  6 :  2.45442795753479\n",
      "Optimization time for model  7 :  0.00017690658569335938\n",
      "Optimization time for model  8 :  0.10532498359680176\n",
      "Optimization time for model  9 :  2.146185874938965\n",
      "Optimization time for model  10 :  0.00014901161193847656\n",
      "Optimization time for model  11 :  0.0001659393310546875\n",
      "Optimization time for model  12 :  0.0001239776611328125\n",
      "Optimization time for model  13 :  0.0001709461212158203\n",
      "Optimization time for model  14 :  0.00011682510375976562\n",
      "Optimization time for model  15 :  10.116237878799438\n",
      "Optimization time for model  16 :  3.013524055480957\n",
      "Optimization time for model  17 :  0.0001850128173828125\n",
      "Optimization time for model  18 :  0.0001728534698486328\n",
      "Optimization time for model  19 :  0.0001690387725830078\n",
      "Optimization time for model  20 :  0.00011610984802246094\n",
      "Optimization time for model  21 :  0.00012111663818359375\n",
      "Optimization time for model  22 :  0.0002090930938720703\n",
      "Optimization time for model  23 :  0.000125885009765625\n",
      "Optimization time for model  24 :  2.235576868057251\n",
      "Optimization time for model  25 :  0.00012993812561035156\n",
      "Optimization time for model  26 :  0.00017786026000976562\n",
      "Optimization time for model  27 :  0.0001220703125\n",
      "Optimization time for model  28 :  0.00017189979553222656\n",
      "Optimization time for model  29 :  0.00011920928955078125\n",
      "Optimization time for model  30 :  0.0001671314239501953\n",
      "Optimization time for model  31 :  5.216686010360718\n",
      "Optimization time for model  0 :  0.00014495849609375\n",
      "Optimization time for model  1 :  0.00012993812561035156\n",
      "Optimization time for model  2 :  0.39567017555236816\n",
      "Optimization time for model  3 :  0.00018405914306640625\n",
      "Optimization time for model  4 :  0.00017499923706054688\n",
      "Optimization time for model  5 :  0.002913951873779297\n",
      "Optimization time for model  6 :  0.7152090072631836\n",
      "Optimization time for model  7 :  2.5703930854797363\n",
      "Optimization time for model  8 :  0.00015592575073242188\n",
      "Optimization time for model  9 :  0.00018715858459472656\n",
      "Optimization time for model  10 :  0.00017499923706054688\n",
      "Optimization time for model  11 :  0.00011992454528808594\n",
      "Optimization time for model  12 :  3.083355188369751\n",
      "Model is feasible\n",
      "Optimization time for model  14 :  0.00018596649169921875\n",
      "Optimization time for model  15 :  0.0001780986785888672\n",
      "Optimization time for model  16 :  0.00011897087097167969\n",
      "Optimization time for model  17 :  0.00014400482177734375\n",
      "Optimization time for model  18 :  0.00011587142944335938\n",
      "Optimization time for model  19 :  0.00011992454528808594\n",
      "Optimization time for model  20 :  2.693337917327881\n",
      "Optimization time for model  21 :  0.00015401840209960938\n",
      "Optimization time for model  22 :  0.00013113021850585938\n",
      "Optimization time for model  23 :  0.0002689361572265625\n",
      "Optimization time for model  24 :  0.0001690387725830078\n",
      "Optimization time for model  25 :  0.00017404556274414062\n",
      "Optimization time for model  26 :  3.9046502113342285\n",
      "Optimization time for model  27 :  0.00013399124145507812\n",
      "Optimization time for model  28 :  0.0001780986785888672\n",
      "Optimization time for model  29 :  0.00011801719665527344\n",
      "Optimization time for model  30 :  0.00011897087097167969\n",
      "Optimization time for model  31 :  0.00017404556274414062\n",
      "Optimization time for model  0 :  0.0001938343048095703\n",
      "Optimization time for model  1 :  0.0001308917999267578\n",
      "Optimization time for model  2 :  0.0010280609130859375\n",
      "Optimization time for model  3 :  10.819159030914307\n",
      "Optimization time for model  4 :  0.00013494491577148438\n",
      "Optimization time for model  5 :  0.0001780986785888672\n",
      "Optimization time for model  6 :  0.0001728534698486328\n",
      "Optimization time for model  7 :  0.3922309875488281\n",
      "Model is feasible\n",
      "Optimization time for model  9 :  0.0001800060272216797\n",
      "Optimization time for model  10 :  0.00011897087097167969\n",
      "Optimization time for model  11 :  0.00012087821960449219\n",
      "Optimization time for model  12 :  0.00017595291137695312\n",
      "Optimization time for model  13 :  0.00012087821960449219\n",
      "Optimization time for model  14 :  0.00041103363037109375\n",
      "Optimization time for model  15 :  0.00012493133544921875\n",
      "Optimization time for model  16 :  0.00012111663818359375\n",
      "Optimization time for model  17 :  0.014889001846313477\n",
      "Optimization time for model  18 :  0.00013017654418945312\n",
      "Optimization time for model  19 :  0.00012111663818359375\n",
      "Optimization time for model  20 :  0.0001709461212158203\n",
      "Optimization time for model  21 :  1.5263822078704834\n",
      "Optimization time for model  22 :  0.00018596649169921875\n",
      "Optimization time for model  23 :  0.0001270771026611328\n",
      "Optimization time for model  24 :  0.00012612342834472656\n",
      "Optimization time for model  25 :  0.00014019012451171875\n",
      "Optimization time for model  26 :  0.00016808509826660156\n",
      "Optimization time for model  27 :  0.0001621246337890625\n",
      "Optimization time for model  28 :  0.00016188621520996094\n",
      "Optimization time for model  29 :  1.9885189533233643\n",
      "Optimization time for model  30 :  0.000186920166015625\n",
      "Optimization time for model  31 :  0.00017786026000976562\n",
      "Optimization time for model  0 :  0.6696150302886963\n",
      "Optimization time for model  1 :  0.0001919269561767578\n",
      "Optimization time for model  2 :  0.00018596649169921875\n",
      "Optimization time for model  3 :  0.0001518726348876953\n",
      "Optimization time for model  4 :  0.0001418590545654297\n",
      "Optimization time for model  5 :  1.9336318969726562\n",
      "Optimization time for model  6 :  0.00014519691467285156\n",
      "Optimization time for model  7 :  0.006532907485961914\n",
      "Optimization time for model  8 :  0.00018405914306640625\n",
      "Optimization time for model  9 :  0.00018286705017089844\n",
      "Optimization time for model  10 :  0.00012493133544921875\n",
      "Optimization time for model  11 :  0.00012302398681640625\n",
      "Optimization time for model  12 :  2.7838239669799805\n",
      "Optimization time for model  13 :  0.0003230571746826172\n",
      "Optimization time for model  14 :  0.0001289844512939453\n",
      "Optimization time for model  15 :  0.00012993812561035156\n",
      "Optimization time for model  16 :  0.0001251697540283203\n",
      "Optimization time for model  17 :  0.00014495849609375\n",
      "Optimization time for model  18 :  0.0001659393310546875\n",
      "Optimization time for model  19 :  0.0001671314239501953\n",
      "Optimization time for model  20 :  0.00014400482177734375\n",
      "Optimization time for model  21 :  0.00019311904907226562\n",
      "Optimization time for model  22 :  0.00012493133544921875\n",
      "Optimization time for model  23 :  0.00011801719665527344\n",
      "Optimization time for model  24 :  0.00016498565673828125\n",
      "Optimization time for model  25 :  6.731055974960327\n",
      "Optimization time for model  26 :  0.00019097328186035156\n",
      "Optimization time for model  27 :  0.00020194053649902344\n",
      "Optimization time for model  28 :  0.00017595291137695312\n",
      "Optimization time for model  29 :  2.299118995666504\n",
      "Optimization time for model  30 :  0.0006880760192871094\n",
      "Optimization time for model  31 :  0.00018596649169921875\n",
      "Optimization time for model  0 :  0.00018215179443359375\n",
      "Optimization time for model  1 :  0.00015211105346679688\n",
      "Optimization time for model  2 :  0.00016617774963378906\n",
      "Optimization time for model  3 :  0.000164031982421875\n",
      "Optimization time for model  4 :  0.00012111663818359375\n",
      "Optimization time for model  5 :  0.00016307830810546875\n",
      "Optimization time for model  6 :  0.00012111663818359375\n",
      "Optimization time for model  7 :  0.0001399517059326172\n",
      "Optimization time for model  8 :  0.00014209747314453125\n",
      "Optimization time for model  9 :  0.026588916778564453\n",
      "Optimization time for model  10 :  0.00017905235290527344\n",
      "Optimization time for model  11 :  0.0001709461212158203\n",
      "Optimization time for model  12 :  0.0001251697540283203\n",
      "Optimization time for model  13 :  0.0001239776611328125\n",
      "Optimization time for model  14 :  0.034689903259277344\n",
      "Optimization time for model  15 :  0.00017905235290527344\n"
     ]
    }
   ],
   "source": [
    "test_models = []\n",
    "gurobi_env = gb.Env()\n",
    "gurobi_env.setParam(\"OutputFlag\", 0)\n",
    "\n",
    "model_files = pkl.load(open(\"Data/corlat/train_test_data/pickle_filenames.pkl\", \"rb\"))\n",
    "# rename from .pickle to .lp\n",
    "filename = []\n",
    "for i in range(len(model_files)):\n",
    "    filename.append(model_files[i].replace(\".pickle\", \".lp\"))\n",
    "    \n",
    "for i in range(len(test_indices)):\n",
    "    model = gb.read(\"Data/corlat/instances/\" + filename[test_indices[i]], env=gurobi_env)\n",
    "    test_models.append(model)\n",
    "    \n",
    "# loop through all test models and calculate average optimization time\n",
    "opt_time = []\n",
    "for i, data in enumerate(valid_loader):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    inputs = inputs.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    \n",
    "    # get slices of test_models according to batch size\n",
    "    len_test_models = len(test_models)\n",
    "\n",
    "    test_models_batch = test_models[i*batch_size: min((i+1)*batch_size, len_test_models)]\n",
    "    \n",
    "    opt_time_batch = calculate_equality_constraint_opt_time(test_models_batch, binary_indices, outputs.detach().cpu().numpy())\n",
    "    \n",
    "    opt_time.append(opt_time_batch)\n",
    "    \n",
    "# save opt_time\n",
    "with open(\"Data/corlat/opt_time_equality_constraint.pickle\", \"wb\") as f:\n",
    "    pkl.dump(opt_time, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average optimization time:  0.44385798549165534\n"
     ]
    }
   ],
   "source": [
    "# flatten opt_time\n",
    "opt_time_flat = [item for sublist in opt_time for item in sublist]\n",
    "print(\"Average optimization time: \", np.mean(opt_time_flat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (optimization)",
   "language": "python",
   "name": "optimization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
