{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch.nn.functional import relu\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = (\n",
    "        False  # Force cuDNN to use a consistent convolution algorithm\n",
    "    )\n",
    "    torch.backends.cudnn.deterministic = (\n",
    "        True  # Force cuDNN to use deterministic algorithms if available\n",
    "    )\n",
    "    torch.use_deterministic_algorithms(\n",
    "        True\n",
    "    )  # Force torch to use deterministic algorithms if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'train_val_split': [0.80, 0.20], # These must sum to 1.0\n",
    "        'batch_size' : 32, # Num samples to average over for gradient updates\n",
    "        'EPOCHS' : 100, # Num times to iterate over the entire dataset\n",
    "        'LEARNING_RATE' : 1e-3, # Learning rate for the optimizer\n",
    "        'BETA1' : 0.9, # Beta1 parameter for the Adam optimizer\n",
    "        'BETA2' : 0.999, # Beta2 parameter for the Adam optimizer\n",
    "        'WEIGHT_DECAY' : 1e-4, # Weight decay parameter for the Adam optimizer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/corlat_preprocessed.pickle\", \"rb\"))\n",
    "except:\n",
    "    # move dir to /ibm/gpfs/home/yjin0055/Project/DayAheadForecast\n",
    "    os.chdir(\"/ibm/gpfs/home/yjin0055/Project/DayAheadForecast\")\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/corlat_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_obj_coef</th>\n",
       "      <th>num_nonzero_coef</th>\n",
       "      <th>lp_relax_val</th>\n",
       "      <th>is_lp_relax_val_frac</th>\n",
       "      <th>lp_sol_val_eq_lb</th>\n",
       "      <th>lp_sol_val_eq_ub</th>\n",
       "      <th>has_lb</th>\n",
       "      <th>has_ub</th>\n",
       "      <th>mean_degree</th>\n",
       "      <th>std_degree</th>\n",
       "      <th>min_degree</th>\n",
       "      <th>max_degree</th>\n",
       "      <th>mean_coef</th>\n",
       "      <th>std_coef</th>\n",
       "      <th>min_coef</th>\n",
       "      <th>max_coef</th>\n",
       "      <th>var_type_B</th>\n",
       "      <th>var_type_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>36.706039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-50.166667</td>\n",
       "      <td>49.837792</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>45.415367</td>\n",
       "      <td>2.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-48.833333</td>\n",
       "      <td>51.275129</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.442327</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>45.415367</td>\n",
       "      <td>2.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-50.166667</td>\n",
       "      <td>49.837792</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>45.415367</td>\n",
       "      <td>2.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-48.833333</td>\n",
       "      <td>51.275129</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>45.415367</td>\n",
       "      <td>2.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-48.833333</td>\n",
       "      <td>51.275129</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   var_obj_coef  num_nonzero_coef  lp_relax_val  is_lp_relax_val_frac  \\\n",
       "0           5.0               6.0      1.000000                  True   \n",
       "1           4.0               6.0      0.000000                  True   \n",
       "2           6.0               6.0      0.442327                  True   \n",
       "3           5.0               6.0     -0.000000                  True   \n",
       "4           3.0               6.0     -0.000000                  True   \n",
       "\n",
       "   lp_sol_val_eq_lb  lp_sol_val_eq_ub  has_lb  has_ub  mean_degree  \\\n",
       "0              True              True    True    True    19.000000   \n",
       "1              True              True    True    True    35.333333   \n",
       "2              True              True    True    True    35.333333   \n",
       "3              True              True    True    True    35.333333   \n",
       "4              True              True    True    True    35.333333   \n",
       "\n",
       "   std_degree  min_degree  max_degree  mean_coef   std_coef  min_coef  \\\n",
       "0   36.706039         1.0       101.0 -50.166667  49.837792    -100.0   \n",
       "1   45.415367         2.0       101.0 -48.833333  51.275129    -100.0   \n",
       "2   45.415367         2.0       101.0 -50.166667  49.837792    -100.0   \n",
       "3   45.415367         2.0       101.0 -48.833333  51.275129    -100.0   \n",
       "4   45.415367         2.0       101.0 -48.833333  51.275129    -100.0   \n",
       "\n",
       "   max_coef  var_type_B  var_type_C  \n",
       "0       1.0           1           0  \n",
       "1       9.0           1           0  \n",
       "2       1.0           1           0  \n",
       "3       9.0           1           0  \n",
       "4       9.0           1           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corlat_dataset[0][\"input\"][\"var_node_features\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the maximum size of N_constraints and N_variables across the dataset.\n",
    "\n",
    "max_N_constraints = max(\n",
    "    len(x[\"input\"][\"constraint_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "max_N_variables = max(\n",
    "    len(x[\"input\"][\"var_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "min_N_constraints = min(\n",
    "    len(x[\"input\"][\"constraint_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "min_N_variables = min(\n",
    "    len(x[\"input\"][\"var_node_features\"]) for x in corlat_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of variables:  466\n",
      "Maximum number of constraints:  551\n",
      "Minimum number of variables:  466\n",
      "Minimum number of constraints:  470\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum number of variables: \", max_N_variables)\n",
    "print(\"Maximum number of constraints: \", max_N_constraints)\n",
    "print(\"Minimum number of variables: \", min_N_variables)\n",
    "print(\"Minimum number of constraints: \", min_N_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variable node features:  18\n",
      "Number of constraint node features:  10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of variable node features: \", len(corlat_dataset[0][\"input\"][\"var_node_features\"].columns))\n",
    "print(\"Number of constraint node features: \", len(corlat_dataset[0][\"input\"][\"constraint_node_features\"].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each variable node features, pad with 0.0s to make it the same length as the maximum number of variables\n",
    "\n",
    "var_node_features = np.stack(\n",
    "    [\n",
    "        np.pad(\n",
    "            x[\"input\"][\"var_node_features\"].values,\n",
    "            ((0, max_N_variables - len(x[\"input\"][\"var_node_features\"])), (0, 0)),\n",
    "            \"constant\",\n",
    "            constant_values=0.0,\n",
    "        )\n",
    "        for x in corlat_dataset\n",
    "    ]\n",
    ")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 466, 18)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_node_features = np.stack(\n",
    "    [\n",
    "        np.pad(\n",
    "            x[\"input\"][\"constraint_node_features\"].values,\n",
    "            ((0, max_N_constraints - len(x[\"input\"][\"constraint_node_features\"])), (0, 0)),\n",
    "            \"constant\",\n",
    "            constant_values=0.0,\n",
    "        )\n",
    "        for x in corlat_dataset\n",
    "    ]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 551, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraint_node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_node_features and constraint_node_features, reshape to (N_samples, -1) to feed into the neural network\n",
    "var_input = var_node_features.reshape(var_node_features.shape[0], -1)\n",
    "constraint_input = constraint_node_features.reshape(constraint_node_features.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of variable features input:  (2000, 8388)\n",
      "Shape of constraint features input:  (2000, 5510)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of variable features input: \", var_input.shape)\n",
    "print(\"Shape of constraint features input: \", constraint_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get A matrix input by stacking the csr_matrix of each sample getting shape of N_samples x (A.shape[0] x A.shape[1])\n",
    "A_input = np.vstack([x[\"input\"][\"A\"] for x in corlat_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_feature_list = []\n",
    "for i in range(len(corlat_dataset)):\n",
    "    n_cons = corlat_dataset[i][\"input\"][\"A\"].shape[0]\n",
    "\n",
    "    # for row in range(n_vars):\n",
    "    #     for col in range(n_cons):\n",
    "    #         if input_dict_list[i][\"A\"][row, col] != 0:\n",
    "    #             adj_matrix[row, n_vars + col] = input_dict_list[i][\"A\"][row, col]\n",
    "    #             adj_matrix[n_vars + col, row] = input_dict_list[i][\"A\"][row, col]\n",
    "\n",
    "    I, J, V = scipy.sparse.find(corlat_dataset[i][\"input\"][\"A\"])\n",
    "    # adj_matrix[I, n_vars + J] = V\n",
    "    # adj_matrix[n_vars + J, I] = V\n",
    "\n",
    "    # # convert to COO format\n",
    "    edge_index = torch.stack([torch.tensor(I), torch.tensor(n_cons + J)], dim=0)\n",
    "\n",
    "    # expand V to 2D\n",
    "    edge_attr = torch.tensor(V).unsqueeze(1)\n",
    "\n",
    "    tmp_dict = {\"edge_index\": edge_index, \"edge_attr\": edge_attr}\n",
    "    A_feature_list.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sample, pad the edge_index and edge_attr to make it the same length as the maximum length of edge_index and edge_attr\n",
    "max_edge_index_len = max([len(x[\"edge_index\"][0]) for x in A_feature_list])\n",
    "max_edge_attr_len = max([len(x[\"edge_attr\"]) for x in A_feature_list])\n",
    "\n",
    "for i in range(len(A_feature_list)):\n",
    "    edge_index = A_feature_list[i][\"edge_index\"]\n",
    "    edge_attr = A_feature_list[i][\"edge_attr\"]\n",
    "\n",
    "    # pad edge_index\n",
    "    edge_index = torch.cat(\n",
    "        [\n",
    "            edge_index,\n",
    "            torch.zeros(\n",
    "                2, max_edge_index_len - len(edge_index[0]), dtype=torch.long\n",
    "            ),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    # pad edge_attr\n",
    "    edge_attr = torch.cat(\n",
    "        [\n",
    "            edge_attr,\n",
    "            torch.zeros(\n",
    "                max_edge_attr_len - len(edge_attr), 1, dtype=torch.float32\n",
    "            ),\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    A_feature_list[i][\"edge_index\"] = edge_index\n",
    "    A_feature_list[i][\"edge_attr\"] = edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the padding is correct by checking the shape of edge_index and edge_attr\n",
    "for i in range(len(A_feature_list)):\n",
    "    assert A_feature_list[i][\"edge_index\"].shape == (2, max_edge_index_len)\n",
    "    assert A_feature_list[i][\"edge_attr\"].shape == (max_edge_attr_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A matrix input:  (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of A matrix input: \", A_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['solution', 'indices', 'input'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corlat_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each solution convert the dictionary to a list of values\n",
    "solutions = [\n",
    "    list(corlat_dataset[i][\"solution\"].values())\n",
    "    for i in range(len(corlat_dataset))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert solutions_list to numpy array\n",
    "solutions = np.array(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the variable features and constraint features into a single input\n",
    "X = np.hstack([var_input, constraint_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.shape[1]\n",
    "out_channels = solutions.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 13898)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_features//4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(n_features//2, n_features//2)\n",
    "        self.fc3 = nn.Linear(n_features//2, n_features//2)\n",
    "        self.fc4 = nn.Linear(n_features//2, out_channels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # add regularization\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check type for one sample of solutions\n",
    "solutions[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X and solutions to float32\n",
    "X = X.astype(np.float32)\n",
    "solutions = solutions.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all samples have same dimension\n",
    "for i in range(len(solutions)):\n",
    "    assert solutions[i].shape == (100,)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    assert X[i].shape == (13898,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split to get indices for train and test\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(len(solutions)), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train = X[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_train = solutions[train_idx]\n",
    "y_test = solutions[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, solutions, test_size=0.2, random_state=42, shuffle=True)\n",
    "# save the train and test data in the directory Data/corlat/\n",
    "np.save(\"Data/corlat/X_train.npy\", X_train)\n",
    "np.save(\"Data/corlat/X_test.npy\", X_test)\n",
    "np.save(\"Data/corlat/y_train.npy\", y_train)\n",
    "np.save(\"Data/corlat/y_test.npy\", y_test)\n",
    "np.save(\"Data/corlat/train_idx.npy\", train_idx)\n",
    "np.save(\"Data/corlat/test_idx.npy\", test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()\n",
    "net = torch.compile(net)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "# create the dataloader for X and solutions\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train), torch.tensor(y_train)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_test), torch.tensor(y_test)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "params = list(net.parameters())\n",
    "\n",
    "# optimizer = AdamW(params, lr=config['LEARNING_RATE'], weight_decay=1e-4)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "# optimizer = dadaptation.DAdaptAdam(params, lr=1, log_every=5, betas=(BETA1, BETA2), weight_decay=1e-4, decouple=True)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "total_steps = len(train_loader)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config['LEARNING_RATE'], steps_per_epoch=total_steps, epochs=config['EPOCHS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.804 lr: 0.000040\n",
      "Epoch 2 loss: 0.696 lr: 0.000043\n",
      "Epoch 3 loss: 0.674 lr: 0.000051\n",
      "Epoch 4 loss: 0.659 lr: 0.000064\n",
      "Epoch 5 loss: 0.648 lr: 0.000082\n",
      "Epoch 6 loss: 0.635 lr: 0.000104\n",
      "Epoch 7 loss: 0.621 lr: 0.000132\n",
      "Epoch 8 loss: 0.605 lr: 0.000163\n",
      "Epoch 9 loss: 0.592 lr: 0.000199\n",
      "Epoch 10 loss: 0.580 lr: 0.000238\n",
      "Epoch 11 loss: 0.568 lr: 0.000280\n",
      "Epoch 12 loss: 0.559 lr: 0.000325\n",
      "Epoch 13 loss: 0.552 lr: 0.000372\n",
      "Epoch 14 loss: 0.547 lr: 0.000421\n",
      "Epoch 15 loss: 0.544 lr: 0.000470\n",
      "Epoch 16 loss: 0.541 lr: 0.000521\n",
      "Epoch 17 loss: 0.540 lr: 0.000571\n",
      "Epoch 18 loss: 0.539 lr: 0.000620\n",
      "Epoch 19 loss: 0.536 lr: 0.000669\n",
      "Epoch 20 loss: 0.536 lr: 0.000716\n",
      "Epoch 21 loss: 0.535 lr: 0.000761\n",
      "Epoch 22 loss: 0.533 lr: 0.000803\n",
      "Epoch 23 loss: 0.533 lr: 0.000842\n",
      "Epoch 24 loss: 0.532 lr: 0.000877\n",
      "Epoch 25 loss: 0.531 lr: 0.000909\n",
      "Epoch 26 loss: 0.530 lr: 0.000936\n",
      "Epoch 27 loss: 0.530 lr: 0.000959\n",
      "Epoch 28 loss: 0.529 lr: 0.000977\n",
      "Epoch 29 loss: 0.528 lr: 0.000990\n",
      "Epoch 30 loss: 0.528 lr: 0.000997\n",
      "Epoch 31 loss: 0.527 lr: 0.001000\n",
      "Epoch 32 loss: 0.527 lr: 0.000999\n",
      "Epoch 33 loss: 0.527 lr: 0.000998\n",
      "Epoch 34 loss: 0.526 lr: 0.000995\n",
      "Epoch 35 loss: 0.525 lr: 0.000992\n",
      "Epoch 36 loss: 0.526 lr: 0.000987\n",
      "Epoch 37 loss: 0.526 lr: 0.000982\n",
      "Epoch 38 loss: 0.524 lr: 0.000975\n",
      "Epoch 39 loss: 0.524 lr: 0.000968\n",
      "Epoch 40 loss: 0.524 lr: 0.000960\n",
      "Epoch 41 loss: 0.524 lr: 0.000950\n",
      "Epoch 42 loss: 0.523 lr: 0.000940\n",
      "Epoch 43 loss: 0.523 lr: 0.000929\n",
      "Epoch 44 loss: 0.523 lr: 0.000917\n",
      "Epoch 45 loss: 0.522 lr: 0.000904\n",
      "Epoch 46 loss: 0.521 lr: 0.000891\n",
      "Epoch 47 loss: 0.521 lr: 0.000876\n",
      "Epoch 48 loss: 0.521 lr: 0.000861\n",
      "Epoch 49 loss: 0.521 lr: 0.000845\n",
      "Epoch 50 loss: 0.521 lr: 0.000829\n",
      "Epoch 51 loss: 0.521 lr: 0.000811\n",
      "Epoch 52 loss: 0.521 lr: 0.000794\n",
      "Epoch 53 loss: 0.520 lr: 0.000775\n",
      "Epoch 54 loss: 0.520 lr: 0.000756\n",
      "Epoch 55 loss: 0.520 lr: 0.000737\n",
      "Epoch 56 loss: 0.520 lr: 0.000717\n",
      "Epoch 57 loss: 0.520 lr: 0.000696\n",
      "Epoch 58 loss: 0.519 lr: 0.000675\n",
      "Epoch 59 loss: 0.521 lr: 0.000654\n",
      "Epoch 60 loss: 0.519 lr: 0.000633\n",
      "Epoch 61 loss: 0.519 lr: 0.000611\n",
      "Epoch 62 loss: 0.518 lr: 0.000589\n",
      "Epoch 63 loss: 0.518 lr: 0.000567\n",
      "Epoch 64 loss: 0.519 lr: 0.000544\n",
      "Epoch 65 loss: 0.518 lr: 0.000522\n",
      "Epoch 66 loss: 0.518 lr: 0.000500\n",
      "Epoch 67 loss: 0.518 lr: 0.000477\n",
      "Epoch 68 loss: 0.517 lr: 0.000455\n",
      "Epoch 69 loss: 0.517 lr: 0.000432\n",
      "Epoch 70 loss: 0.517 lr: 0.000410\n",
      "Epoch 71 loss: 0.517 lr: 0.000388\n",
      "Epoch 72 loss: 0.517 lr: 0.000367\n",
      "Epoch 73 loss: 0.517 lr: 0.000345\n",
      "Epoch 74 loss: 0.517 lr: 0.000324\n",
      "Epoch 75 loss: 0.516 lr: 0.000303\n",
      "Epoch 76 loss: 0.516 lr: 0.000283\n",
      "Epoch 77 loss: 0.516 lr: 0.000263\n",
      "Epoch 78 loss: 0.516 lr: 0.000243\n",
      "Epoch 79 loss: 0.516 lr: 0.000224\n",
      "Epoch 80 loss: 0.516 lr: 0.000206\n",
      "Epoch 81 loss: 0.515 lr: 0.000188\n",
      "Epoch 82 loss: 0.515 lr: 0.000171\n",
      "Epoch 83 loss: 0.514 lr: 0.000154\n",
      "Epoch 84 loss: 0.515 lr: 0.000138\n",
      "Epoch 85 loss: 0.515 lr: 0.000123\n",
      "Epoch 86 loss: 0.515 lr: 0.000109\n",
      "Epoch 87 loss: 0.514 lr: 0.000095\n",
      "Epoch 88 loss: 0.515 lr: 0.000082\n",
      "Epoch 89 loss: 0.514 lr: 0.000071\n",
      "Epoch 90 loss: 0.515 lr: 0.000059\n",
      "Epoch 91 loss: 0.514 lr: 0.000049\n",
      "Epoch 92 loss: 0.514 lr: 0.000040\n",
      "Epoch 93 loss: 0.514 lr: 0.000032\n",
      "Epoch 94 loss: 0.514 lr: 0.000024\n",
      "Epoch 95 loss: 0.514 lr: 0.000018\n",
      "Epoch 96 loss: 0.514 lr: 0.000012\n",
      "Epoch 97 loss: 0.514 lr: 0.000008\n",
      "Epoch 98 loss: 0.514 lr: 0.000004\n",
      "Epoch 99 loss: 0.514 lr: 0.000002\n",
      "Epoch 100 loss: 0.514 lr: 0.000000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d loss: %.3f lr: %.6f' % (epoch + 1, running_loss / len(train_loader), curr_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.6333832131682764\n",
      "Precision:  0.7301238827034656\n",
      "Recall:  0.5592792792792792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91040/2196990482.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_test = y_test.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# validation of the model using f1 score, precision and recall\n",
    "y_pred = net(torch.tensor(X_test))\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "y_test = y_test.astype(np.int)\n",
    "\n",
    "print(\"F1 score: \", f1_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred, average=\"micro\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91040/2751798150.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_test = y_test.astype(np.int)\n",
      "/tmp/ipykernel_91040/2751798150.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_train = y_train.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost model\n",
    "y_test = y_test.astype(np.int)\n",
    "y_train = y_train.astype(np.int)\n",
    "clf = XGBClassifier(tree_method='hist')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.9237003241685676\n",
      "Precision:  0.9232569302772111\n",
      "Recall:  0.9241441441441441\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score: \", f1_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred, average=\"micro\"))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "# save xgboost model\n",
    "print(\"Saving model...\")\n",
    "pkl.dump(clf, open(\"Models/Tabular/xgboost_model_corlat.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "# save neural network model\n",
    "print(\"Saving model...\")\n",
    "# statedict\n",
    "torch.save(net.state_dict(), \"Models/Tabular/neural_network_model_corlat.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try to load the model net = NeuralNetwork()\n",
    "net.load_state_dict(torch.load(\"Models/Tabular/neural_network_model_corlat.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optimization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
