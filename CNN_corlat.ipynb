{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "import os\n",
    "\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "# f1 score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'train_val_split': [0.80, 0.20], # These must sum to 1.0\n",
    "        'batch_size' : 32, # Num samples to average over for gradient updates\n",
    "        'EPOCHS' : 100, # Num times to iterate over the entire dataset\n",
    "        'LEARNING_RATE' : 5e-3, # Learning rate for the optimizer\n",
    "        'BETA1' : 0.9, # Beta1 parameter for the Adam optimizer\n",
    "        'BETA2' : 0.999, # Beta2 parameter for the Adam optimizer\n",
    "        'WEIGHT_DECAY' : 1e-4, # Weight decay parameter for the Adam optimizer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/corlat_preprocessed.pickle\", \"rb\"))\n",
    "except:\n",
    "    # move dir to /ibm/gpfs/home/yjin0055/Project/DayAheadForecast\n",
    "    os.chdir(\"/ibm/gpfs/home/yjin0055/Project/DayAheadForecast\")\n",
    "    corlat_dataset = pkl.load(open(\"Data/corlat/corlat_preprocessed.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_obj_coef</th>\n",
       "      <th>num_nonzero_coef</th>\n",
       "      <th>lp_relax_val</th>\n",
       "      <th>is_lp_relax_val_frac</th>\n",
       "      <th>lp_sol_val_eq_lb</th>\n",
       "      <th>lp_sol_val_eq_ub</th>\n",
       "      <th>has_lb</th>\n",
       "      <th>has_ub</th>\n",
       "      <th>mean_degree</th>\n",
       "      <th>std_degree</th>\n",
       "      <th>min_degree</th>\n",
       "      <th>max_degree</th>\n",
       "      <th>mean_coef</th>\n",
       "      <th>std_coef</th>\n",
       "      <th>min_coef</th>\n",
       "      <th>max_coef</th>\n",
       "      <th>var_type_B</th>\n",
       "      <th>var_type_C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>36.706039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-50.166667</td>\n",
       "      <td>49.837792</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>45.415367</td>\n",
       "      <td>2.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-48.833333</td>\n",
       "      <td>51.275129</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.442327</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>45.415367</td>\n",
       "      <td>2.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-50.166667</td>\n",
       "      <td>49.837792</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>45.415367</td>\n",
       "      <td>2.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-48.833333</td>\n",
       "      <td>51.275129</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>35.333333</td>\n",
       "      <td>45.415367</td>\n",
       "      <td>2.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-48.833333</td>\n",
       "      <td>51.275129</td>\n",
       "      <td>-100.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   var_obj_coef  num_nonzero_coef  lp_relax_val  is_lp_relax_val_frac  \\\n",
       "0           5.0               6.0      1.000000                  True   \n",
       "1           4.0               6.0      0.000000                  True   \n",
       "2           6.0               6.0      0.442327                  True   \n",
       "3           5.0               6.0     -0.000000                  True   \n",
       "4           3.0               6.0     -0.000000                  True   \n",
       "\n",
       "   lp_sol_val_eq_lb  lp_sol_val_eq_ub  has_lb  has_ub  mean_degree  \\\n",
       "0              True              True    True    True    19.000000   \n",
       "1              True              True    True    True    35.333333   \n",
       "2              True              True    True    True    35.333333   \n",
       "3              True              True    True    True    35.333333   \n",
       "4              True              True    True    True    35.333333   \n",
       "\n",
       "   std_degree  min_degree  max_degree  mean_coef   std_coef  min_coef  \\\n",
       "0   36.706039         1.0       101.0 -50.166667  49.837792    -100.0   \n",
       "1   45.415367         2.0       101.0 -48.833333  51.275129    -100.0   \n",
       "2   45.415367         2.0       101.0 -50.166667  49.837792    -100.0   \n",
       "3   45.415367         2.0       101.0 -48.833333  51.275129    -100.0   \n",
       "4   45.415367         2.0       101.0 -48.833333  51.275129    -100.0   \n",
       "\n",
       "   max_coef  var_type_B  var_type_C  \n",
       "0       1.0           1           0  \n",
       "1       9.0           1           0  \n",
       "2       1.0           1           0  \n",
       "3       9.0           1           0  \n",
       "4       9.0           1           0  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corlat_dataset[0][\"input\"][\"var_node_features\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the maximum size of N_constraints and N_variables across the dataset.\n",
    "\n",
    "max_N_constraints = max(\n",
    "    len(x[\"input\"][\"constraint_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "max_N_variables = max(\n",
    "    len(x[\"input\"][\"var_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "min_N_constraints = min(\n",
    "    len(x[\"input\"][\"constraint_node_features\"]) for x in corlat_dataset\n",
    ")\n",
    "\n",
    "min_N_variables = min(\n",
    "    len(x[\"input\"][\"var_node_features\"]) for x in corlat_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of variables:  466\n",
      "Maximum number of constraints:  551\n",
      "Minimum number of variables:  466\n",
      "Minimum number of constraints:  470\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum number of variables: \", max_N_variables)\n",
    "print(\"Maximum number of constraints: \", max_N_constraints)\n",
    "print(\"Minimum number of variables: \", min_N_variables)\n",
    "print(\"Minimum number of constraints: \", min_N_constraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of variable node features:  18\n",
      "Number of constraint node features:  10\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of variable node features: \", len(corlat_dataset[0][\"input\"][\"var_node_features\"].columns))\n",
    "print(\"Number of constraint node features: \", len(corlat_dataset[0][\"input\"][\"constraint_node_features\"].columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each variable node features, pad with 0.0s to make it the same length as the maximum number of variables\n",
    "\n",
    "var_node_features = np.stack(\n",
    "    [\n",
    "        np.pad(\n",
    "            x[\"input\"][\"var_node_features\"].values,\n",
    "            ((0, max_N_variables - len(x[\"input\"][\"var_node_features\"])), (0, 0)),\n",
    "            \"constant\",\n",
    "            constant_values=0.0,\n",
    "        )\n",
    "        for x in corlat_dataset\n",
    "    ]\n",
    ")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 466, 18)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_node_features = np.stack(\n",
    "    [\n",
    "        np.pad(\n",
    "            x[\"input\"][\"constraint_node_features\"].values,\n",
    "            ((0, max_N_constraints - len(x[\"input\"][\"constraint_node_features\"])), (0, 0)),\n",
    "            \"constant\",\n",
    "            constant_values=0.0,\n",
    "        )\n",
    "        for x in corlat_dataset\n",
    "    ]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 551, 10)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constraint_node_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var_node_features and constraint_node_features, reshape to (N_samples, -1) to feed into the neural network\n",
    "var_input = var_node_features.reshape(var_node_features.shape[0], -1)\n",
    "constraint_input = constraint_node_features.reshape(constraint_node_features.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of variable features input:  (2000, 8388)\n",
      "Shape of constraint features input:  (2000, 5510)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of variable features input: \", var_input.shape)\n",
    "print(\"Shape of constraint features input: \", constraint_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get A matrix input by stacking the csr_matrix of each sample getting shape of N_samples x (A.shape[0] x A.shape[1])\n",
    "A_input = np.vstack([x[\"input\"][\"A\"] for x in corlat_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_feature_list = []\n",
    "for i in range(len(corlat_dataset)):\n",
    "    n_cons = corlat_dataset[i][\"input\"][\"A\"].shape[0]\n",
    "\n",
    "    # for row in range(n_vars):\n",
    "    #     for col in range(n_cons):\n",
    "    #         if input_dict_list[i][\"A\"][row, col] != 0:\n",
    "    #             adj_matrix[row, n_vars + col] = input_dict_list[i][\"A\"][row, col]\n",
    "    #             adj_matrix[n_vars + col, row] = input_dict_list[i][\"A\"][row, col]\n",
    "\n",
    "    I, J, V = scipy.sparse.find(corlat_dataset[i][\"input\"][\"A\"])\n",
    "    # adj_matrix[I, n_vars + J] = V\n",
    "    # adj_matrix[n_vars + J, I] = V\n",
    "\n",
    "    # # convert to COO format\n",
    "    edge_index = torch.stack([torch.tensor(I), torch.tensor(n_cons + J)], dim=0)\n",
    "\n",
    "    # expand V to 2D\n",
    "    edge_attr = torch.tensor(V).unsqueeze(1)\n",
    "\n",
    "    tmp_dict = {\"edge_index\": edge_index, \"edge_attr\": edge_attr}\n",
    "    A_feature_list.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sample, pad the edge_index and edge_attr to make it the same length as the maximum length of edge_index and edge_attr\n",
    "max_edge_index_len = max([len(x[\"edge_index\"][0]) for x in A_feature_list])\n",
    "max_edge_attr_len = max([len(x[\"edge_attr\"]) for x in A_feature_list])\n",
    "\n",
    "for i in range(len(A_feature_list)):\n",
    "    edge_index = A_feature_list[i][\"edge_index\"]\n",
    "    edge_attr = A_feature_list[i][\"edge_attr\"]\n",
    "\n",
    "    # pad edge_index\n",
    "    edge_index = torch.cat(\n",
    "        [\n",
    "            edge_index,\n",
    "            torch.zeros(\n",
    "                2, max_edge_index_len - len(edge_index[0]), dtype=torch.long\n",
    "            ),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    # pad edge_attr\n",
    "    edge_attr = torch.cat(\n",
    "        [\n",
    "            edge_attr,\n",
    "            torch.zeros(\n",
    "                max_edge_attr_len - len(edge_attr), 1, dtype=torch.float32\n",
    "            ),\n",
    "        ],\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    A_feature_list[i][\"edge_index\"] = edge_index\n",
    "    A_feature_list[i][\"edge_attr\"] = edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the padding is correct by checking the shape of edge_index and edge_attr\n",
    "for i in range(len(A_feature_list)):\n",
    "    assert A_feature_list[i][\"edge_index\"].shape == (2, max_edge_index_len)\n",
    "    assert A_feature_list[i][\"edge_attr\"].shape == (max_edge_attr_len, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A matrix input:  (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of A matrix input: \", A_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['solution', 'indices', 'input'])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corlat_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each solution convert the dictionary to a list of values\n",
    "solutions = [\n",
    "    list(corlat_dataset[i][\"solution\"].values())\n",
    "    for i in range(len(corlat_dataset))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert solutions_list to numpy array\n",
    "solutions = np.array(solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the variable features and constraint features into a single input\n",
    "X = np.hstack([var_input, constraint_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(13898, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 100)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check type for one sample of solutions\n",
    "solutions[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert X and solutions to float32\n",
    "X = X.astype(np.float32)\n",
    "solutions = solutions.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all samples have same dimension\n",
    "for i in range(len(solutions)):\n",
    "    assert solutions[i].shape == (100,)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    assert X[i].shape == (13898,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork()\n",
    "net = torch.compile(net)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "# create the dataloader for X and solutions\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X), torch.tensor(solutions)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "params = list(net.parameters())\n",
    "\n",
    "# optimizer = AdamW(params, lr=config['LEARNING_RATE'], weight_decay=1e-4)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "# optimizer = dadaptation.DAdaptAdam(params, lr=1, log_every=5, betas=(BETA1, BETA2), weight_decay=1e-4, decouple=True)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "total_steps = len(train_loader)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config['LEARNING_RATE'], steps_per_epoch=total_steps, epochs=config['EPOCHS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.878 lr: 0.000200\n",
      "Epoch 2 loss: 0.629 lr: 0.000213\n",
      "Epoch 3 loss: 0.580 lr: 0.000253\n",
      "Epoch 4 loss: 0.548 lr: 0.000318\n",
      "Epoch 5 loss: 0.535 lr: 0.000408\n",
      "Epoch 6 loss: 0.529 lr: 0.000522\n",
      "Epoch 7 loss: 0.531 lr: 0.000659\n",
      "Epoch 8 loss: 0.536 lr: 0.000817\n",
      "Epoch 9 loss: 0.529 lr: 0.000995\n",
      "Epoch 10 loss: 0.526 lr: 0.001190\n",
      "Epoch 11 loss: 0.529 lr: 0.001401\n",
      "Epoch 12 loss: 0.532 lr: 0.001625\n",
      "Epoch 13 loss: 0.524 lr: 0.001860\n",
      "Epoch 14 loss: 0.533 lr: 0.002103\n",
      "Epoch 15 loss: 0.528 lr: 0.002351\n",
      "Epoch 16 loss: 0.526 lr: 0.002602\n",
      "Epoch 17 loss: 0.521 lr: 0.002853\n",
      "Epoch 18 loss: 0.525 lr: 0.003101\n",
      "Epoch 19 loss: 0.525 lr: 0.003344\n",
      "Epoch 20 loss: 0.521 lr: 0.003578\n",
      "Epoch 21 loss: 0.522 lr: 0.003802\n",
      "Epoch 22 loss: 0.525 lr: 0.004013\n",
      "Epoch 23 loss: 0.522 lr: 0.004208\n",
      "Epoch 24 loss: 0.521 lr: 0.004386\n",
      "Epoch 25 loss: 0.523 lr: 0.004544\n",
      "Epoch 26 loss: 0.520 lr: 0.004680\n",
      "Epoch 27 loss: 0.521 lr: 0.004794\n",
      "Epoch 28 loss: 0.522 lr: 0.004884\n",
      "Epoch 29 loss: 0.519 lr: 0.004948\n",
      "Epoch 30 loss: 0.518 lr: 0.004987\n",
      "Epoch 31 loss: 0.517 lr: 0.005000\n",
      "Epoch 32 loss: 0.517 lr: 0.004997\n",
      "Epoch 33 loss: 0.519 lr: 0.004990\n",
      "Epoch 34 loss: 0.516 lr: 0.004977\n",
      "Epoch 35 loss: 0.517 lr: 0.004960\n",
      "Epoch 36 loss: 0.518 lr: 0.004937\n",
      "Epoch 37 loss: 0.516 lr: 0.004909\n",
      "Epoch 38 loss: 0.514 lr: 0.004877\n",
      "Epoch 39 loss: 0.516 lr: 0.004840\n",
      "Epoch 40 loss: 0.515 lr: 0.004798\n",
      "Epoch 41 loss: 0.516 lr: 0.004752\n",
      "Epoch 42 loss: 0.512 lr: 0.004701\n",
      "Epoch 43 loss: 0.517 lr: 0.004645\n",
      "Epoch 44 loss: 0.518 lr: 0.004585\n",
      "Epoch 45 loss: 0.514 lr: 0.004521\n",
      "Epoch 46 loss: 0.515 lr: 0.004453\n",
      "Epoch 47 loss: 0.517 lr: 0.004382\n",
      "Epoch 48 loss: 0.513 lr: 0.004306\n",
      "Epoch 49 loss: 0.512 lr: 0.004226\n",
      "Epoch 50 loss: 0.511 lr: 0.004144\n",
      "Epoch 51 loss: 0.509 lr: 0.004057\n",
      "Epoch 52 loss: 0.513 lr: 0.003968\n",
      "Epoch 53 loss: 0.511 lr: 0.003876\n",
      "Epoch 54 loss: 0.509 lr: 0.003781\n",
      "Epoch 55 loss: 0.507 lr: 0.003683\n",
      "Epoch 56 loss: 0.507 lr: 0.003583\n",
      "Epoch 57 loss: 0.509 lr: 0.003481\n",
      "Epoch 58 loss: 0.511 lr: 0.003377\n",
      "Epoch 59 loss: 0.509 lr: 0.003271\n",
      "Epoch 60 loss: 0.511 lr: 0.003163\n",
      "Epoch 61 loss: 0.507 lr: 0.003055\n",
      "Epoch 62 loss: 0.510 lr: 0.002945\n",
      "Epoch 63 loss: 0.508 lr: 0.002834\n",
      "Epoch 64 loss: 0.509 lr: 0.002722\n",
      "Epoch 65 loss: 0.508 lr: 0.002610\n",
      "Epoch 66 loss: 0.505 lr: 0.002498\n",
      "Epoch 67 loss: 0.507 lr: 0.002386\n",
      "Epoch 68 loss: 0.506 lr: 0.002274\n",
      "Epoch 69 loss: 0.508 lr: 0.002163\n",
      "Epoch 70 loss: 0.505 lr: 0.002052\n",
      "Epoch 71 loss: 0.504 lr: 0.001942\n",
      "Epoch 72 loss: 0.506 lr: 0.001833\n",
      "Epoch 73 loss: 0.504 lr: 0.001726\n",
      "Epoch 74 loss: 0.503 lr: 0.001620\n",
      "Epoch 75 loss: 0.504 lr: 0.001516\n",
      "Epoch 76 loss: 0.502 lr: 0.001414\n",
      "Epoch 77 loss: 0.502 lr: 0.001314\n",
      "Epoch 78 loss: 0.502 lr: 0.001216\n",
      "Epoch 79 loss: 0.503 lr: 0.001121\n",
      "Epoch 80 loss: 0.501 lr: 0.001029\n",
      "Epoch 81 loss: 0.503 lr: 0.000940\n",
      "Epoch 82 loss: 0.500 lr: 0.000854\n",
      "Epoch 83 loss: 0.501 lr: 0.000771\n",
      "Epoch 84 loss: 0.501 lr: 0.000692\n",
      "Epoch 85 loss: 0.502 lr: 0.000616\n",
      "Epoch 86 loss: 0.500 lr: 0.000544\n",
      "Epoch 87 loss: 0.500 lr: 0.000476\n",
      "Epoch 88 loss: 0.500 lr: 0.000413\n",
      "Epoch 89 loss: 0.499 lr: 0.000353\n",
      "Epoch 90 loss: 0.499 lr: 0.000298\n",
      "Epoch 91 loss: 0.499 lr: 0.000247\n",
      "Epoch 92 loss: 0.498 lr: 0.000201\n",
      "Epoch 93 loss: 0.497 lr: 0.000159\n",
      "Epoch 94 loss: 0.498 lr: 0.000122\n",
      "Epoch 95 loss: 0.497 lr: 0.000090\n",
      "Epoch 96 loss: 0.498 lr: 0.000062\n",
      "Epoch 97 loss: 0.498 lr: 0.000040\n",
      "Epoch 98 loss: 0.498 lr: 0.000022\n",
      "Epoch 99 loss: 0.498 lr: 0.000010\n",
      "Epoch 100 loss: 0.498 lr: 0.000002\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d loss: %.3f lr: %.6f' % (epoch + 1, running_loss / len(train_loader), curr_lr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (optimization)",
   "language": "python",
   "name": "optimization"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
